はい、承知いたしました。
新たなご要望に合わせて、**問題文**と**解説**には日本語訳を併記し、**回答の選択肢は英語のまま**とする形式に修正します。

サンプルとして、No.1からNo.10までを修正した内容を以下に出力します。

-----

# Google Cloud Associate Data Practitioner 問題集 (V12.95版)

## 目次

  * [NO.1: Secure Dataset Sharing with External Partners](https://www.google.com/search?q=%23no1)
  * [NO.2: One-Time Load from Local Files to BigQuery](https://www.google.com/search?q=%23no2)
  * [NO.3: Collaborative Environment for BigQuery Data Analysis](https://www.google.com/search?q=%23no3)
  * [NO.4: Access Control for Regional Transaction Data](https://www.google.com/search?q=%23no4)
  * [NO.5: ETL Pipeline for Real-Time Streaming Data](https://www.google.com/search?q=%23no5)
  * [NO.6: Cloud Storage Cost Reduction and Compliance](https://www.google.com/search?q=%23no6)
  * [NO.7: Data Pipeline Availability for Single-Zone Outages](https://www.google.com/search?q=%23no7)
  * [NO.8: Handling Activity Spikes in Pub/Sub](https://www.google.com/search?q=%23no8)
  * [NO.9: Improving Cloud SQL Read Performance](https://www.google.com/search?q=%23no9)
  * [NO.10: Migrating Existing Data Warehouse to BigQuery](https://www.google.com/search?q=%23no10)
  * [NO.11: Product Demand Forecasting with BigQuery ML](https://www.google.com/search?q=%23no11)
  * [NO.12: Low-Latency Data Ingestion from Cloud Storage to BigQuery](https://www.google.com/search?q=%23no12)
  * [NO.13: Improving Streaming Data Dashboard Performance](https://www.google.com/search?q=%23no13)
  * [NO.14: Access Control in Looker's Shared Folder](https://www.google.com/search?q=%23no14)
  * [NO.15: ETL Pipeline Design for Streaming Data](https://www.google.com/search?q=%23no15)
  * [NO.16: Data Manipulation Methodology to Prevent SQL Injection](https://www.google.com/search?q=%23no16)
  * [NO.17: Access Control for Sensitive Data in BigQuery](https://www.google.com/search?q=%23no17)
  * [NO.18: Aggregating and Visualizing Monthly Inventory Data](https://www.google.com/search?q=%23no18)
  * [NO.19: Automatic Deletion of Temporary Files in Cloud Storage](https://www.google.com/search?q=%23no19)
  * [NO.20: Near Real-Time Processing of JSON Telemetry Data](https://www.google.com/search?q=%23no20)
  * [NO.21: Migrating On-Premises MySQL to Google Cloud](https://www.google.com/search?q=%23no21)
  * [NO.22: Troubleshooting Failed Scheduled Queries in BigQuery](https://www.google.com/search?q=%23no22)
  * [NO.23: Low-Overhead Data Processing Triggered by Cloud Storage Files](https://www.google.com/search?q=%23no23)
  * [NO.24: Recommended Approach for Transforming and Loading JSON Data to BigQuery](https://www.google.com/search?q=%23no24)
  * [NO.25: Making Sensitive Data Unreadable for Departed Employees](https://www.google.com/search?q=%23no25)
  * [NO.26: Securely Sharing BigQuery Datasets with External Partners](https://www.google.com/search?q=%23no26)
  * [NO.27: Data Management Strategy for Access and Compliance Requirements](https://www.google.com/search?q=%23no27)
  * [NO.28: One-Time SQL Analysis on Parquet Files in Cloud Storage](https://www.google.com/search?q=%23no28)
  * [NO.29: Creating a Customer Churn Prediction Model with BigQuery ML](https://www.google.com/search?q=%23no29)
  * [NO.30: Appropriate Storage Selection for Customer Data from Multiple Sources](https://www.google.com/search?q=%23no30)
  * [NO.31: Access Control for Regional Sales Metrics in Looker](https://www.google.com/search?q=%23no31)
  * [NO.32: Processing Pipeline for Real-Time Gaming Activity Data](https://www.google.com/search?q=%23no32)
  * [NO.33: Fixed Budgeting for Departmental BigQuery Query Costs](https://www.google.com/search?q=%23no33)
  * [NO.34: Customized Automated Delivery of Looker Dashboards](https://www.google.com/search?q=%23no34)
  * [NO.35: Low-Code Data Transfer from Google Ads to BigQuery](https://www.google.com/search?q=%23no35)
  * [NO.36: Decreasing Transfer Time for a Large Number of Small Files](https://www.google.com/search?q=%23no36)
  * [NO.37: Building a Purchase Prediction Model with BigQuery ML](https://www.google.com/search?q=%23no37)
  * [NO.38: Granting BigQuery Access to a New Data Analyst](https://www.google.com/search?q=%23no38)
  * [NO.39: Cost-Effective Storage Design for a Video-Sharing Website](https://www.google.com/search?q=%23no39)
  * [NO.40: Representing Audio Files from Cloud Storage in BigQuery](https://www.google.com/search?q=%23no40)
  * [NO.41: Managed Solution for Building SQL-Based ELT Processes](https://www.google.com/search?q=%23no41)
  * [NO.42: Granting Least Privilege to an Application's Service Account](https://www.google.com/search?q=%23no42)
  * [NO.43: De-identifying PII Data from Data Feeds](https://www.google.com/search?q=%23no43)
  * [NO.44: Creating a Sales Metrics Dashboard Across Multiple Tables in Looker](https://www.google.com/search?q=%23no44)
  * [NO.45: Limiting Data Access Between Teams in an Enterprise BI Platform](https://www.google.com/search?q=%23no45)
  * [NO.46: Efficiently Analyzing Large BigQuery Datasets in Colab Enterprise](https://www.google.com/search?q=%23no46)
  * [NO.47: Creating a Customer Churn Prediction Model with BigQuery ML (Repeat)](https://www.google.com/search?q=%23no47)
  * [NO.48: Granting Payroll Specialist Access to Aggregated Performance Data](https://www.google.com/search?q=%23no48)
  * [NO.49: Troubleshooting Intermittent Failures in a Dataflow Pipeline](https://www.google.com/search?q=%23no49)
  * [NO.50: Determining When to Retrain a BigQuery ML Model](https://www.google.com/search?q=%23no50)
  * [NO.51: Seven-Year Data Retention Requirement in BigQuery](https://www.google.com/search?q=%23no51)
  * [NO.52: Sharing a BigQuery Dataset While Minimizing Unauthorized Copying](https://www.google.com/search?q=%23no52)
  * [NO.53: Selecting a Model for Predicting Clicks on Online Ads](https://www.google.com/search?q=%23no53)
  * [NO.54: Choosing Storage Classes for an ML Project Lifecycle](https://www.google.com/search?q=%23no54)
  * [NO.55: Adding a New Field to a Looker Report without Develop Permission](https://www.google.com/search?q=%23no55)
  * [NO.56: Scheduling and Automating Data Pipelines with Complex Dependencies](https://www.google.com/search?q=%23no56)
  * [NO.57: Quickly Cleaning Inconsistent Data in BigQuery](https://www.google.com/search?q=%23no57)
  * [NO.58: ETL Pipeline Design for Customer Data from Multiple Sources](https://www.google.com/search?q=%23no58)
  * [NO.59: Migrating 300 TB of On-Premises Data to Cloud Storage in a Few Days](https://www.google.com/search?q=%23no59)
  * [NO.60: Cost-Effective Storage Class for Historical Data Accessed Monthly](https://www.google.com/search?q=%23no60)
  * [NO.61: Ensuring Three-Year Data Retention in a Cloud Storage Bucket](https://www.google.com/search?q=%23no61)
  * [NO.62: Optimizing Storage Costs to Meet Data Tiering and Retention Rules](https://www.google.com/search?q=%23no62)
  * [NO.63: Efficiently Generating Weekly Aggregated Sales Reports from Large Data](https://www.google.com/search?q=%23no63)
  * [NO.64: Quickly Creating and Re-running Risk Reports Using a Public Dataset](https://www.google.com/search?q=%23no64)
  * [NO.65: Preventing Data Loss for a Critical BigQuery Table for Month-End Reports](https://www.google.com/search?q=%23no65)
  * [NO.66: Migrating On-Premises Apache Airflow DAGs to Google Cloud](https://www.google.com/search?q=%23no66)
  * [NO.67: Cleaning Inconsistent Data in Cloud Storage](https://www.google.com/search?q=%23no67)
  * [NO.68: Least Privilege IAM Role to Restart Dataflow Streaming Pipelines](https://www.google.com/search?q=%23no68)
  * [NO.69: Serverless Pipeline for Real-Time Streaming Data Analysis](https://www.google.com/search?q=%23no69)
  * [NO.70: Online Migration from On-Premises File Server to Cloud Storage with Bandwidth Limit](https://www.google.com/search?q=%23no70)
  * [NO.71: Quickly Troubleshooting a Failed Dataflow Job](https://www.google.com/search?q=%23no71)
  * [NO.72: Data Storage Encryption Keys Requiring Complete Manual Control](https://www.google.com/search?q=%23no72)
  * [NO.73: Automatically Aggregating and Ranking Regional Sales Data Across Tables](https://www.google.com/search?q=%23no73)
  * [NO.74: Transforming and Loading Daily CSV Files into BigQuery](https://www.google.com/search?q=%23no74)
  * [NO.75: Quickly Recovering a Data Processing Pipeline When a Stage Fails](https://www.google.com/search?q=%23no75)
  * [NO.76: Migrating On-Premises MySQL to Google Cloud with Minimal Downtime](https://www.google.com/search?q=%23no76)
  * [NO.77: Migrating from On-Premises NFS to Google Cloud while Controlling Bandwidth](https://www.google.com/search?q=%23no77)
  * [NO.78: Summarizing Customer Feedback in BigQuery Using Gemini LLM](https://www.google.com/search?q=%23no78)
  * [NO.79: Visualizing Profit Margin Using LookML](https://www.google.com/search?q=%23no79)
  * [NO.80: Monitoring and Alerting for a Dataflow Pipeline](https://www.google.com/search?q=%23no80)
  * [NO.81: Cloud Storage Bucket Strategy for High Availability in Two Specific Regions](https://www.google.com/search?q=%23no81)
  * [NO.82: Sentiment Analysis of Customer Reviews with Diverse Language Patterns](https://www.google.com/search?q=%23no82)
  * [NO.83: Selecting Appropriate Tools for Multiple Data Type Use Cases](https://www.google.com/search?q=%23no83)
  * [NO.84: Scheduling a Daily Spark Job on Dataproc and Sending a Report](https://www.google.com/search?q=%23no84)
  * [NO.85: Summarizing Customer Feedback in BigQuery Using Gemini LLM (Repeat)](https://www.google.com/search?q=%23no85)
  * [NO.86: Data Validation and Cleansing Pipeline for Real-Time Analysis](https://www.google.com/search?q=%23no86)
  * [NO.87: Efficiently Deleting Data Older Than One Year from BigQuery](https://www.google.com/search?q=%23no87)
  * [NO.88: Processing Large-Scale Log Data by Leveraging Existing Spark Scripts](https://www.google.com/search?q=%23no88)
  * [NO.89: Comparing Budget Data in Google Sheets with Actuals in BigQuery](https://www.google.com/search?q=%23no89)
  * [NO.90: Visual Pipeline to Process Streaming Data from Multiple Regions](https://www.google.com/search?q=%23no90)
  * [NO.91: Batch Transformation Pipeline Using Only SQL with Git Integration](https://www.google.com/search?q=%23no91)
  * [NO.92: Efficient and Cost-Effective Transformation and Loading of a Small Dataset](https://www.google.com/search?q=%23no92)
  * [NO.93: Solution to Create Aggregated Metrics While Allowing Raw Data Queries](https://www.google.com/search?q=%23no93)
  * [NO.94: Data Encryption Without Operational Overhead of Managing Encryption Keys](https://www.google.com/search?q=%23no94)
  * [NO.95: Low-Code Solution to Process Batch Data from Multiple Sources](https://www.google.com/search?q=%23no95)
  * [NO.96: High Availability for a 24/7 Financial Service's Cloud SQL for PostgreSQL](https://www.google.com/search?q=%23no96)
  * [NO.97: ETL Solution Using SQL and JavaScript with Version Control and Quality Checks](https://www.google.com/search?q=%23no97)
  * [NO.98: Data Recovery for Cloud SQL for PostgreSQL if the Primary Region is Destroyed](https://www.google.com/search?q=%23no98)
  * [NO.99: Migrating Existing Data Warehouse to BigQuery (Repeat)](https://www.google.com/search?q=%23no99)
  * [NO.100: Visualizing Profit Margin Using LookML (Repeat)](https://www.google.com/search?q=%23no100)
  * [NO.101: Serverless Pipeline to Translate Product Reviews in Multiple Languages](https://www.google.com/search?q=%23no101)
  * [NO.102: Calculating Weekly Moving Average of Sales by Location Using SQL](https://www.google.com/search?q=%23no102)
  * [NO.103: Serverless Migration of On-Premises Apache Spark Workloads](https://www.google.com/search?q=%23no103)
  * [NO.104: Building a Customer Churn Prediction Model on a 50 PB Dataset](https://www.google.com/search?q=%23no104)
  * [NO.105: Access Control for Regional Transaction Data (Repeat)](https://www.google.com/search?q=%23no105)
  * [NO.106: Solution to Automate and Monitor an Existing ETL Pipeline Without Rewriting Code](https://www.google.com/search?q=%23no106)
  * [NO.107: Migrating Over 500 TB of Data with Low Bandwidth in a Few Days](https://www.google.com/search?q=%23no107)
  * [NO.108: Managed ETL Tool for a Team of Python Developers](https://www.google.com/search?q=%23no108)

-----

### <a name="no1"></a>**NO.1**

You created a curated dataset of market trends in BigQuery that you want to share with multiple external partners. You want to control the rows and columns that each partner has access to. You want to follow Google-recommended practices. What should you do?

あなたは、BigQuery内に市場トレンドに関するキュレーション済みデータセットを作成し、複数の外部パートナーと共有したいと考えています。各パートナーがアクセスできる行と列を制御したいと考えています。Googleが推奨するプラクティスに従いたい場合、どうすべきですか？

A. Publish the dataset in Analytics Hub. Grant dataset-level access to each partner by using subscriptions.
B. Create a separate Cloud Storage bucket for each partner. Export the dataset to each bucket and assign each partner to their respective bucket. Grant bucket-level access by using 1AM roles.
C. Grant each partner read access to the BigQuery dataset by using 1AM roles.
D. Create a separate project for each partner and copy the dataset into each project. Publish each dataset in Analytics Hub. Grant dataset-level access to each partner by using subscriptions.

**正解: A**

**解説:**
Comprehensive and Detailed in Depth Explanation:
Why A is correct: Analytics Hub allows you to share datasets with external partners while maintaining control over access. Subscriptions allow granular control.
Why other options are incorrect:B: Cloud storage is for files, not bigquery datasets.
C: IAM roles do not allow for granular row and column level control.
D: Creating a separate project for each partner is complex and not scalable.

包括的で詳細な解説：
Aが正しい理由：Analytics Hubを使用すると、アクセスを制御しながら外部パートナーとデータセットを共有できます。サブスクリプションにより、詳細な制御が可能になります。
他の選択肢が不適切な理由：B：Cloud Storageはファイル用であり、BigQueryデータセット用ではありません。
C：IAMロールでは、行および列レベルの詳細な制御はできません。
D：パートナーごとに個別のプロジェクトを作成するのは複雑で、スケーラブルではありません。

-----

### <a name="no2"></a>**NO.2**

You are using your own data to demonstrate the capabilities of BigQuery to your organization's leadership team. You need to perform a one-time load of the files stored on your local machine into BigQuery using as little effort as possible. What should you do?

あなたは、組織のリーダーシップチームにBigQueryの能力を実証するために、独自のデータを使用しています。ローカルマシンに保存されているファイルを、できるだけ少ない労力でBigQueryに一度だけロードする必要があります。どうすべきですか？

A. Write and execute a Python script using the BigQuery Storage Write API library.
B. Create a Dataproc cluster, copy the files to Cloud Storage, and write an Apache Spark job using the spark-bigquery-connector.
C. Execute the bq load command on your local machine.
D. Create a Dataflow job using the Apache Beam FileIO and BigQueryIO connectors with a local runner.

**正解: C**

**解説:**
Comprehensive and Detailed In-Depth Explanation:
A one-time load with minimal effort points to a simple, out-of-the-box tool. The files are local, so the solution must bridge on-premises to BigQuery easily.

  * Option A: A Python script with the Storage Write API requires coding, setup (authentication, libraries), and debugging-more effort than necessary for a one-time task.
  * Option B: Dataproc with Spark involves cluster creation, file transfer to Cloud Storage, and job scripting-far too complex for a simple load.
  * Option C: The bq load command (part of the Google Cloud SDK) is a CLI tool that uploads local files (e.g., CSV, JSON) directly to BigQuery with one command (e.g., bq load --source_format=CSV dataset. table file.csv). It's pre-built, requires no coding, and leverages existing SDK installation, minimizing effort.

包括的で詳細な解説：
最小限の労力でのワンタイムロードは、シンプルですぐに使えるツールを示唆しています。ファイルはローカルにあるため、ソリューションはオンプレミスからBigQueryへ簡単に橋渡しする必要があります。

  * 選択肢A：Storage Write APIを使用したPythonスクリプトには、コーディング、設定（認証、ライブラリ）、デバッグが必要で、ワンタイムタスクには必要以上の労力がかかります。
  * 選択肢B：DataprocとSparkには、クラスタ作成、Cloud Storageへのファイル転送、ジョブのスクリプト作成が含まれ、単純なロードには複雑すぎます。
  * 選択肢C：bq loadコマンド（Google Cloud SDKの一部）は、ローカルファイル（例：CSV、JSON）を1つのコマンドで直接BigQueryにアップロードするCLIツールです（例：`bq load --source_format=CSV dataset.table file.csv`）。これは事前構築済みで、コーディング不要で、既存のSDKインストールを活用するため、労力を最小限に抑えます。

-----

### <a name="no3"></a>**NO.3**

Your team needs to analyze large datasets stored in BigQuery to identify trends in user behavior. The analysis will involve complex statistical calculations, Python packages, and visualizations. You need to recommend a managed collaborative environment to develop and share the analysis. What should you recommend?

あなたのチームは、ユーザー行動のトレンドを特定するために、BigQueryに保存されている大規模なデータセットを分析する必要があります。分析には、複雑な統計計算、Pythonパッケージ、および可視化が含まれます。分析を開発・共有するための、管理された共同作業環境を推奨する必要があります。何を推奨すべきですか？

A. Create a Colab Enterprise notebook and connect the notebook to BigQuery. Share the notebook with your team. Analyze the data and generate visualizations in Colab Enterprise.
B. Create a statistical model by using BigQuery ML. Share the query with your team. Analyze the data and generate visualizations in Looker Studio.
C. Create a Looker Studio dashboard and connect the dashboard to BigQuery. Share the dashboard with your team. Analyze the data and generate visualizations in Looker Studio.
D. Connect Google Sheets to BigQuery by using Connected Sheets. Share the Google Sheet with your team. Analyze the data and generate visualizations in Google Sheets.

**正解: A**

**解説:**
Using a Colab Enterprise notebook connected to BigQuery provides a managed, collaborative environment ideal for complex statistical calculations, Python packages, and visualizations. Colab Enterprise supports Python libraries for advanced analytics and offers seamless integration with BigQuery for querying large datasets. It allows teams to collaboratively develop and share analyses while taking advantage of its visualization capabilities. This approach is particularly suitable for tasks involving sophisticated computations and custom visualizations.

BigQueryに接続されたColab Enterpriseノートブックを使用すると、複雑な統計計算、Pythonパッケージ、および可視化に理想的な、管理された共同作業環境が提供されます。Colab Enterpriseは、高度な分析のためのPythonライブラリをサポートし、大規模データセットをクエリするためのBigQueryとのシームレスな統合を提供します。これにより、チームは洗練された計算やカスタム可視化を伴うタスクに特に適した方法で、分析を共同で開発・共有できます。

-----

### <a name="no4"></a>**NO.4**

You are a database administrator managing sales transaction data by region stored in a BigQuery table. You need to ensure that each can only see the transactions in their region. What should you do?

あなたは、BigQueryテーブルに保存されている地域別の売上トランザクションデータを管理するデータベース管理者です。各担当者が自分の地域のトランザクションのみを表示できるようにする必要があります。どうすべきですか？

A. Grant the appropriate 1AM permissions on the dataset.
B. Create a data masking rule.
C. Create a row-level access policy
D. Add a policy tag in BigQuery.

**正解: C**

**解説:**
A row-level access policy is the correct solution because it is specifically designed to filter which rows a user can see based on defined conditions. You can create a policy that filters the data based on the user's identity (e.g., WHERE region = SESSION_USER()), ensuring that sales representatives can only query rows corresponding to their assigned region.

  * IAM permissions operate at the project, dataset, or table level and cannot control access to individual rows.

  * Data masking is used to obscure data within a column (e.g., showing only the last four digits of a number), not to filter rows.

  * Policy tags are used for column-level security to restrict access to sensitive columns, not rows.

行レベルのアクセス ポリシーは、定義された条件に基づいてユーザーが表示できる行をフィルタリングするために特別に設計されているため、これが正しい解決策です。ユーザーのIDに基づいてデータをフィルタリングするポリシー（例： WHERE region = SESSION_USER()）を作成でき、これにより営業担当者は自身に割り当てられた地域に対応する行のみをクエリできるようになります。

  * IAM権限はプロジェクト、データセット、またはテーブルレベルで機能し、個々の行へのアクセスは制御できません。

  * データマスキングは列内のデータを不明瞭にするために使用され（例：番号の下4桁のみを表示）、行をフィルタリングするものではありません。

  * ポリシータグは、機密性の高い列へのアクセスを制限するための列レベルのセキュリティに使用され、行には使用されません。
-----

### <a name="no5"></a>**NO.5**

Your organization needs to implement near real-time analytics for thousands of events arriving each second in Pub/Sub. The incoming messages require transformations. You need to configure a pipelinethat processes, transforms, and loads the data into BigQuery while minimizing development time. What should you do?

あなたの組織は、Pub/Subに毎秒到着する何千ものイベントに対して、ほぼリアルタイムの分析を実装する必要があります。受信メッセージには変換が必要です。開発時間を最小限に抑えながら、データを処理、変換し、BigQueryにロードするパイプラインを構成する必要があります。どうすべきですか？

A. Use a Google-provided Dataflow template to process the Pub/Sub messages, perform transformations, and write the results to BigQuery.
B. Create a Cloud Data Fusion instance and configure Pub/Sub as a source. Use Data Fusion to process the Pub/Sub messages, perform transformations, and write the results to BigQuery.
C. Load the data from Pub/Sub into Cloud Storage using a Cloud Storage subscription. Create a Dataproc cluster, use PySpark to perform transformations in Cloud Storage, and write the results to BigQuery.
D. Use Cloud Run functions to process the Pub/Sub messages, perform transformations, and write the results to BigQuery.

**正解: A**

**解説:**
Using aGoogle-provided Dataflow templateis the most efficient and development-friendly approach to implement near real-time analytics for Pub/Sub messages. Dataflow templates are pre-built and optimized for processing streaming data, allowing you to quickly configure and deploy a pipeline with minimal development effort. These templates can handle message ingestion from Pub/Sub, perform necessary transformations, and load the processed data into BigQuery, ensuring scalability and low latency for near real- time analytics.

Googleが提供するDataflowテンプレートを使用することは、Pub/Subメッセージのほぼリアルタイム分析を実装するための最も効率的で開発者に優しいアプローチです。Dataflowテンプレートは、ストリーミングデータの処理用に事前構築され、最適化されているため、最小限の開発労力でパイプラインを迅速に構成・展開できます。これらのテンプレートは、Pub/Subからのメッセージ取り込み、必要な変換の実行、処理済みデータのBigQueryへのロードを処理でき、ほぼリアルタイム分析のためのスケーラビリティと低遅延を保証します。

-----

### <a name="no6"></a>**NO.6**

You manage a large amount of data in Cloud Storage, including raw data, processed data, and backups. Your organization is subject to strict compliance regulations that mandate data immutability for specific data types. You want to use an efficient process to reduce storage costs while ensuring that your storage strategy meets retention requirements. What should you do?

あなたは、生データ、処理済みデータ、バックアップなど、Cloud Storageで大量のデータを管理しています。あなたの組織は、特定のデータタイプに対してデータの不変性を義務付ける厳格なコンプライアンス規制の対象となっています。保持要件を満たしながら、ストレージ戦略がストレージコストを削減する効率的なプロセスを使用したいと考えています。どうすべきですか？

A. Configure lifecycle management rules to transition objects to appropriate storage classes based on access patterns. Set up Object Versioning for all objects to meet immutability requirements.
B. Move objects to different storage classes based on their age and access patterns. Use Cloud Key Management Service (Cloud KMS) to encrypt specific objects with customer-managed encryption keys (CMEK) to meet immutability requirements.
C. Create a Cloud Run function to periodically check object metadata, and move objects to the appropriate storage class based on age and access patterns. Use object holds to enforce immutability for specific objects.
D. Use object holds to enforce immutability for specific objects, and configure lifecycle management rules to transition objects to appropriate storage classes based on age and access patterns.

**正解: D**

**解説:**
Usingobject holdsandlifecycle management rulesis the most efficient and compliant strategy for this scenario because:

  * Immutability: Object holds (temporary or event-based) ensure that objects cannot be deleted or overwritten, meeting strict compliance regulations for data immutability.
  * Cost efficiency: Lifecycle management rules automatically transition objects to more cost-effective storage classes based on their age and access patterns.
  * Compliance and automation: This approach ensures compliance with retention requirements while reducing manual effort, leveraging built-in Cloud Storage features.

オブジェクトホールドとライフサイクル管理ルールを使用することは、このシナリオで最も効率的でコンプライアンスに準拠した戦略です。なぜなら：

  * 不変性：オブジェクトホールド（一時的またはイベントベース）は、オブジェクトが削除または上書きされないことを保証し、データ不変性に関する厳格なコンプライアンス規制を満たします。
  * コスト効率：ライフサイクル管理ルールは、オブジェクトをその年齢やアクセスパターンに基づいて、よりコスト効率の高いストレージクラスに自動的に移行させます。
  * コンプライアンスと自動化：このアプローチは、Cloud Storageの組み込み機能を活用して、手作業を減らしながら保持要件へのコンプライアンスを保証します。

-----

### <a name="no7"></a>**NO.7**

You are constructing a data pipeline to process sensitive customer data stored in a Cloud Storage bucket. You need to ensure that this data remains accessible, even in the event of a single- zone outage. What should you do?

あなたは、Cloud Storageバケットに保存されている機密性の高い顧客データを処理するためのデータパイプラインを構築しています。このデータが、単一ゾーンの障害が発生した場合でもアクセス可能であり続けることを保証する必要があります。どうすべきですか？

A. Set up a Cloud CDN in front of the bucket.
B. Enable Object Versioning on the bucket.
C. Store the data in a multi-region bucket.
D. Store the data in Nearline storage.

**正解: C**

**解説:**
Storing the data in amulti-region bucketensures high availability and durability, even in the event of a single- zone outage. Multi-region buckets replicate data across multiple locations within the selected region, providing resilience against zone-level failures and ensuring that the data remains accessible. This approach is particularly suitable for sensitive customer data that must remain available without interruptions. A single-zone outage requires high availability across zones or regions.

  * Option A: Cloud CDN caches content for web delivery but doesn't protect against underlying storage outages-it's for performance, not availability of the source data.
  * Option B: Object Versioning retains old versions of objects, protecting against overwrites or deletions, but doesn't ensure availability during a zone failure (still tied to one location).
  * Option C: Multi-region buckets (e.g., us or eu) replicate data across multiple regions, ensuring accessibility even if a single zone or region fails. This provides the highest availability for sensitive data in a pipeline.

データをマルチリージョンバケットに保存することで、単一ゾーンの障害が発生した場合でも高い可用性と耐久性が保証されます。マルチリージョンバケットは、選択したリージョン内の複数の場所にデータを複製し、ゾーンレベルの障害に対する回復力を提供し、データがアクセス可能であり続けることを保証します。このアプローチは、中断なく利用可能でなければならない機密性の高い顧客データに特に適しています。単一ゾーンの障害には、ゾーンまたはリージョン間での高い可用性が必要です。

  * 選択肢A：Cloud CDNはWeb配信のためにコンテンツをキャッシュしますが、基盤となるストレージの障害から保護するものではありません。これはパフォーマンスのためであり、ソースデータの可用性のためではありません。
  * 選択肢B：オブジェクトのバージョニングはオブジェクトの古いバージョンを保持し、上書きや削除から保護しますが、ゾーン障害時の可用性は保証しません（依然として1つの場所に紐づいています）。
  * 選択肢C：マルチリージョンバケット（例：usまたはeu）は、複数のリージョンにデータを複製し、単一のゾーンまたはリージョンが失敗した場合でもアクセス可能性を保証します。これは、機密データに対して最高の可用性を提供します。

-----

### <a name="no8"></a>**NO.8**

Your organization sends IoT event data to a Pub/Sub topic. Subscriber applications read and perform transformations on the messages before storing them in the data warehouse. During particularly busy times when more data is being written to the topic, you notice that the subscriber applications are not acknowledging messages within the deadline. You need to modify your pipeline to handle these activity spikes and continue to process the messages. What should you do?

あなたの組織は、IoTイベントデータをPub/Subトピックに送信しています。サブスクライバーアプリケーションは、メッセージを読み取り、データウェアハウスに保存する前に変換を実行します。特にトピックに多くのデータが書き込まれている忙しい時間帯に、サブスクライバーアプリケーションが期限内にメッセージを確認応答していないことに気づきました。これらのアクティビティスパイクを処理し、メッセージの処理を継続するためにパイプラインを変更する必要があります。どうすべきですか？

A. Retry messages until they are acknowledged.
B. Implement flow control on the subscribers
C. Forward unacknowledged messages to a dead-letter topic.
D. Seek back to the last acknowledged message.

**正解: B**

**解説:**
Implementingflow control on the subscribersallows the subscriber applications to manage message processing during activity spikes by controlling the rate at which messages are pulled and processed. This prevents overwhelming the subscribers and ensures that messages are acknowledged within the deadline. Flow control helps maintain the stability of your pipeline during high-traffic periods without dropping or delaying messages unnecessarily.

サブスクライバーにフロー制御を実装することで、サブスクライバーアプリケーションは、メッセージがプルされ処理されるレートを制御することにより、アクティビティスパイク中のメッセージ処理を管理できます。これにより、サブスクライバーが過負荷になるのを防ぎ、メッセージが期限内に確認応答されることが保証されます。フロー制御は、メッセージを不必要にドロップしたり遅延させたりすることなく、高トラフィック期間中のパイプラインの安定性を維持するのに役立ちます。

-----

### <a name="no9"></a>**NO.9**

You manage a web application that stores data in a Cloud SQL database. You need to improve the read performance of the application by offloading read traffic from the primary database instance. You want to implement a solution that minimizes effort and cost. What should you do?

あなたは、Cloud SQLデータベースにデータを保存するWebアプリケーションを管理しています。プライマリデータベースインスタンスから読み取りトラフィックをオフロードすることで、アプリケーションの読み取りパフォーマンスを向上させる必要があります。労力とコストを最小限に抑えるソリューションを実装したいと考えています。どうすべきですか？

A. Use Cloud CDN to cache frequently accessed data.
B. Store frequently accessed data in a Memorystore instance.
C. Migrate the database to a larger Cloud SQL instance.
D. Enable automatic backups, and create a read replica of the Cloud SQL instance.

**正解: D**

**解説:**
Enabling automatic backups and creating a read replica of the Cloud SQL instance is the best solution to improve read performance. Read replicas allow you to offload read traffic from the primary database instance, reducing its load and improving overall performance. This approach is cost- effective and easy to implement within Cloud SQL. It ensures that the primary instance focuses on write operations while replicas handle read queries, providing a seamless performance boost with minimal effort.

自動バックアップを有効にし、Cloud SQLインスタンスのリードレプリカを作成することが、読み取りパフォーマンスを向上させるための最良のソリューションです。リードレプリカを使用すると、プライマリデータベースインスタンスから読み取りトラフィックをオフロードでき、その負荷を軽減し、全体的なパフォーマンスを向上させることができます。このアプローチは費用対効果が高く、Cloud SQL内で簡単に実装できます。プライマリインスタンスが書き込み操作に集中し、レプリカが読み取りクエリを処理することで、最小限の労力でシームレスなパフォーマンス向上を実現します。

-----

### <a name="no10"></a>**NO.10**

Your organization has decided to migrate their existing enterprise data warehouse to BigQuery. The existing data pipeline tools already support connectors to BigQuery. You need to identify a data migration approach that optimizes migration speed. What should you do?

あなたの組織は、既存のエンタープライズデータウェアハウスをBigQueryに移行することを決定しました。既存のデータパイプラインツールは、すでにBigQueryへのコネクタをサポートしています。移行速度を最適化するデータ移行アプローチを特定する必要があります。どうすべきですか？

A. Create a temporary file system to facilitate data transfer from the existing environment to Cloud Storage. Use Storage Transfer Service to migrate the data into BigQuery.
B. Use the Cloud Data Fusion web interface to build data pipelines. Create a directed acyclic graph (DAG) that facilitates pipeline orchestration.
C. Use the existing data pipeline tool's BigQuery connector to reconfigure the data mapping.
D. Use the BigQuery Data Transfer Service to recreate the data pipeline and migrate the data into BigQuery.

**正解: C**

**解説:**
Since your existing data pipeline tools already support connectors to BigQuery, the most efficient approach is touse the existing data pipeline tool's BigQuery connectorto reconfigure the data mapping. This leverages your current tools, reducing migration complexity and setup time, while optimizing migration speed. By reconfiguring the data mapping within the existing pipeline, you can seamlessly direct the data into BigQuery without needing additional services or intermediary steps.

既存のデータパイプラインツールはすでにBigQueryへのコネクタをサポートしているため、最も効率的なアプローチは、既存のデータパイプラインツールのBigQueryコネクタを使用してデータマッピングを再構成することです。これにより、現在のツールを活用し、移行の複雑さと設定時間を短縮し、移行速度を最適化します。既存のパイプライン内でデータマッピングを再構成することにより、追加のサービスや中間ステップを必要とせずに、データをシームレスにBigQueryに送ることができます。

-----

はい、承知いたしました。
No.11からNo.50まで、ご指定のフォーマットで引き続き出力します。

-----

### <a name="no11"></a>**NO.11**

You manage an ecommerce website that has a diverse range of products. You need to forecast future product demand accurately to ensure that your company has sufficient inventory to meet customer needs and avoid stockouts. Your company's historical sales data is stored in a BigQuery table. You need to create a scalable solution that takes into account the seasonality and historical data to predict product demand. What should you do?

あなたは、多様な製品を持つeコマースウェブサイトを管理しています。顧客のニーズを満たし、在庫切れを避けるために十分な在庫を確保するため、将来の製品需要を正確に予測する必要があります。あなたの会社の過去の販売データはBigQueryテーブルに保存されています。季節性と過去のデータを考慮して製品需要を予測する、スケーラブルなソリューションを作成する必要があります。どうすべきですか？

A. Use the historical sales data to train and create a BigQuery ML time series model. Use the ML. FORECAST function call to output the predictions into a new BigQuery table.
B. Use Colab Enterprise to create a Jupyter notebook. Use the historical sales data to train a custom prediction model in Python.
C. Use the historical sales data to train and create a BigQuery ML linear regression model. Use the ML. PREDICT function call to output the predictions into a new BigQuery table.
D. Use the historical sales data to train and create a BigQuery ML logistic regression model. Use the ML.PREDICT function call to output the predictions into a new BigQuery table.

**正解: A**

**解説:**
Comprehensive and Detailed In--Depth Explanation:
Forecasting product demand with seasonality requires a time series model, and BigQuery ML offers a scalable, serverless solution. Let's analyze:

  * Option A: BigQuery ML's time series models (e.g., ARIMA_PLUS) are designed for forecasting with seasonality and trends. The ML.FORECAST function generates predictions based on historical data, storing them in a table. This is scalable (no infrastructure) and integrates natively with BigQuery, ideal for ecommerce demand prediction.
  * Option B: Colab Enterprise with a custom Python model (e.g., Prophet) is flexible but requires coding, maintenance, and potentially exporting data, reducing scalability compared to BigQuery ML's in-place processing.
  * Option C: Linear regression predicts continuous values but doesn't handle seasonality or time series patterns effectively, making it unsuitable for demand forecasting.

包括的で詳細な解説：
季節性を考慮した製品需要の予測には時系列モデルが必要であり、BigQuery MLはスケーラブルでサーバーレスなソリューションを提供します。分析してみましょう：

  * 選択肢A: BigQuery MLの時系列モデル（例：ARIMA_PLUS）は、季節性やトレンドを伴う予測のために設計されています。ML.FORECAST関数は、過去のデータに基づいて予測を生成し、それらをテーブルに保存します。これはスケーラブル（インフラ不要）であり、BigQueryとネイティブに統合されているため、eコマースの需要予測に理想的です。
  * 選択肢B: Colab EnterpriseとカスタムPythonモデル（例：Prophet）は柔軟ですが、コーディング、メンテナンス、およびデータの潜在的なエクスポートが必要であり、BigQuery MLのインプレース処理と比較してスケーラビリティが低下します。
  * 選択肢C: 線形回帰は連続値を予測しますが、季節性や時系列パターンを効果的に処理しないため、需要予測には不向きです。

-----

### <a name="no12"></a>**NO.12**

You are developing a data ingestion pipeline to load small CSV files into BigQuery from Cloud Storage. You want to load these files upon arrival to minimize data latency. You want to accomplish this with minimal cost and maintenance. What should you do?

あなたは、Cloud Storageから小さなCSVファイルをBigQueryにロードするためのデータ取り込みパイプラインを開発しています。データの遅延を最小限に抑えるために、これらのファイルが到着次第ロードしたいと考えています。これを最小限のコストとメンテナンスで実現したいです。どうすべきですか？

A. Use the bq command-line tool within a Cloud Shell instance to load the data into BigQuery.
B. Create a Cloud Composer pipeline to load new files from Cloud Storage to BigQuery and schedule it to run every 10 minutes.
C. Create a Cloud Run function to load the data into BigQuery that is triggered when data arrives in Cloud Storage.
D. Create a Dataproc cluster to pull CSV files from Cloud Storage, process them using Spark, and write the results to BigQuery.

**正解: C**

**解説:**
Using aCloud Run functiontriggered by Cloud Storage to load the data into BigQuery is the best solution because it minimizes both cost and maintenance while providing low-latency data ingestion. Cloud Run is a serverless platform that automatically scales based on the workload, ensuring efficient use of resources without requiring a dedicated instance or cluster. It integrates seamlessly with Cloud Storage event notifications, enabling real-time processing of incoming files and loading them into BigQuery. This approach is cost-effective, scalable, and easy to manage.

Cloud StorageによってトリガーされるCloud Run関数を使用してデータをBigQueryにロードすることは、コストとメンテナンスの両方を最小限に抑えつつ、低遅延のデータ取り込みを提供するため、最良のソリューションです。Cloud Runは、ワークロードに基づいて自動的にスケーリングするサーバーレスプラットフォームであり、専用のインスタンスやクラスタを必要とせずにリソースの効率的な使用を保証します。Cloud Storageのイベント通知とシームレスに統合され、受信ファイルのリアルタイム処理とBigQueryへのロードを可能にします。このアプローチは費用対効果が高く、スケーラブルで、管理が容易です。

-----

### <a name="no13"></a>**NO.13**

Your organization's business analysts require near real-time access to streaming data. However, they are reporting that their dashboard queries are loading slowly. After investigating BigQuery query performance, you discover the slow dashboard queries perform several joins and aggregations. You need to improve the dashboard loading time and ensure that the dashboard data is as up-to-date as possible. What should you do?

あなたの組織のビジネスアナリストは、ストリーミングデータへのほぼリアルタイムのアクセスを必要としています。しかし、彼らはダッシュボードのクエリの読み込みが遅いと報告しています。BigQueryのクエリパフォーマンスを調査したところ、遅いダッシュボードのクエリが複数の結合と集計を実行していることがわかりました。ダッシュボードの読み込み時間を改善し、ダッシュボードのデータが可能な限り最新であることを保証する必要があります。どうすべきですか？

A. Disable BiqQuery query result caching.
B. Modify the schema to use parameterized data types.
C. Create a scheduled query to calculate and store intermediate results.
D. Create materialized views.

**正解: D**

**解説:**
Creatingmaterialized viewsis the best solution to improve dashboard loading time while ensuring that the data is as up-to-date as possible. Materialized views precompute and cache the results of complex joins and aggregations, significantly reducing query execution time for dashboards. They also automatically update as the underlying data changes, ensuring near real-time access to fresh data. This approach optimizes query performance and provides an efficient and scalable solution for streaming data dashboards.

マテリアライズドビューを作成することは、ダッシュボードの読み込み時間を改善し、データが可能な限り最新であることを保証するための最良のソリューションです。マテリアライズドビューは、複雑な結合や集計の結果を事前に計算してキャッシュするため、ダッシュボードのクエリ実行時間を大幅に短縮します。また、基になるデータが変更されると自動的に更新されるため、最新データへのほぼリアルタイムのアクセスが保証されます。このアプローチは、クエリのパフォーマンスを最適化し、ストリーミングデータダッシュボードのための効率的でスケーラブルなソリューションを提供します。

-----

### <a name="no14"></a>**NO.14**

Your organization consists of two hundred employees on five different teams. The leadership team is concerned that any employee can move or delete all Looker dashboards saved in the Shared folder. You need to create an easy-to-manage solution that allows the five different teams in your organization to view content in the Shared folder, but only be able to move or delete their team- specific dashboard. What should you do?

あなたの組織は5つの異なるチームに所属する200人の従業員で構成されています。リーダーシップチームは、どの従業員でも共有フォルダに保存されているすべてのLookerダッシュボードを移動または削除できることを懸念しています。あなたの組織の5つの異なるチームが共有フォルダのコンテンツを閲覧できるが、チーム固有のダッシュボードのみを移動または削除できるようにする、管理しやすいソリューションを作成する必要があります。どうすべきですか？

A. 1. Create Looker groups representing each of the five different teams, and add users to their corresponding group. 2. Create five subfolders inside the Shared folder. Grant each group the View access level to their corresponding subfolder.
B. 1. Move all team-specific content into the dashboard owner s personal folder. 2. Change the access level of the Shared folder to View for the All Users group. 3. Instruct each user to create content for their team in the user's personal folder.
C. 1. Change the access level of the Shared folder to View for the All Users group. 2. Create Looker groups representing each of the five different teams, and add users to their corresponding group. 3. Create five subfolders inside the Shared folder. Grant each group the Manage Access, Edit access level to their corresponding subfolder.
D. 1. Change the access level of the Shared folder to View for the All Users group. 2. Create five subfolders inside the Shared folder. Grant each team member the Manage Access, Edit access level to their corresponding subfolder.

**正解: C**

**解説:**
Comprehensive and Detailed in Depth Explanation:
Why C is correct:Setting the Shared folder to "View" ensures everyone can see the content. Creating Looker groups simplifies access management. Subfolders allow granular permissions for each team. Granting "Manage Access, Edit" allows teams to modify only their own content.
Why other options are incorrect:A: Grants View access only, so teams can't edit.
B: Moving content to personal folders defeats the purpose of sharing.
D: Grants edit access to all members of the team, not the team as a whole, which is not ideal.

包括的で詳細な解説：
Cが正しい理由：共有フォルダを「閲覧」に設定すると、全員がコンテンツを閲覧できるようになります。Lookerグループを作成すると、アクセス管理が簡素化されます。サブフォルダにより、各チームに詳細な権限を設定できます。「アクセス管理、編集」権限を付与すると、チームは自分たちのコンテンツのみを変更できます。
他の選択肢が不適切な理由：A：閲覧アクセスのみを付与するため、チームは編集できません。
B：コンテンツを個人フォルダに移動すると、共有の目的が損なわれます。
D：チーム全体ではなく、チームの全メンバーに編集アクセスを付与するため、理想的ではありません。

-----

### <a name="no15"></a>**NO.15**

You need to create a data pipeline for a new application. Your application will stream data that needs to be enriched and cleaned. Eventually, the data will be used to train machine learning models. You need to determine the appropriate data manipulation methodology and which Google Cloud services to use in this pipeline. What should you choose?

新しいアプリケーションのためのデータパイプラインを作成する必要があります。アプリケーションは、エンリッチ化とクレンジングが必要なデータをストリーミングします。最終的に、データは機械学習モデルのトレーニングに使用されます。このパイプラインで適切なデータ操作方法論と使用すべきGoogle Cloudサービスを決定する必要があります。何を選択すべきですか？

A. ETL; Dataflow -> BigQuery
B. ETL: Cloud Data Fusion -> Cloud Storage
C. ELT; Cloud Storage -> Bigtable
D. ELT: Cloud SQL -> Analytics Hub

**正解: A**

**解説:**
Comprehensive and Detailed In-Depth Explanation:
Streaming data requiring enrichment and cleaning before ML training suggests an ETL (Extract, Transform, Load) approach, with a focus on real-time processing and a data warehouse for ML.

  * Option A: ETL with Dataflow (streaming transformations) and BigQuery (storage/ML training) is Google's recommended pattern for streaming pipelines. Dataflow handles enrichment/cleaning, and BigQuery supports ML model training (BigQuery ML).
  * Option B: ETL with Cloud Data Fusion to Cloud Storage is batch-oriented and lacks streaming focus. Cloud Storage isn't ideal for ML training directly.
  * Option C: ELT (load then transform) with Cloud Storage to Bigtable is misaligned-Bigtable is for NoSQL, not ML training or post-load transformation.

包括的で詳細な解説：
MLトレーニングの前にエンリッチ化とクレンジングを必要とするストリーミングデータは、リアルタイム処理とML用データウェアハウスに焦点を当てたETL（抽出、変換、ロード）アプローチを示唆しています。

  * 選択肢A：Dataflow（ストリーミング変換）とBigQuery（ストレージ/MLトレーニング）を使用したETLは、ストリーミングパイプラインに対するGoogleの推奨パターンです。Dataflowはエンリッチ化/クレンジングを処理し、BigQueryはMLモデルのトレーニング（BigQuery ML）をサポートします。
  * 選択肢B：Cloud Data FusionからCloud StorageへのETLはバッチ指向であり、ストリーミングに焦点が合っていません。Cloud Storageは直接MLトレーニングには理想的ではありません。
  * 選択肢C：Cloud StorageからBigtableへのELT（ロードしてから変換）は不適切です。BigtableはNoSQL用であり、MLトレーニングやロード後の変換用ではありません。

-----

### <a name="no16"></a>**NO.16**

You need to design a data pipeline that ingests data from CSV, Avro, and Parquet files into Cloud Storage. The data includes raw user input. You need to remove all malicious SQL injections before storing the data in BigQuery. Which data manipulation methodology should you choose?

あなたは、CSV、Avro、ParquetファイルからCloud Storageにデータを取り込むデータパイプラインを設計する必要があります。データには未加工のユーザー入力が含まれています。BigQueryにデータを保存する前に、すべての悪意のあるSQLインジェクションを削除する必要があります。どのデータ操作方法論を選択すべきですか？

A. EL
B. ELT
C. ETL
D. ETLT

**正解: C**

**解説:**
The ETL (Extract, Transform, Load) methodology is the best approach for this scenario because it allows you to extract data from the files, transform it by applying the necessary data cleansing (including removing malicious SQL injections), and then load the sanitized data into BigQuery. By transforming the data before loading it into BigQuery, you ensure that only clean and safe data is stored, which is critical for security and data quality.

ETL（抽出、変換、ロード）方法論は、このシナリオに最適なアプローチです。なぜなら、ファイルからデータを抽出し、必要なデータクレンジング（悪意のあるSQLインジェクションの削除を含む）を適用して変換し、その後サニタイズされたデータをBigQueryにロードできるからです。データをBigQueryにロードする前に変換することで、クリーンで安全なデータのみが保存されることを保証し、これはセキュリティとデータ品質にとって重要です。

-----

### <a name="no17"></a>**NO.17**

You are a data analyst working with sensitive customer data in BigQuery. You need to ensure that only authorized personnel within your organization can query this data, while following the principle of least privilege. What should you do?

あなたは、BigQueryで機密性の高い顧客データを扱うデータアナリストです。最小権限の原則に従いながら、組織内の承認された担当者のみがこのデータをクエリできるようにする必要があります。どうすべきですか？

A. Enable access control by using IAM roles.
B. Update dataset privileges by using the SQL GRANT statement.
C. Export the data to Cloud Storage, and use signed URLs to authorize access.
D. Encrypt the data by using customer-managed encryption keys (CМЕК).

**正解: A**

**解説:**
Comprehensive and Detailed In-Depth Explanation:
BigQuery uses IAM for access control, adhering to the principle of least privilege by granting only necessary permissions.

  * Option A: IAM roles (e.g., roles/bigquery.dataViewer for read-only) restrict query access to authorized users, aligning with Google's security best practices.
  * Option B: BigQuery doesn't support SQL GRANT for dataset privileges; access is managed via IAM or authorized views.
  * Option C: Exporting to Cloud Storage with signed URLs bypasses BigQuery's native controls and adds complexity.

包括的で詳細な解説：
BigQueryはIAMを使用してアクセス制御を行い、必要な権限のみを付与することで最小権限の原則を遵守します。

  * 選択肢A：IAMロール（例：読み取り専用のroles/bigquery.dataViewer）は、承認されたユーザーへのクエリアクセスを制限し、Googleのセキュリティベストプラクティスに沿っています。
  * 選択肢B：BigQueryはデータセット権限に対するSQL GRANTをサポートしていません。アクセスはIAMまたは承認済みビューを介して管理されます。
  * 選択肢C：署名付きURLを使用してCloud Storageにエクスポートすると、BigQueryのネイティブな制御がバイパスされ、複雑さが増します。

-----

### <a name="no18"></a>**NO.18**

Your team wants to create a monthly report to analyze inventory data that is updated daily. You need to aggregate the inventory counts by using only the most recent month of data, and save the results to be used in a Looker Studio dashboard. What should you do?

あなたのチームは、毎日更新される在庫データを分析するための月次レポートを作成したいと考えています。最新月のデータのみを使用して在庫数を集計し、その結果をLooker Studioダッシュボードで使用できるように保存する必要があります。どうすべきですか？

A. Create a materialized view in BigQuery that uses the SUM() function and the DATE_SUB() function.
B. Create a saved query in the BigQuery console that uses the SUM() function and the DATE_SUB() function. Re-run the saved query every month, and save the results to a BigQuery table.
C. Create a BigQuery table that uses the SUM() function and the_PARTITIONDATE filter.
D. Create a BigQuery table that uses the SUM() function and the DATE_DIFF() function.

**正解: A**

**解説:**
Creating amaterialized view in BigQuerywith theSUM() function and theDATE_SUB() function is the best approach. Materialized views allow you to pre-aggregate and cache query results, making them efficient for repeated access, such as monthly reporting. By using theDATE_SUB() function, you can filter the inventory data to include only the most recent month. This approach ensures that the aggregation is up-to-date with minimal latency and provides efficient integration with Looker Studio for dashboarding.

SUM()関数とDATE_SUB()関数を使用してBigQueryにマテリアライズドビューを作成することが最善のアプローチです。マテリアライズドビューを使用すると、クエリ結果を事前に集計してキャッシュできるため、月次レポートなどの繰り返しアクセスに効率的です。DATE_SUB()関数を使用することで、在庫データをフィルタリングして最新月のデータのみを含めることができます。このアプローチにより、集計が最小限の遅延で最新の状態に保たれ、Looker Studioとの効率的な統合が可能になり、ダッシュボード作成に役立ちます。

-----

### <a name="no19"></a>**NO.19**

You manage a Cloud Storage bucket that stores temporary files created during data processing. These temporary files are only needed for seven days, after which they are no longer needed. To reduce storage costs and keep your bucket organized, you want to automatically delete these files once they are older than seven days. What should you do?

あなたは、データ処理中に作成される一時ファイルを保存するCloud Storageバケットを管理しています。これらの一時ファイルは7日間だけ必要で、その後は不要になります。ストレージコストを削減し、バケットを整理された状態に保つために、7日を過ぎたこれらのファイルを自動的に削除したいと考えています。どうすべきですか？

A. Set up a Cloud Scheduler job that invokes a weekly Cloud Run function to delete files older than seven days.
B. Configure a Cloud Storage lifecycle rule that automatically deletes objects older than seven days.
C. Develop a batch process using Dataflow that runs weekly and deletes files based on their age.
D. Create a Cloud Run function that runs daily and deletes files older than seven days.

**正解: B**

**解説:**
Configuring aCloud Storage lifecycle ruleto automatically delete objects older than seven days is the best solution because:

  * Built-in feature: Cloud Storage lifecycle rules are specifically designed to manage object lifecycles, such as automatically deleting or transitioning objects based on age.
  * No additional setup: It requires no external services or custom code, reducing complexity and maintenance.
  * Cost-effective: It directly achieves the goal of deleting files after seven days without incurring additional compute costs.

7日を過ぎたオブジェクトを自動的に削除するCloud Storageのライフサイクルルールを設定することが最善の解決策です。なぜなら：

  * 組み込み機能：Cloud Storageのライフサイクルルールは、年齢に基づいてオブジェクトを自動的に削除または移行するなど、オブジェクトのライフサイクルを管理するために特別に設計されています。
  * 追加設定不要：外部サービスやカスタムコードを必要としないため、複雑さとメンテナンスが軽減されます。
  * 費用対効果：追加の計算コストを発生させることなく、7日後にファイルを削除するという目標を直接達成します。

-----

### <a name="no20"></a>**NO.20**

Your company is building a near real-time streaming pipeline to process JSON telemetry data from small appliances. You need to process messages arriving at a Pub/Sub topic, capitalize letters in the serial number field, and write results to BigQuery. You want to use a managed service and write a minimal amount of code for underlying transformations. What should you do?

あなたの会社は、小型家電からのJSONテレメトリデータを処理するために、ほぼリアルタイムのストリーミングパイプラインを構築しています。Pub/Subトピックに到着するメッセージを処理し、シリアル番号フィールドの文字を大文字に変換し、結果をBigQueryに書き込む必要があります。マネージドサービスを使用し、基盤となる変換のためのコードは最小限に抑えたいと考えています。どうすべきですか？

A. Use a Pub/Sub to BigQuery subscription, write results directly to BigQuery, and schedule a transformation query to run every five minutes.
B. Use a Pub/Sub to Cloud Storage subscription, write a Cloud Run service that is triggered when objects arrive in the bucket, performs the transformations, and writes the results to BigQuery.
C. Use the "Pub/Sub to BigQuery" Dataflow template with a UDF, and write the results to BigQuery.
D. Use a Pub/Sub push subscription, write a Cloud Run service that accepts the messages, performs the transformations, and writes the results to BigQuery.

**正解: C**

**解説:**
Using the"Pub/Sub to BigQuery" Dataflow template with a UDF (User-Defined Function)is the optimal choice because it combines near real-time processing, minimal code for transformations, and scalability. The UDF allows for efficient implementation of custom transformations, such as capitalizing letters in the serial number field, while Dataflow handles the rest of the managed pipeline seamlessly.

「Pub/Sub to BigQuery」DataflowテンプレートとUDF（ユーザー定義関数）を使用することが最適な選択です。なぜなら、ほぼリアルタイムの処理、変換のための最小限のコード、スケーラビリティを兼ね備えているからです。UDFを使用すると、シリアル番号フィールドの文字を大文字にするなどのカスタム変換を効率的に実装でき、残りの管理されたパイプラインはDataflowがシームレスに処理します。

-----

### <a name="no21"></a>**NO.21**

You are migrating data from a legacy on-premises MySQL database to Google Cloud. The database contains various tables with different data types and sizes, including large tables with millions of rows and transactional data. You need to migrate this data while maintaining data integrity, and minimizing downtime and cost. What should you do?

あなたは、レガシーなオンプレミスのMySQLデータベースからGoogle Cloudへデータを移行しています。データベースには、数百万行を持つ大きなテーブルやトランザクションデータを含む、さまざまなデータ型とサイズの多様なテーブルが含まれています。データの完全性を維持し、ダウンタイムとコストを最小限に抑えながら、このデータを移行する必要があります。どうすべきですか？

A. Set up a Cloud Composer environment to orchestrate a custom data pipeline. Use a Python script to extract data from the MySQL database and load it to MySQL on Compute Engine.
B. Export the MySQL database to CSV files, transfer the files to Cloud Storage by using Storage Transfer Service, and load the files into a Cloud SQL for MySQL instance.
C. Use Database Migration Service to replicate the MySQL database to a Cloud SQL for MySQL instance.
D. Use Cloud Data Fusion to migrate the MySQL database to MySQL on Compute Engine.

**正解: C**

**解説:**
Using Database Migration Service (DMS) to replicate the MySQL database to a Cloud SQL for MySQL instance is the best approach. DMS is a fully managed service designed for migrating databases to Google Cloud with minimal downtime and cost. It supports continuous data replication, ensuring data integrity during the migration process, and handles schema and data transfer efficiently. This solution is particularly suited for large tables and transactional data, as it maintains real-time synchronization between the source and target databases, minimizing downtime for the migration.

Database Migration Service（DMS）を使用してMySQLデータベースをCloud SQL for MySQLインスタンスにレプリケートすることが最善のアプローチです。DMSは、最小限のダウンタイムとコストでデータベースをGoogle Cloudに移行するために設計されたフルマネージドサービスです。継続的なデータレプリケーションをサポートし、移行プロセス中のデータ完全性を保証し、スキーマとデータの転送を効率的に処理します。このソリューションは、ソースデータベースとターゲットデータベース間のリアルタイム同期を維持し、移行のダウンタイムを最小限に抑えるため、大きなテーブルやトランザクションデータに特に適しています。

-----

### <a name="no22"></a>**NO.22**

Your organization uses scheduled queries to perform transformations on data stored in BigQuery. You discover that one of your scheduled queries has failed. You need to troubleshoot the issue as quickly as possible. What should you do?

あなたの組織は、スケジュールされたクエリを使用してBigQueryに保存されているデータの変換を実行しています。スケジュールされたクエリの1つが失敗したことを発見しました。できるだけ早く問題をトラブルシューティングする必要があります。どうすべきですか？

A. Navigate to the Logs Explorer page in Cloud Logging. Use filters to find the failed job, and analyze the error details.
B. Set up a log sink using the gcloud CLI to export BigQuery audit logs to BigQuery. Query those logs to identify the error associated with the failed job ID.
C. Request access from your admin to the BigQuery information_schema. Query the jobs view with the failed job ID, and analyze error details.
D. Navigate to the Scheduled queries page in the Google Cloud console. Select the failed job, and analyze the error details.

**正解: D**

**解説:**
The most direct and quickest way to troubleshoot a failed scheduled query is to go to the purpose-built UI for it. The Scheduled queries page in the Google Cloud console provides a direct view of all scheduled queries, their run history, and their status (success or failure). For any failed run, this page offers a direct link to the specific error messages and job details, eliminating the need to search or filter through broader tools like Cloud Logging or write custom queries against INFORMATION_SCHEMA.

スケジュールされたクエリの失敗を最も直接的かつ迅速にトラブルシューティングする方法は、その目的のために構築されたUIにアクセスすることです。Google Cloudコンソールの**[スケジュールされたクエリ]** ページでは、すべてのスケジュールされたクエリ、その実行履歴、およびステータス（成功または失敗）を直接表示できます。失敗した実行については、このページから特定のエラーメッセージとジョブ詳細への直接リンクが提供されるため、Cloud Loggingのような広範なツールで検索やフィルタリングを行ったり、INFORMATION_SCHEMAに対してカスタムクエリを作成したりする必要がありません。

-----

### <a name="no23"></a>**NO.23**

You work for an online retail company. Your company collects customer purchase data in CSV files and pushes them to Cloud Storage every 10 minutes. The data needs to be transformed and loaded intoBigQuery for analysis. The transformation involves cleaning the data, removing duplicates, and enriching it with product information from a separate table in BigQuery. You need to implement a low-overhead solution that initiates data processing as soon as the files are loaded into Cloud Storage. What should you do?

あなたはオンライン小売会社で働いています。あなたの会社は顧客の購入データをCSVファイルで収集し、10分ごとにCloud Storageにプッシュしています。データは変換され、分析のためにBigQueryにロードされる必要があります。変換には、データのクレンジング、重複の削除、およびBigQueryの別テーブルからの製品情報によるエンリッチ化が含まれます。ファイルがCloud Storageにロードされるとすぐにデータ処理を開始する、低オーバーヘッドなソリューションを実装する必要があります。どうすべきですか？

A. Use Cloud Composer sensors to detect files loading in Cloud Storage. Create a Dataproc cluster, and use a Composer task to execute a job on the cluster to process and load the data into BigQuery.
B. Schedule a direct acyclic graph (DAG) in Cloud Composer to run hourly to batch load the data from Cloud Storage to BigQuery, and process the data in BigQuery using SQL.
C. Use Dataflow to implement a streaming pipeline using anOBJECT_FINALIZEnotification from Pub /Sub to read the data from Cloud Storage, perform the transformations, and write the data to BigQuery.
D. Create a Cloud Data Fusion job to process and load the data from Cloud Storage into BigQuery. Create anOBJECT_FINALIZE notification in Pub/Sub, and trigger a Cloud Run function to start the Cloud Data Fusion job as soon as new files are loaded.

**正解: C**

**解説:**
Using Dataflowto implement a streaming pipeline triggered by anOBJECT_FINALIZEnotification from Pub /Sub is the best solution. This approach automatically starts the data processing as soon as new files are uploaded to Cloud Storage, ensuring low latency. Dataflow can handle the data cleaning, deduplication, and enrichment with product information from the BigQuery table in a scalable and efficient manner. This solution minimizes overhead, as Dataflow is a fully managed service, and it is well-suited for real-time or near-real-time data pipelines.

Pub/SubからのOBJECT_FINALIZE通知によってトリガーされるストリーミングパイプラインをDataflowで実装することが最善の解決策です。このアプローチは、新しいファイルがCloud Storageにアップロードされるとすぐにデータ処理を自動的に開始し、低遅延を保証します。Dataflowは、BigQueryテーブルからの製品情報によるデータクレンジング、重複排除、エンリッチ化をスケーラブルかつ効率的な方法で処理できます。このソリューションは、Dataflowがフルマネージドサービスであるためオーバーヘッドを最小限に抑え、リアルタイムまたはほぼリアルタイムのデータパイプラインに適しています。

-----

### <a name="no24"></a>**NO.24**

You are working on a project that requires analyzing daily social media data. You have 100 GB of JSON formatted data stored in Cloud Storage that keeps growing. You need to transform and load this data into BigQuery for analysis. You want to follow the Google- recommended approach. What should you do?

あなたは、日々のソーシャルメディアデータを分析する必要があるプロジェクトに取り組んでいます。Cloud Storageには100GBのJSON形式のデータが保存されており、増え続けています。このデータを変換し、分析のためにBigQueryにロードする必要があります。Googleが推奨するアプローチに従いたいと考えています。どうすべきですか？

A. Manually download the data from Cloud Storage. Use a Python script to transform and upload the data into BigQuery.
B. Use Cloud Run functions to transform and load the data into BigQuery.
C. Use Dataflow to transform the data and write the transformed data to BigQuery.
D. Use Cloud Data Fusion to transfer the data into BigQuery raw tables, and use SQL to transform it.

**正解: C**

**解説:**
Comprehensive and Detailed in Depth Explanation:
Why C is correct: Dataflow is a fully managed service for transforming and enriching data in both batch and streaming modes. Dataflow is googles recomended way to transform large datasets. It is designed for parallel processing, making it suitable for large datasets.
Why other options are incorrect:A: Manual downloading and scripting is not scalable or efficient.
B: Cloud Run functions are for stateless applications, not large data transformations.
D: While Cloud Data fusion could work, Dataflow is more optimized for large scale data transformation.

包括的で詳細な解説：
Cが正しい理由：Dataflowは、バッチモードとストリーミングモードの両方でデータを変換・エンリッチ化するためのフルマネージドサービスです。Dataflowは、Googleが大規模なデータセットを変換するために推奨する方法です。並列処理用に設計されており、大規模なデータセットに適しています。
他の選択肢が不適切な理由：A：手動でのダウンロードとスクリプト作成は、スケーラブルでも効率的でもありません。
B：Cloud Run関数はステートレスアプリケーション用であり、大規模なデータ変換用ではありません。
D：Cloud Data Fusionも機能しますが、Dataflowの方が大規模なデータ変換に最適化されています。

-----

### <a name="no25"></a>**NO.25**

Your organization stores highly personal data in BigQuery and needs to comply with strict data privacy regulations. You need to ensure that sensitive data values are rendered unreadable whenever an employee leaves the organization. What should you do?

あなたの組織は、非常に個人的なデータをBigQueryに保存しており、厳格なデータプライバシー規制に準拠する必要があります。従業員が退職するたびに、機密データ値が読み取り不能になるようにする必要があります。どうすべきですか？

A. Use AEAD functions and delete keys when employees leave the organization.
B. Use dynamic data masking and revoke viewer permissions when employees leave the organization.
C. Use customer-managed encryption keys (CMEK) and delete keys when employees leave the organization.
D. Use column-level access controls with policy tags and revoke viewer permissions when employees leave the organization.

**正解: C**

**解説:**
Using customer-managed encryption keys (CMEK) allows you to encrypt highly sensitive data in BigQuery with encryption keys managed by your organization. When an employee leaves the organization, you can render the data unreadable by deleting or revoking access to the encryption keys associated with the data. This approach ensures compliance with strict data privacy regulations by making the data inaccessible without the encryption keys, providing strong control over data access and security.

顧客管理の暗号化キー（CMEK）を使用すると、組織が管理する暗号化キーでBigQuery内の機密性の高いデータを暗号化できます。従業員が退職した場合、データに関連付けられた暗号化キーへのアクセスを削除または取り消すことで、データを読み取り不能にすることができます。このアプローチは、暗号化キーなしではデータにアクセスできなくすることで厳格なデータプライバシー規制への準拠を保証し、データアクセスとセキュリティに対する強力な制御を提供します。

-----

### <a name="no26"></a>**NO.26**

Your organization has several datasets in BigQuery. The datasets need to be shared with your external partners so that they can run SQL queries without needing to copy the data to their own projects. You have organized each partner's data in its own BigQuery dataset. Each partner should be able to access only their data. You want to share the data while following Google-recommended practices. What should you do?

あなたの組織はBigQueryに複数のデータセットを持っています。外部パートナーが自分のプロジェクトにデータをコピーすることなくSQLクエリを実行できるように、データセットを共有する必要があります。あなたは各パートナーのデータを独自のBigQueryデータセットに整理しました。各パートナーは自分のデータにのみアクセスできるべきです。Googleが推奨するプラクティスに従ってデータを共有したいと考えています。どうすべきですか？

A. Use Analytics Hub to create a listing on a private data exchange for each partner dataset. Allow each partner to subscribe to their respective listings.
B. Create a Dataflow job that reads from each BigQuery dataset and pushes the data into a dedicated Pub /Sub topic for each partner. Grant each partner the pubsub. subscriber IAM role.
C. Export the BigQuery data to a Cloud Storage bucket. Grant the partners the storage.objectUser IAM role on the bucket.
D. Grant the partners the bigquery.user IAM role on the BigQuery project.

**正解: A**

**解説:**
Using Analytics Hub to create a listing on a private data exchange for each partner dataset is the Google- recommended practice for securely sharing BigQuery data with external partners. Analytics Hub allows you to manage data sharing at scale, enabling partners to query datasets directly without needing to copy the data into their own projects. By creating separate listings for each partner dataset and allowing only the respective partner to subscribe, you ensure that partners can access only their specific data, adhering to the principle of least privilege. This approach is secure, efficient, and designed for scenarios involving external data sharing.

各パートナーデータセットに対してプライベートなデータ交換でリスティングをAnalytics Hubで作成することが、外部パートナーと安全にBigQueryデータを共有するためのGoogle推奨プラクティスです。Analytics Hubを使用すると、大規模なデータ共有を管理でき、パートナーは自分のプロジェクトにデータをコピーすることなくデータセットを直接クエリできます。各パートナーデータセットに個別のリスティングを作成し、それぞれのパートナーのみが購読できるようにすることで、パートナーが特定のデータにのみアクセスできるようにし、最小権限の原則を遵守します。このアプローチは安全で効率的であり、外部データ共有を含むシナリオ向けに設計されています。

-----

### <a name="no27"></a>**NO.27**

You have a BigQuery dataset containing sales data. This data is actively queried for the first 6 months. After that, the data is not queried but needs to be retained for 3 years for compliance reasons. You need to implement a data management strategy that meets access and compliance requirements, while keeping cost and administrative overhead to a minimum. What should you do?

あなたは売上データを含むBigQueryデータセットを持っています。このデータは最初の6か月間は活発にクエリされます。その後、データはクエリされませんが、コンプライアンス上の理由から3年間保持する必要があります。アクセスとコンプライアンスの要件を満たし、コストと管理オーバーヘッドを最小限に抑えるデータ管理戦略を実装する必要があります。どうすべきですか？

A. Use BigQuery long-term storage for the entire dataset. Set up a Cloud Run function to delete the data from BigQuery after 3 years.
B. Partition a BigQuery table by month. After 6 months, export the data to Coldline storage. Implement a lifecycle policy to delete the data from Cloud Storage after 3 years.
C. Set up a scheduled query to export the data to Cloud Storage after 6 months. Write a stored procedure to delete the data from BigQuery after 3 years.
D. Store all data in a single BigQuery table without partitioning or lifecycle policies.

**正解: B**

**解説:**
Partitioning the BigQuery table by month allows efficient querying of recent data for the first 6 months, reducing query costs. After 6 months, exporting the data toColdline storageminimizes storage costs for data that is rarely accessed but needs to be retained for compliance. Implementing a lifecycle policy in Cloud Storage automates the deletion of the data after 3 years, ensuring compliance while reducing administrative overhead. This approach balances cost efficiency and compliance requirements effectively.

BigQueryテーブルを月ごとにパーティション分割すると、最初の6か月間の最近のデータを効率的にクエリでき、クエリコストを削減できます。6か月後、データをColdlineストレージにエクスポートすると、めったにアクセスされないがコンプライアンスのために保持する必要があるデータのストレージコストが最小限に抑えられます。Cloud Storageでライフサイクルポリシーを実装すると、3年後にデータが自動的に削除され、管理オーバーヘッドを削減しながらコンプライアンスを確保できます。このアプローチは、コスト効率とコンプライアンス要件のバランスを効果的に取ります。

-----

### <a name="no28"></a>**NO.28**

Your organization has a petabyte of application logs stored as Parquet files in Cloud Storage. You need to quickly perform a one-time SQL-based analysis of the files and join them to data that already resides in BigQuery. What should you do?

あなたの組織は、Cloud StorageにParquetファイルとして保存されているペタバイト規模のアプリケーションログを持っています。これらのファイルに対して一度だけのSQLベースの分析を迅速に実行し、すでにBigQueryに存在するデータと結合する必要があります。どうすべきですか？

A. Create a Dataproc cluster, and write a PySpark job to join the data from BigQuery to the files in Cloud Storage.
B. Launch a Cloud Data Fusion environment, use plugins to connect to BigQuery and Cloud Storage, and use the SQL join operation to analyze the data.
C. Create external tables over the files in Cloud Storage, and perform SQL joins to tables in BigQuery to analyze the data.
D. Use the bq load command to load the Parquet files into BigQuery, and perform SQL joins to analyze the data.

**正解: C**

**解説:**
Creating external tables over the Parquet files in Cloud Storage allows you to perform SQL-based analysis and joins with data already in BigQuery without needing to load the files into BigQuery. This approach is efficient for a one-time analysis as it avoids the time and cost associated with loading large volumes of data into BigQuery. External tables provide seamless integration with Cloud Storage, enabling quick and cost- effective analysis of data stored in Parquet format.

Cloud Storage内のParquetファイルの上に外部テーブルを作成すると、ファイルをBigQueryにロードすることなく、SQLベースの分析とBigQueryにすでに存在するデータとの結合を実行できます。このアプローチは、大量のデータをBigQueryにロードする時間とコストを回避できるため、一度だけの分析には効率的です。外部テーブルはCloud Storageとのシームレスな統合を提供し、Parquet形式で保存されたデータの迅速かつ費用対効果の高い分析を可能にします。

-----

### <a name="no29"></a>**NO.29**

Your retail company wants to predict customer churn using historical purchase data stored in BigQuery. The dataset includes customer demographics, purchase history, and a label indicating whether the customer churned or not. You want to build a machine learning model to identify customers at risk of churning. You need to create and train a logistic regression model for predicting customer churn, using the customer_data table with the churned column as the target label. Which BigQuery ML query should you use?

あなたの小売会社は、BigQueryに保存されている過去の購入データを使用して顧客の離反を予測したいと考えています。データセットには、顧客の人口統計、購入履歴、および顧客が離反したかどうかを示すラベルが含まれています。離反のリスクがある顧客を特定するための機械学習モデルを構築したいと考えています。customer_dataテーブルとchurned列をターゲットラベルとして使用して、顧客離反を予測するためのロジスティック回帰モデルを作成およびトレーニングする必要があります。どのBigQuery MLクエリを使用すべきですか？

A. CREATE OR REPLACE MODEL churn_prediction_model OPTIONS (model_type='logistic reg') AS SELECT * FROM customer
B. CREATE OR REPLACE MODEL churn_prediction_model OPTIONS (model_type='logistic_reg') AS SELECT * EXCEPT (churned), churned AS label FROM customer data;
C. CREATE OR REPLACE MODEL churn_prediction_model OPTIONS (model_type='logistic_reg') AS SELECT EXCEPT (churned) FROM customer_data;
D. CREATE OR REPLACE MODEL churn_prediction_model OPTIONS (model type='logistic reg') AS SELECT churned as label FROM customer data;

**正解: B**

**解説:**
In BigQuery ML, when creating a logistic regression model to predict customer churn, the correct query should:
Exclude the target label column (in this case, churned) from the feature columns, as it is used for training and not as a feature input.
Rename the target label column to label, as BigQuery ML requires the target column to be named label.
The chosen query satisfies these requirements:
SELECT * EXCEPT (churned), churned AS label: Excludes churned from features and renames it to label.
The OPTIONS(model_type='logistic_reg') specifies that a logistic regression model is being trained.
This setup ensures the model is correctly trained using the features in the dataset while targeting the churned column for predictions.

BigQuery MLで顧客離反を予測するためのロジスティック回帰モデルを作成する場合、正しいクエリは次のようになります：
ターゲットラベル列（この場合はchurned）はトレーニングに使用され、特徴入力ではないため、特徴列から除外します。
BigQuery MLではターゲット列をlabelという名前にする必要があるため、ターゲットラベル列の名前をlabelに変更します。
選択したクエリはこれらの要件を満たしています：
SELECT * EXCEPT (churned), churned AS label：churnedを特徴から除外し、labelに名前を変更します。
OPTIONS(model_type='logistic_reg')は、ロジスティック回帰モデルがトレーニングされることを指定します。
この設定により、データセット内の特徴を使用してモデルが正しくトレーニングされ、churned列を予測のターゲットとすることが保証されます。

-----

### <a name="no30"></a>**NO.30**

Your retail company collects customer data from various sources: Online transactions: Stored in a MySQL database Customer feedback: Stored as text files on a company server Social media activity: Streamed in real-time from social media platforms You are designing a data pipeline to extract this data. Which Google Cloud storage system(s) should you select for further analysis and ML model training?

あなたの小売会社は、さまざまなソースから顧客データを収集しています：オンライン取引：MySQLデータベースに保存、顧客フィードバック：会社のサーバーにテキストファイルとして保存、ソーシャルメディアアクティビティ：ソーシャルメディアプラットフォームからリアルタイムでストリーミング。あなたはこのデータを抽出するためのデータパイプラインを設計しています。さらなる分析とMLモデルのトレーニングのために、どのGoogle Cloudストレージシステムを選択すべきですか？

A. 1. Online transactions: Cloud Storage 2. Customer feedback: Cloud Storage 3. Social media activity: Cloud Storage
B. 1. Online transactions: BigQuery 2. Customer feedback: Cloud Storage 3. Social media activity: BigQuery
C. 1. Online transactions: Bigtable 2. Customer feedback: Cloud Storage 3. Social media activity: CloudSQL for MySQL
D. 1. Online transactions: Cloud SQL for MySQL 2. Customer feedback: BigQuery 3. Social media activity: Cloud Storage

**正解: B**

**解説:**
Online transactions: Storing the transactional data inBigQueryis ideal because BigQuery is a serverless data warehouse optimized for querying and analyzing structured data at scale. It supports SQL queries and is suitable for structured transactional data.
Customer feedback: Storing customer feedback inCloud Storageis appropriate as it allows you to store unstructured text files reliably and at a low cost. Cloud Storage also integrates well with data processing and ML tools for further analysis.
Social media activity: Storing real-time social media activity inBigQueryis optimal because BigQuery supports streaming inserts, enabling real-time ingestion and analysis of data. This allows immediate analysis and integration into dashboards or ML pipelines.

オンライン取引：トランザクションデータをBigQueryに保存することが理想的です。なぜなら、BigQueryは大規模な構造化データをクエリ・分析するために最適化されたサーバーレスのデータウェアハウスだからです。SQLクエリをサポートし、構造化されたトランザクションデータに適しています。
顧客フィードバック：顧客フィードバックをCloud Storageに保存するのが適切です。非構造化テキストファイルを低コストで確実に保存できるからです。Cloud Storageは、さらなる分析のためにデータ処理やMLツールともうまく統合します。
ソーシャルメディアアクティビティ：リアルタイムのソーシャルメディアアクティビティをBigQueryに保存するのが最適です。BigQueryはストリーミングインサートをサポートしており、リアルタイムのデータ取り込みと分析を可能にするからです。これにより、即時の分析やダッシュボード、MLパイプラインへの統合が可能になります。

-----

### <a name="no31"></a>**NO.31**

You have created a LookML model and dashboard that shows daily sales metrics for five regional managers to use. You want to ensure that the regional managers can only see sales metrics specific to their region. You need an easy-to-implement solution. What should you do?

あなたは、5人の地域マネージャーが使用するために、日々の売上指標を示すLookMLモデルとダッシュボードを作成しました。地域マネージャーが自分の地域に特有の売上指標のみを閲覧できるようにする必要があります。実装が簡単なソリューションが必要です。どうすべきですか？

A. Create asales_regionuser attribute, and assign each manager's region as the value of their user attribute. Add anaccess_filterExplore filter on theregion_namedimension by using thesales_regionuser attribute.
B. Create five different Explores with thesql_always_filterExplore filter applied on theregion_namedimension. Set eachregion_namevalue to the corresponding region for each manager.
C. Create separate Looker dashboards for each regional manager. Set the default dashboard filter to the corresponding region for each manager.
D. Create separate Looker instances for each regional manager. Copy the LookML model and dashboard to each instance. Provision viewer access to the corresponding manager.

**正解: A**

**解説:**
Using asales_region user attributeis the best solution because it allows you to dynamically filter data based on each manager's assigned region. By adding anaccess_filterExplore filter on theregion_namedimension that references thesales_regionuser attribute, each manager sees only the sales metrics specific to their region. This approach is easy to implement, scalable, and avoids duplicating dashboards or Explores, making it both efficient and maintainable.

sales_regionユーザー属性を使用することが最善の解決策です。なぜなら、各マネージャーに割り当てられた地域に基づいてデータを動的にフィルタリングできるからです。sales_regionユーザー属性を参照するaccess_filterExploreフィルターをregion_nameディメンションに追加することで、各マネージャーは自分の地域に特有の売上指標のみを閲覧できます。このアプローチは実装が簡単でスケーラブルであり、ダッシュボードやExploreを複製する必要がないため、効率的で保守性も高くなります。

-----

### <a name="no32"></a>**NO.32**

You work for a gaming company that collects real-time player activity data. This data is streamed into Pub /Sub and needs to be processed and loaded into BigQuery for analysis. The processing involves filtering, enriching, and aggregating the data before loading it into partitioned BigQuery tables. Youneed to design a pipeline that ensures low latency and high throughput while following a Google- recommended approach. What should you do?

あなたはゲーム会社で働いており、リアルタイムのプレイヤーアクティビティデータを収集しています。このデータはPub/Subにストリーミングされ、分析のために処理されてBigQueryにロードされる必要があります。処理には、パーティション分割されたBigQueryテーブルにロードする前に、データのフィルタリング、エンリッチ化、集計が含まれます。Googleが推奨するアプローチに従い、低遅延と高スループットを保証するパイプラインを設計する必要があります。どうすべきですか？

A. Use Cloud Composer to orchestrate a workflow that reads the data from Pub/Sub, processes the data using a Python script, and writes it to BigQuery.
B. Use Dataproc to create an Apache Spark streaming job that reads the data from Pub/Sub, processes the data, and writes it to BigQuery.
C. Use Dataflow to create a streaming pipeline that reads the data from Pub/Sub, processes the data, and writes it to BigQuery using the streaming API.
D. Use Cloud Run functions to subscribe to the Pub/Sub topic, process the data, and write it to BigQuery using the streaming API.

**正解: C**

**解説:**
Comprehensive and Detailed in Depth Explanation:
Why C is correct: Dataflow is the recommended service for real-time stream processing on Google Cloud. It provides scalable and reliable processing with low latency and high throughput. Dataflow's streaming API is optimized for Pub/Sub integration and BigQuery streaming inserts.
Why other options are incorrect:A: Cloud Composer is for batch orchestration, not real-time streaming.
B: Dataproc and Spark streaming are more complex and not as efficient as Dataflow for this task.
D: Cloud Run functions are for stateless, event-driven applications, not continuous stream processing.

包括的で詳細な解説：
Cが正しい理由：DataflowはGoogle Cloudでのリアルタイムストリーム処理に推奨されるサービスです。低遅延と高スループットでスケーラブルかつ信頼性の高い処理を提供します。DataflowのストリーミングAPIは、Pub/Sub統合とBigQueryストリーミングインサートに最適化されています。
他の選択肢が不適切な理由：A：Cloud Composerはバッチオーケストレーション用であり、リアルタイムストリーミング用ではありません。
B：DataprocとSparkストリーミングはこのタスクにはより複雑で、Dataflowほど効率的ではありません。
D：Cloud Run関数はステートレスなイベント駆動型アプリケーション用であり、継続的なストリーム処理用ではありません。

-----

### <a name="no33"></a>**NO.33**

Your organization has several datasets in their data warehouse in BigQuery. Several analyst teams in different departments use the datasets to run queries. Your organization is concerned about the variability of their monthly BigQuery costs. You need to identify a solution that creates a fixed budget for costs associated with the queries run by each department. What should you do?

あなたの組織は、BigQueryのデータウェアハウスに複数のデータセットを持っています。異なる部門のいくつかのアナリストチームが、データセットを使用してクエリを実行します。組織は、毎月のBigQueryコストの変動を懸念しています。各部門が実行するクエリに関連するコストの固定予算を作成するソリューションを特定する必要があります。どうすべきですか？

A. Create a custom quota for each analyst in BigQuery.
B. Create a single reservation by using BigQuery editions. Assign all analysts to the reservation.
C. Assign each analyst to a separate project associated with their department. Create a single reservation by using BigQuery editions. Assign all projects to the reservation.
D. Assign each analyst to a separate project associated with their department. Create a single reservation for each department by using BigQuery editions. Create assignments for each project in the appropriate reservation.

**正解: D**

**解説:**
Assigning each analyst to a separate project associated with their department and creating asingle reservation for each departmentusing BigQuery editionsallows for precise cost management. By assigning each project to its department's reservation, you can allocate fixed compute resources and budgets for each department, ensuring that their query costs are predictable and controlled. This approach aligns with your organization's goal of creating a fixed budget for query costs while maintaining departmental separation and accountability.

各アナリストを所属部門に関連付けられた個別のプロジェクトに割り当て、BigQueryエディションを使用して各部門に単一の予約を作成することで、正確なコスト管理が可能になります。各プロジェクトをその部門の予約に割り当てることで、各部門に固定の計算リソースと予算を割り当てることができ、クエリコストが予測可能で管理されていることを保証します。このアプローチは、部門ごとの分離と説明責任を維持しながら、クエリコストの固定予算を作成するという組織の目標に合致します。

-----

### <a name="no34"></a>**NO.34**

Your company uses Looker to generate and share reports with various stakeholders. You have a complex dashboard with several visualizations that needs to be delivered to specific stakeholders on a recurring basis, with customized filters applied for each recipient. You need an efficient and scalable solution to automate the delivery of this customized dashboard. You want to follow the Google-recommended approach. What should you do?

あなたの会社は、さまざまな利害関係者とレポートを生成・共有するためにLookerを使用しています。複数のビジュアライゼーションを持つ複雑なダッシュボードがあり、各受信者ごとにカスタマイズされたフィルターを適用して、特定の利害関係者に定期的に配信する必要があります。このカスタマイズされたダッシュボードの配信を自動化するための、効率的でスケーラブルなソリューションが必要です。Googleが推奨するアプローチに従いたいと考えています。どうすべきですか？

A. Create a separate LookML model for each stakeholder with predefined filters, and schedule the dashboards using the Looker Scheduler.
B. Create a script using the Looker Python SDK, and configure user attribute filter values. Generate a new scheduled plan for each stakeholder.
C. Embed the Looker dashboard in a custom web application, and use the application's scheduling features to send the report with personalized filters.
D. Use the Looker Scheduler with a user attribute filter on the dashboard, and send the dashboard with personalized filters to each stakeholder based on their attributes.

**正解: D**

**解説:**
Using theLooker Schedulerwithuser attribute filtersis the Google-recommended approach to efficiently automate the delivery of a customized dashboard. User attribute filters allow you to dynamically customize the dashboard's content based on the recipient's attributes, ensuring each stakeholder sees data relevant to them. This approach is scalable, does not require creating separate models or custom scripts, and leverages Looker's built-in functionality to automate recurring deliveries effectively.

ユーザー属性フィルター付きのLookerスケジューラを使用することが、カスタマイズされたダッシュボードの配信を効率的に自動化するためのGoogle推奨アプローチです。ユーザー属性フィルターを使用すると、受信者の属性に基づいてダッシュボードのコンテンツを動的にカスタマイズでき、各利害関係者が自分に関連するデータを閲覧できるようになります。このアプローチはスケーラブルであり、個別のモデルやカスタムスクリプトを作成する必要がなく、Lookerの組み込み機能を活用して定期的な配信を効果的に自動化します。

-----

### <a name="no35"></a>**NO.35**

Your team uses the Google Ads platform to visualize metrics. You want to export the data to BigQuery to get more granular insights. You need to execute a one-time transfer of historical data and automatically update data daily. You want a solution that is low-code, serverless, and requires minimal maintenance. What should you do?

あなたのチームは、指標を視覚化するためにGoogle Adsプラットフォームを使用しています。より詳細な洞察を得るために、データをBigQueryにエクスポートしたいと考えています。過去のデータのワンタイム転送を実行し、毎日データを自動的に更新する必要があります。ローコードでサーバーレス、かつ最小限のメンテナンスで済むソリューションを求めています。どうすべきですか？

A. Export the historical data to BigQuery by using BigQuery Data Transfer Service. Use Cloud Composer for daily automation.
B. Export the historical data to Cloud Storage by using Storage Transfer Service. Use Pub/Sub to trigger a Dataflow template that loads data for daily automation.
C. Export the historical data as a CSV file. Import the file into BigQuery for analysis. Use Cloud Composer for daily automation.
D. Export the historical data to BigQuery by using BigQuery Data Transfer Service. Use BigQuery Data Transfer Service for daily automation.

**正解: D**

**解説:**
(Explanation combined with question D in source document. The correct answer is D, which implies both one-time and daily transfers are handled by BigQuery Data Transfer Service.)

(解説はソースドキュメントの選択肢Dに統合されています。正解はDであり、ワンタイム転送と日次転送の両方がBigQuery Data Transfer Serviceによって処理されることを意味します。)

-----

### <a name="no36"></a>**NO.36**

You have an existing weekly Storage Transfer Service transfer job from Amazon S3 to a Nearline Cloud Storage bucket in Google Cloud. Each week, the job moves a large number of relatively small files. As the number of files to be transferred each week has grown over time, you are at risk of no longer completing the transfer in the allocated time frame. You need to decrease the total transfer time by replacing the process. Your solution should minimize costs where possible. What should you do?

あなたは、Amazon S3からGoogle CloudのNearline Cloud Storageバケットへの既存の週次Storage Transfer Service転送ジョブを持っています。毎週、ジョブは多数の比較的小さなファイルを移動します。毎週転送されるファイルの数が時間とともに増加したため、割り当てられた時間内に転送を完了できなくなるリスクがあります。プロセスを置き換えることで、総転送時間を短縮する必要があります。ソリューションは可能な限りコストを最小限に抑えるべきです。どうすべきですか？

A. Create a transfer job using the Google Cloud CLI, and specify the Standard storage class with the - custom-storage-class flag.
B. Create parallel transfer jobs using include and exclude prefixes.
C. Create a batch Dataflow job that is scheduled weekly to migrate the data from Amazon S3 to Cloud Storage.
D. Create an agent-based transfer job that utilizes multiple transfer agents on Compute Engine instances.

**正解: B**

**解説:**
Comprehensive and Detailed in Depth Explanation:
Why B is correct: Creating parallel transfer jobs by using include and exclude prefixes allows you to split the data into smaller chunks and transfer them in parallel. This can significantly increase throughput and reduce the overall transfer time.
Why other options are incorrect:A: Changing the storage class to Standard will not improve transfer speed.
C: Dataflow is a complex solution for a simple file transfer task.
D: Agent-based transfer is suitable for large files or network limitations, but not for a large number of small files.

包括的で詳細な解説：
Bが正しい理由：包含および除外プレフィックスを使用して並列転送ジョブを作成すると、データをより小さなチャンクに分割して並行して転送できます。これにより、スループットが大幅に向上し、総転送時間が短縮されます。
他の選択肢が不適切な理由：A：ストレージクラスをStandardに変更しても転送速度は向上しません。
C：Dataflowは単純なファイル転送タスクには複雑なソリューションです。
D：エージェントベースの転送は、大きなファイルやネットワークの制約には適していますが、多数の小さなファイルには適していません。

-----

### <a name="no37"></a>**NO.37**

You work for an ecommerce company that has a BigQuery dataset that contains customer purchase history, demographics, and website interactions. You need to build a machine learning (ML) model to predict which customers are most likely to make a purchase in the next month. You have limited engineering resources and need to minimize the ML expertise required for the solution. What should you do?

あなたはeコマース企業で働いており、顧客の購入履歴、人口統計、ウェブサイトのインタラクションを含むBigQueryデータセットを持っています。来月購入する可能性が最も高い顧客を予測するための機械学習（ML）モデルを構築する必要があります。エンジニアリングリソースは限られており、ソリューションに必要なMLの専門知識を最小限に抑える必要があります。どうすべきですか？

A. Use BigQuery ML to create a logistic regression model for purchase prediction.
B. Use Vertex Al Workbench to develop a custom model for purchase prediction.
C. Use Colab Enterprise to develop a custom model for purchase prediction.
D. Export the data to Cloud Storage, and use AutoML Tables to build a classification model for purchase prediction.

**正解: A**

**解説:**
Using BigQuery ML is the best solution in this case because:
Ease of use: BigQuery ML allows users to build machine learning models using SQL, which requires minimal ML expertise.
Integrated platform: Since the data already exists in BigQuery, there's no need to move it to another service, saving time and engineering resources.
Logistic regression: This is an appropriate model for binary classification tasks like predicting the likelihood of a customer making a purchase in the next month.

この場合、BigQuery MLを使用することが最善の解決策です。なぜなら：
使いやすさ：BigQuery MLを使用すると、ユーザーはSQLを使用して機械学習モデルを構築でき、最小限のML専門知識で済みます。
統合プラットフォーム：データはすでにBigQueryに存在するため、別のサービスに移動する必要がなく、時間とエンジニアリングリソースを節約できます。
ロジスティック回帰：これは、来月の顧客の購入可能性を予測するような二項分類タスクに適したモデルです。

-----

### <a name="no38"></a>**NO.38**

Your organization has highly sensitive data that gets updated once a day and is stored across multiple datasets in BigQuery. You need to provide a new data analyst access to query specific data in BigQuery while preventing access to sensitive data. What should you do?

あなたの組織は、日に一度更新される機密性の高いデータをBigQueryの複数のデータセットにまたがって保存しています。新しいデータアナリストに、機密データへのアクセスを防ぎつつ、BigQuery内の特定のデータをクエリするアクセス権を提供する必要があります。どうすべきですか？

A. Grant the data analyst the BigQuery Job User IAM role in the Google Cloud project.
B. Create a materialized view with the limited data in a new dataset. Grant the data analyst BigQuery Data Viewer IAM role in the dataset and the BigQuery Job User IAM role in the Google Cloud project.
C. Create a new Google Cloud project, and copy the limited data into a BigQuery table. Grant the data analyst the BigQuery Data Owner IAM role in the new Google Cloud project.
D. Grant the data analyst the BigQuery Data Viewer IAM role in the Google Cloud project.

**正解: B**

**解説:**
Creating amaterialized viewwith the limited data in a new dataset and granting the data analyst theBigQuery Data Viewerrole on the dataset and theBigQuery Job Userrole in the project ensures that the analyst can query only the non-sensitive data without access to sensitive datasets. Materialized views allow you to predefine what subset of data is visible, providing a secure and efficient way to control access while maintaining compliance with data governance policies. This approach follows the principle of least privilege while meeting the requirements.

新しいデータセットに限定されたデータを持つマテリアライズドビューを作成し、データアナリストにそのデータセットに対するBigQueryデータ閲覧者ロールとGoogle Cloudプロジェクトに対するBigQueryジョブユーザーロールを付与することで、アナリストが機密データセットにアクセスすることなく、非機密データのみをクエリできるようにします。マテリアライズドビューを使用すると、表示されるデータのサブセットを事前に定義でき、データガバナンスポリシーへのコンプライアンスを維持しながら、安全かつ効率的にアクセスを制御できます。このアプローチは、要件を満たしつつ、最小権限の原則に従います。

-----

### <a name="no39"></a>**NO.39**

Your company has developed a website that allows users to upload and share video files. These files are most frequently accessed and shared when they are initially uploaded. Over time, the files are accessed and shared less frequently, although some old video files may remain very popular. You need to design a storage system that is simple and cost-effective. What should you do?

あなたの会社は、ユーザーがビデオファイルをアップロードして共有できるウェブサイトを開発しました。これらのファイルは、最初にアップロードされたときに最も頻繁にアクセス・共有されます。時間が経つにつれて、ファイルのアクセス・共有は少なくなりますが、古いビデオファイルの中には非常に人気が残るものもあります。シンプルで費用対効果の高いストレージシステムを設計する必要があります。どうすべきですか？

A. Create a single-region bucket with custom Object Lifecycle Management policies based on upload date.
B. Create a single-region bucket with Autoclass enabled.
C. Create a single-region bucket. Configure a Cloud Scheduler job that runs every 24 hours and changes the storage class based on upload date.
D. Create a single-region bucket with Archive as the default storage class.

**正解: B**

**解説:**
The storage system must balance cost, simplicity, and access patterns: high initial access, decreasing over time, with some files remaining popular. Google Cloud Storage offers tailored options for this:

  * Option A: Custom Object Lifecycle Management (OLM) policies (e.g., transition to Nearline after 30 days, Archive after 90 days) are effective but static. They don't adapt to actual usage, so popular old files in Archive would incur high retrieval costs.
  * Option B: Autoclass automatically adjusts storage classes (Standard, Nearline, Coldline, Archive) based on object access patterns, not just age. It keeps frequently accessed files in Standard (low latency /cost for access) and moves inactive ones to cheaper classes, minimizing costs while preserving simplicity. This fits the "some files remain popular" nuance.
  * Option C: A Cloud Scheduler job to manually change classes daily is complex (requires scripting, monitoring), error-prone, and less cost-effective than automated solutions like Autoclass or OLM.

ストレージシステムは、コスト、シンプルさ、アクセスパターン（初期のアクセスは多いが時間とともに減少し、一部のファイルは人気が続く）のバランスを取る必要があります。Google Cloud Storageは、これに適したオプションを提供しています：

  * 選択肢A：カスタムのオブジェクトライフサイクル管理（OLM）ポリシー（例：30日後にNearline、90日後にArchiveへ移行）は効果的ですが静的です。実際の使用状況に適応しないため、Archiveにある人気の古いファイルは高い取得コストが発生します。
  * 選択肢B：Autoclassは、年齢だけでなくオブジェクトのアクセスパターンに基づいてストレージクラス（Standard, Nearline, Coldline, Archive）を自動的に調整します。頻繁にアクセスされるファイルはStandard（低遅延/アクセス費用）に保持し、非アクティブなファイルはより安価なクラスに移動させることで、シンプルさを保ちながらコストを最小限に抑えます。これは「一部のファイルは人気が続く」というニュアンスに適合します。
  * 選択肢C：Cloud Schedulerジョブで毎日手動でクラスを変更するのは複雑（スクリプト作成、監視が必要）で、エラーが発生しやすく、AutoclassやOLMのような自動化ソリューションよりも費用対効果が低いです。

-----

### <a name="no40"></a>**NO.40**

Your company's customer support audio files are stored in a Cloud Storage bucket. You plan to analyze the audio files' metadata and file content within BigQuery to create inference by using BigQuery ML. You need to create a corresponding table in BigQuery that represents the bucket containing the audio files. What should you do?

あなたの会社のカスタマーサポートの音声ファイルは、Cloud Storageバケットに保存されています。BigQuery MLを使用して推論を作成するために、BigQuery内で音声ファイルのメタデータとファイルコンテンツを分析する予定です。音声ファイルを含むバケットを表す対応するテーブルをBigQueryに作成する必要があります。どうすべきですか？

A. Create an external table.
B. Create a temporary table.
C. Create a native table.
D. Create an object table.

**正解: D**

**解説:**
To analyze audio files stored in a Cloud Storage bucket and represent them in BigQuery, you should create an object table. Object tables in BigQuery are designed to represent objects stored in Cloud Storage, including their metadata. This enables you to query the metadata of audio files directly from BigQuery without duplicating the data. Once the object table is created, you can use it in conjunction with other BigQuery ML workflows for inference and analysis.

Cloud Storageバケットに保存されている音声ファイルを分析し、それらをBigQueryで表現するには、オブジェクトテーブルを作成する必要があります。BigQueryのオブジェクトテーブルは、メタデータを含め、Cloud Storageに保存されているオブジェクトを表現するように設計されています。これにより、データを複製することなく、BigQueryから直接音声ファイルのメタデータをクエリできます。オブジェクトテーブルが作成されると、推論や分析のために他のBigQuery MLワークフローと組み合わせて使用できます。

-----

### <a name="no41"></a>**NO.41**

You are designing a BigQuery data warehouse with a team of experienced SQL developers. You need to recommend a cost-effective, fully-managed, serverless solution to build ELT processes with SQL pipelines. Your solution must include source code control, environment parameterization, and data quality checks. What should you do?

あなたは、経験豊富なSQL開発者のチームと共にBigQueryデータウェアハウスを設計しています。SQLパイプラインでELTプロセスを構築するための、費用対効果が高く、フルマネージドでサーバーレスなソリューションを推奨する必要があります。ソリューションには、ソースコード管理、環境のパラメータ化、およびデータ品質チェックが含まれている必要があります。どうすべきですか？

A. Use Cloud Data Fusion to visually design and manage the pipelines.
B. Use Dataform to build, orchestrate, and monitor the pipelines.
C. Use Dataproc to run MapReduce jobs for distributed data processing.
D. Use Cloud Composer to orchestrate and run data workflows.

**正解: B**

**解説:**
Comprehensive and Detailed In-Depth Explanation:
The solution must support SQL-based ELT, be serverless and cost-effective, and include advanced features like version control and quality checks. Let's dive in:

  * Option A: Cloud Data Fusion is a visual ETL tool, not SQL-centric (uses plugins), and isn't fully serverless (requires instance management). It lacks native source code control and parameterization.
  * Option B: Dataform is a serverless, SQL-based ELT platform for BigQuery. It uses SQLX scripts, integrates with Git for version control, supports environment variables (parameterization), and offers assertions for data quality-all meeting the requirements cost-effectively.
  * Option C: Dataproc is for Spark/MapReduce, not SQL ELT, and requires cluster management, contradicting serverless and cost goals.

包括的で詳細な解説：
ソリューションはSQLベースのELTをサポートし、サーバーレスで費用対効果が高く、バージョン管理や品質チェックなどの高度な機能を含んでいる必要があります。詳しく見ていきましょう：

  * 選択肢A：Cloud Data Fusionは視覚的なETLツールであり、SQL中心ではありません（プラグインを使用）。また、完全にサーバーレスではなく（インスタンス管理が必要）、ネイティブのソースコード管理やパラメータ化機能がありません。
  * 選択肢B：Dataformは、BigQuery向けのサーバーレスでSQLベースのELTプラットフォームです。SQLXスクリプトを使用し、バージョン管理のためにGitと統合し、環境変数（パラメータ化）をサポートし、データ品質のためのアサーションを提供しており、すべての要件を費用対効果高く満たしています。
  * 選択肢C：DataprocはSpark/MapReduce用であり、SQL ELT用ではありません。また、クラスタ管理が必要で、サーバーレスとコストの目標に反します。

-----

### <a name="no42"></a>**NO.42**

You are designing an application that will interact with several BigQuery datasets. You need to grant the application's service account permissions that allow it to query and update tables within the datasets, and list all datasets in a project within your application. You want to follow the principle of least privilege. Which pre- defined IAM role(s) should you apply to the service account?

あなたは、いくつかのBigQueryデータセットと対話するアプリケーションを設計しています。アプリケーションのサービスアカウントに、データセット内のテーブルをクエリおよび更新し、アプリケーション内でプロジェクトのすべてのデータセットを一覧表示できる権限を付与する必要があります。最小権限の原則に従いたいと考えています。どの事前定義済みIAMロールをサービスアカウントに適用すべきですか？

A. roles/bigquery.jobUser and roles/bigquery.dataOwner
B. roles/bigquery.connectionUser and roles/bigquery.dataViewer
C. roles/bigquery.admin
D. roles/bigquery.user and roles/bigquery.filteredDataViewer

**正解: A**

**解説:**

  * roles/bigquery.jobUser: This role allows a user or service account to run BigQuery jobs, including queries. This is necessary for the application to interact with and query the tables.

  * roles/bigquery.dataOwner: This role grants full control over BigQuery datasets and tables. It allows the service account to update tables, which is a requirement of the application.

  * Why other options are incorrect:

      * B. roles/bigquery.connectionUser and roles/bigquery.dataViewer: roles/bigquery.connectionUser is used for external connections, which is not required for this task. roles/bigquery.dataViewer only allows viewing data, not updating it.
      * C. roles/bigquery.admin: roles/bigquery.admin grants excessive permissions. Following the principle of least privilege, this role is too broad.
      * D. roles/bigquery.user and roles/bigquery.filteredDataViewer: roles/bigquery.user grants the ability to run queries, but not the ability to modify data. roles /bigquery.filteredDataViewer only provides permission to view filtered data, which is not sufficient for updating tables.

  * Principle of Least Privilege: By assigning roles/bigquery.jobUser and roles/bigquery.dataOwner, we provide the application with the exact permissions it needs without granting unnecessary access.

  * roles/bigquery.jobUser：このロールは、ユーザーまたはサービスアカウントがクエリを含むBigQueryジョブを実行することを許可します。これは、アプリケーションがテーブルと対話し、クエリを実行するために必要です。

  * roles/bigquery.dataOwner：このロールは、BigQueryのデータセットとテーブルに対する完全な制御を許可します。これにより、サービスアカウントはテーブルを更新でき、これはアプリケーションの要件です。

  * 他の選択肢が不適切な理由：

      * B. roles/bigquery.connectionUserとroles/bigquery.dataViewer：roles/bigquery.connectionUserは外部接続に使用され、このタスクには不要です。roles/bigquery.dataViewerはデータの閲覧のみを許可し、更新は許可しません。
      * C. roles/bigquery.admin：roles/bigquery.adminは過剰な権限を付与します。最小権限の原則に従うと、このロールは広すぎます。
      * D. roles/bigquery.userとroles/bigquery.filteredDataViewer：roles/bigquery.userはクエリの実行能力を付与しますが、データの変更能力は付与しません。roles/bigquery.filteredDataViewerはフィルタリングされたデータの閲覧権限のみを提供し、テーブルの更新には不十分です。

  * 最小権限の原則：roles/bigquery.jobUserとroles/bigquery.dataOwnerを割り当てることで、不要なアクセスを許可することなく、アプリケーションが必要とする正確な権限を提供します。

-----

### <a name="no43"></a>**NO.43**

You work for a healthcare company that has a large on-premises data system containing patient records with personally identifiable information (PII) such as names, addresses, and medical diagnoses. You need a standardized managed solution that de-identifies PII across all your data feeds prior to ingestion to Google Cloud. What should you do?

あなたはヘルスケア企業で働いており、名前、住所、医療診断などの個人を特定できる情報（PII）を含む患者記録を格納した大規模なオンプレミスデータシステムを持っています。Google Cloudへの取り込み前に、すべてのデータフィードにわたってPIIを匿名化する、標準化されたマネージドソリューションが必要です。どうすべきですか？

A. Use Cloud Run functions to create a serverless data cleaning pipeline. Store the cleaned data in BigQuery.
B. Use Cloud Data Fusion to transform the data. Store the cleaned data in BigQuery.
C. Load the data into BigQuery, and inspect the data by using SQL queries. Use Dataflow to transform the data and remove any errors.
D. Use Apache Beam to read the data and perform the necessary cleaning and transformation operations.Store the cleaned data in BigQuery.

**正解: B**

**解説:**
UsingCloud Data Fusionis the best solution for this scenario because:

  * Standardized managed solution: Cloud Data Fusion provides a visual interface for building data pipelines and includes prebuilt connectors and transformations for data cleaning and de- identification.
  * Compliance: It ensures sensitive data such as PII is de-identified prior to ingestion into Google Cloud, adhering to regulatory requirements for healthcare data.
  * Ease of use: Cloud Data Fusion is designed for transforming and preparing data, making it a managed and user-friendly tool for this purpose.

このシナリオではCloud Data Fusionを使用することが最善の解決策です。なぜなら：

  * 標準化されたマネージドソリューション：Cloud Data Fusionは、データパイプラインを構築するための視覚的なインターフェースを提供し、データクレンジングと匿名化のための事前構築済みコネクタと変換を含んでいます。
  * コンプライアンス：PIIなどの機密データがGoogle Cloudに取り込まれる前に匿名化されることを保証し、ヘルスケアデータの規制要件を遵守します。
  * 使いやすさ：Cloud Data Fusionはデータの変換と準備のために設計されており、この目的のためのマネージドで使いやすいツールです。

-----

### <a name="no44"></a>**NO.44**

Your company uses Looker to visualize and analyze sales data. You need to create a dashboard that displays sales metrics, such as sales by region, product category, and time period. Each metric relies on its own set of attributes distributed across several tables. You need to provide users the ability to filter the data by specific sales representatives and view individual transactions. You want to follow the Google-recommended approach. What should you do?

あなたの会社は、売上データを視覚化し分析するためにLookerを使用しています。地域、製品カテゴリ、期間ごとの売上などの売上指標を表示するダッシュボードを作成する必要があります。各指標は、複数のテーブルに分散した独自の属性セットに依存しています。ユーザーに、特定の営業担当者でデータをフィルタリングし、個々のトランザクションを表示する機能を提供する必要があります。Googleが推奨するアプローチに従いたいと考えています。どうすべきですか？

A. Create multiple Explores, each focusing on each sales metric. Link the Explores together in a dashboard using drill-down functionality.
B. Use BigQuery to create multiple materialized views, each focusing on a specific sales metric. Build the dashboard using these views.
C. Create a single Explore with all sales metrics. Build the dashboard using this Explore.
D. Use Looker's custom visualization capabilities to create a single visualization that displays all the sales metrics with filtering and drill-down functionality.

**正解: C**

**解説:**
Creating asingle Explorewith all the sales metrics is the Google-recommended approach. This Explore should be designed to include all relevant attributes and dimensions, enabling users to analyze sales data by region, product category, time period, and other filters like sales representatives. With a well- structured Explore, you can efficiently build a dashboard that supports filtering and drill-down functionality. This approach simplifies maintenance, provides a consistent data model, and ensures users have the flexibility to interact with and analyze the data seamlessly within a unified framework.

すべての売上指標を含む単一のExploreを作成することが、Googleが推奨するアプローチです。このExploreは、関連するすべての属性とディメンションを含むように設計し、ユーザーが地域、製品カテゴリ、期間、および営業担当者のような他のフィルターで売上データを分析できるようにすべきです。よく構造化されたExploreを使用すると、フィルタリングとドリルダウン機能をサポートするダッシュボードを効率的に構築できます。このアプローチは、メンテナンスを簡素化し、一貫したデータモデルを提供し、ユーザーが統一されたフレームワーク内でシームレスにデータを操作・分析できる柔軟性を確保します。

-----

### <a name="no45"></a>**NO.45**

Your company is setting up an enterprise business intelligence platform. You need to limit data access between many different teams while following the Google-recommended approach. What should you do first?

あなたの会社は、エンタープライズビジネスインテリジェンスプラットフォームをセットアップしています。Googleが推奨するアプローチに従いながら、多くの異なるチーム間のデータアクセスを制限する必要があります。最初に何をすべきですか？

A. Create a separate Looker Studio report for each team, and share each report with the individuals within each team.
B. Create one Looker Studio report with multiple pages, and add each team's data as a separate data source to the report.
C. Create a Looker (Google Cloud core) instance, and create a separate dashboard for each team.
D. Create a Looker (Google Cloud core) instance, and configure different Looker groups for each team.

**正解: D**

**解説:**
Comprehensive and Detailed In-Depth Explanation:
For an enterprise BI platform with data access control across teams, Google recommends Looker (Google Cloud core) over Looker Studio for its robust access management. The "first" step focuses on setting up the foundation.

  * Option A: Looker Studio reports are lightweight but lack granular access control beyond sharing. Creating separate reports per team is inefficient and unscalable.
  * Option B: One Looker Studio report with multiple pages and data sources doesn't enforce team- level access control natively-users could access all pages/data.
  * Option C: Creating a Looker instance with separate dashboards per team is a step forward but skips the foundational access control setup (groups), reducing scalability.

包括的で詳細な解説：
チーム間のデータアクセス制御を備えたエンタープライズBIプラットフォームには、堅牢なアクセス管理機能を持つLooker（Google Cloudコア）がLooker Studioよりも推奨されます。「最初」のステップは、基盤のセットアップに焦点を当てます。

  * 選択肢A：Looker Studioレポートは軽量ですが、共有以外の詳細なアクセス制御がありません。チームごとに個別のレポートを作成するのは非効率的でスケーラブルではありません。
  * 選択肢B：複数のページとデータソースを持つ1つのLooker Studioレポートでは、ネイティブにチームレベルのアクセス制御を強制できません。ユーザーはすべてのページ/データにアクセスできてしまいます。
  * 選択肢C：チームごとに個別のダッシュボードを持つLookerインスタンスを作成するのは一歩前進ですが、基盤となるアクセス制御のセットアップ（グループ）をスキップしており、スケーラビリティが低下します。

-----

### <a name="no46"></a>**NO.46**

Your data science team needs to collaboratively analyze a 25 TB BigQuery dataset to support the development of a machine learning model. You want to use Colab Enterprise notebooks while ensuring efficient data access and minimizing cost. What should you do?

あなたのデータサイエンスチームは、機械学習モデルの開発をサポートするために、25TBのBigQueryデータセットを共同で分析する必要があります。効率的なデータアクセスを確保し、コストを最小限に抑えながら、Colab Enterpriseノートブックを使用したいと考えています。どうすべきですか？

A. Export the BigQuery dataset to Google Drive. Load the dataset into the Colab Enterprise notebook using Pandas.
B. Use BigQuery magic commands within a Colab Enterprise notebook to query and analyze the data.
C. Create a Dataproc cluster connected to a Colab Enterprise notebook, and use Spark to process the data in BigQuery.
D. Copy the BigQuery dataset to the local storage of the Colab Enterprise runtime, and analyze the data using Pandas.

**正解: B**

**解説:**
Comprehensive and Detailed In-Depth Explanation:
For a 25 TB dataset, efficiency and cost require minimizing data movement and leveraging BigQuery's scalability within Colab Enterprise.

  * Option A: Exporting 25 TB to Google Drive and loading via Pandas is impractical (size limits, transfer costs) and slow.
  * Option B: BigQuery magic commands (%%bigquery) in Colab Enterprise allow direct querying of BigQuery data, keeping processing in the cloud, reducing costs, and enabling collaboration.
  * Option C: Dataproc with Spark adds cluster costs and complexity, unnecessary when BigQuery can handle the workload.

包括的で詳細な解説：
25TBのデータセットの場合、効率とコストを考慮すると、データ移動を最小限に抑え、Colab Enterprise内でBigQueryのスケーラビリティを活用する必要があります。

  * 選択肢A：25TBをGoogleドライブにエクスポートし、Pandasでロードするのは非現実的（サイズ制限、転送コスト）で遅いです。
  * 選択肢B：Colab Enterprise内のBigQueryマジックコマンド（%%bigquery）を使用すると、BigQueryデータを直接クエリでき、処理をクラウド内に留めることでコストを削減し、共同作業を可能にします。
  * 選択肢C：DataprocとSparkは、BigQueryがワークロードを処理できる場合には不要なクラスタコストと複雑さを追加します。

-----

### <a name="no47"></a>**NO.47**

Your retail company wants to predict customer churn using historical purchase data stored in BigQuery. The dataset includes customer demographics, purchase history, and a label indicating whether the customer churned or not. You want to build a machine learning model to identify customers at risk of churning. You need to create and train a logistic regression model for predicting customer churn, using the customer_data table with the churned column as the target label. Which BigQuery ML query should you use?

あなたの小売会社は、BigQueryに保存されている過去の購入データを使用して顧客の離反を予測したいと考えています。データセットには、顧客の人口統計、購入履歴、および顧客が離反したかどうかを示すラベルが含まれています。離反のリスクがある顧客を特定するための機械学習モデルを構築したいと考えています。customer_dataテーブルとchurned列をターゲットラベルとして使用して、顧客離反を予測するためのロジスティック回帰モデルを作成およびトレーニングする必要があります。どのBigQuery MLクエリを使用すべきですか？

A. CREATE OR REPLACE MODEL churn_prediction_model OPTIONS(model_uype='logisric_reg') AS SELECT * from cusromer_data;
B. CREATE OR REPLACE MODEL churn_prediction_model OPTIONS (rr.odel_type=' logisric_reg *) AS select * except(churned), churned AS label FROM customer_data;
C. CREATE OR REPLACE MODEL churn_prediction_model options (model type='logistic_reg') AS select churned as label FROM customer_data;
D. CREATE OR REPLACE MODEL churn_prediction_model options(model_type='logistic_reg*) as select'except(churned) FROM customer_data;

**正解: B**

**解説:**
Comprehensive and Detailed in Depth Explanation:
Why B is correct: BigQuery ML requires the target label to be explicitly named label. EXCEPT(churned) selects all columns except the churned column, which becomes the features. churned AS label renames the churned column to label, which is required for BigQuery ML. logistic_reg is the correct model_type option.
Why other options are incorrect:A: Does not rename the target column to label. Also has a typo in the model type.
C: Only selects the target label, not the features.
D: Has a syntax error with the single quote before except.

包括的で詳細な解説：
Bが正しい理由：BigQuery MLでは、ターゲットラベルを明示的に「label」という名前にする必要があります。EXCEPT(churned)は、特徴となるchurned列を除くすべての列を選択します。churned AS labelは、BigQuery MLで必須であるchurned列を「label」に名前変更します。「logistic_reg」は正しいmodel_typeオプションです。
他の選択肢が不適切な理由：A：ターゲット列を「label」に名前変更していません。また、モデルタイプにタイプミスがあります。
C：特徴ではなく、ターゲットラベルのみを選択しています。
D：「except」の前に単一引用符があり、構文エラーです。

-----

### <a name="no48"></a>**NO.48**

Your organization has a BigQuery dataset that contains sensitive employee information such as salaries and performance reviews. The payroll specialist in the HR department needs to have continuous access to aggregated performance data, but they do not need continuous access to other sensitive data. You need to grant the payroll specialist access to the performance data without granting them access to the entire dataset using the simplest and most secure approach. What should you do?

あなたの組織は、給与や業績評価などの機密性の高い従業員情報を含むBigQueryデータセットを保有しています。人事部の給与担当者は、集計された業績データに継続的にアクセスする必要がありますが、他の機密データに継続的にアクセスする必要はありません。最もシンプルで安全なアプローチを使用して、データセット全体へのアクセスを許可することなく、給与担当者に業績データへのアクセスを許可する必要があります。どうすべきですか？

A. Use authorized views to share query results with the payroll specialist.
B. Create row-level and column-level permissions and policies on the table that contains performance data in the dataset. Provide the payroll specialist with the appropriate permission set.
C. Create a table with the aggregated performance data. Use table-level permissions to grant access to the payroll specialist.
D. Create a SQL query with the aggregated performance data. Export the results to an Avro file in a Cloud Storage bucket. Share the bucket with the payroll specialist.

**正解: A**

**解説:**
Usingauthorized viewsis the simplest and most secure way to grant the payroll specialist access to aggregated performance data without exposing the entire dataset. Authorized views allow you to create a view in BigQuery that contains only the query results for the aggregated performance data. The payroll specialist can query the view without being granted access to the underlying sensitive data. This approach ensures security, adheres to the principle of least privilege, and eliminates the need to manage complex row-level or column- level permissions.

承認済みビューを使用することが、データセット全体を公開することなく、集計された業績データへのアクセスを給与担当者に許可する最もシンプルで安全な方法です。承認済みビューを使用すると、集計された業績データのクエリ結果のみを含むビューをBigQueryに作成できます。給与担当者は、基になる機密データへのアクセス権を付与されることなく、そのビューをクエリできます。このアプローチはセキュリティを確保し、最小権限の原則を遵守し、複雑な行レベルまたは列レベルの権限を管理する必要性を排除します。

-----

### <a name="no49"></a>**NO.49**

You have a Dataflow pipeline that processes website traffic logs stored in Cloud Storage and writes the processed data to BigQuery. You noticed that the pipeline is failing intermittently. You need to troubleshoot the issue. What should you do?

あなたは、Cloud Storageに保存されているウェブサイトのトラフィックログを処理し、処理済みのデータをBigQueryに書き込むDataflowパイプラインを持っています。パイプラインが断続的に失敗していることに気づきました。問題をトラブルシューティングする必要があります。どうすべきですか？

A. Use Cloud Logging to identify error groups in the pipeline's logs. Use Cloud Monitoring to create a dashboard that tracks the number of errors in each group.
B. Use Cloud Logging to create a chart displaying the pipeline's error logs. Use Metrics Explorer to validate the findings from the chart.
C. Use Cloud Logging to view error messages in the pipeline's logs. Use Cloud Monitoring to analyze the pipeline's metrics, such as CPU utilization and memory usage.
D. Use the Dataflow job monitoring interface to check the pipeline's status every hour. Use Cloud Profiler to analyze the pipeline's metrics, such as CPU utilization and memory usage.

**正解: C**

**解説:**
To troubleshoot intermittent failures in a Dataflow pipeline, you should useCloud Loggingto view detailed error messages in the pipeline's logs. These logs provide insights into the specific issues causing failures, such as data format errors or resource limitations. Additionally, you should useCloud Monitoringto analyze the pipeline's metrics, such as CPU utilization, memory usage, and throughput, to identify performance bottlenecks or resource constraints that may contribute to the failures. This approach provides a comprehensive view of the pipeline's health and helps pinpoint the root cause of the intermittent issues.

Dataflowパイプラインの断続的な障害をトラブルシューティングするには、Cloud Loggingを使用してパイプラインのログで詳細なエラーメッセージを表示する必要があります。これらのログは、データ形式のエラーやリソースの制限など、障害を引き起こしている特定の問題についての洞察を提供します。さらに、Cloud Monitoringを使用して、CPU使用率、メモリ使用量、スループットなどのパイプラインのメトリクスを分析し、障害に寄与している可能性のあるパフォーマンスのボトルネックやリソースの制約を特定する必要があります。このアプローチは、パイプラインの健全性の包括的なビューを提供し、断続的な問題の根本原因を特定するのに役立ちます。

-----

### <a name="no50"></a>**NO.50**

You used BigQuery ML to build a customer purchase propensity model six months ago. You want to compare the current serving data with the historical serving data to determine whether you need to retrain the model. What should you do?

あなたは6ヶ月前にBigQuery MLを使用して顧客の購入傾向モデルを構築しました。現在のサービングデータと過去のサービングデータを比較して、モデルを再トレーニングする必要があるかどうかを判断したいと考えています。どうすべきですか？

A. Compare the two different models.
B. Evaluate the data skewness.
C. Evaluate data drift.
D. Compare the confusion matrix.

**正解: C**

**解説:**
Evaluating data drift involves analyzing changes in the distribution of the current serving data compared to the historical data used to train the model. If significant drift is detected, it indicates that the data patterns have changed over time, which can impact the model's performance. This analysis helps determine whether retraining the model is necessary to ensure its predictions remain accurate and relevant. Data drift evaluation is a standard approach for monitoring machine learning models over time.

データドリフトの評価には、現在のサービングデータの分布の変化を、モデルのトレーニングに使用された過去のデータと比較して分析することが含まれます。重大なドリフトが検出された場合、それは時間の経過とともにデータパターンが変化したことを示しており、モデルのパフォーマンスに影響を与える可能性があります。この分析は、モデルの予測が正確で関連性を保つために再トレーニングが必要かどうかを判断するのに役立ちます。データドリフトの評価は、機械学習モデルを長期にわたって監視するための標準的なアプローチです。

-----

はい、承知いたしました。
No.51から最後の問題まで、指定されたフォーマットで引き続き出力します。

-----

### <a name="no51"></a>**NO.51**

You work for a financial organization that stores transaction data in BigQuery. Your organization has a regulatory requirement to retain data for a minimum of seven years for auditing purposes. You need to ensure that the data is retained for seven years using an efficient and cost- optimized approach. What should you do?

あなたは金融機関で働いており、取引データをBigQueryに保存しています。あなたの組織には、監査目的でデータを最低7年間保持するという規制要件があります。効率的でコストが最適化されたアプローチを使用して、データが7年間保持されるようにする必要があります。どうすべきですか？

A. Create a partition by transaction date, and set the partition expiration policy to seven years.
B. Set the table-level retention policy in BigQuery to seven years.
C. Set the dataset-level retention policy in BigQuery to seven years.
D. Export the BigQuery tables to Cloud Storage daily, and enforce a lifecycle management policy that has a seven-year retention rule.

**正解: B**

**解説:**
Setting a table-level retention policy policy in BigQuery to seven years is the most efficient and cost- optimized solution to meet the regulatory requirement. A table-level retention policy ensures that the data cannot be deleted or overwritten before the specified retention period expires, providing compliance with auditing requirements while keeping the data within BigQuery for easy access and analysis. This approach avoids the complexity and additional costs of exporting data to Cloud Storage.

BigQueryでテーブルレベルの保持ポリシーを7年に設定することが、規制要件を満たすための最も効率的でコストが最適化されたソリューションです。テーブルレベルの保持ポリシーは、指定された保持期間が経過する前にデータが削除または上書きされないことを保証し、監査要件へのコンプライアンスを提供しながら、簡単なアクセスと分析のためにデータをBigQuery内に保持します。このアプローチは、データをCloud Storageにエクスポートする複雑さと追加コストを回避します。

-----

### <a name="no52"></a>**NO.52**

Another team in your organization is requesting access to a BigQuery dataset. You need to share the dataset with the team while minimizing the risk of unauthorized copying of data. You also want to create a reusable framework in case you need to share this data with other teams in the future. What should you do?

あなたの組織の別のチームが、BigQueryデータセットへのアクセスを要求しています。データの不正コピーのリスクを最小限に抑えながら、データセットをチームと共有する必要があります。また、将来このデータを他のチームと共有する場合に備えて、再利用可能なフレームワークを作成したいと考えています。どうすべきですか？

A. Create authorized views in the team's Google Cloud project that is only accessible by the team.
B. Create a private exchange using Analytics Hub with data egress restriction, and grant access to the team members.
C. Enable domain restricted sharing on the project. Grant the team members the BigQuery Data Viewer IAM role on the dataset.
D. Export the dataset to a Cloud Storage bucket in the team's Google Cloud project that is only accessible by the team.

**正解: B**

**解説:**
Using Analytics Hub to create a private exchange with data egress restrictions ensures controlled sharing of the dataset while minimizing the risk of unauthorized copying. This approach allows you to provide secure, managed access to the dataset without giving direct access to the raw data. The egress restriction ensures that data cannot be exported or copied outside the designated boundaries. Additionally, this solution provides a reusable framework that simplifies future data sharing with other teams or projects while maintaining strict data governance.

Analytics Hubを使用してデータエグレス制限付きのプライベートエクスチェンジを作成すると、不正コピーのリスクを最小限に抑えながら、データセットの管理された共有が保証されます。このアプローチにより、生データへの直接アクセスを許可することなく、データセットへの安全で管理されたアクセスを提供できます。エグレス制限により、データが指定された境界外にエクスポートまたはコピーされないことが保証されます。さらに、このソリューションは、厳格なデータガバナンスを維持しながら、将来他のチームやプロジェクトとのデータ共有を簡素化する再利用可能なフレームワークを提供します。

-----

### <a name="no53"></a>**NO.53**

You want to build a model to predict the likelihood of a customer clicking on an online advertisement. You have historical data in BigQuery that includes features such as user demographics, ad placement, and previous click behavior. After training the model, you want to generate predictions on new data. Which model type should you use in BigQuery ML?

あなたは、顧客がオンライン広告をクリックする可能性を予測するモデルを構築したいと考えています。BigQueryには、ユーザーの人口統計、広告の配置、以前のクリック行動などの特徴を含む過去のデータがあります。モデルをトレーニングした後、新しいデータで予測を生成したいと考えています。BigQuery MLではどのモデルタイプを使用すべきですか？

A. Linear regression
B. Matrix factorization
C. Logistic regression
D. K-means clustering

**正解: C**

**解説:**
Comprehensive and Detailed In-Depth Explanation:
Predicting the likelihood of a click (binary outcome: click or no-click) requires a classification model. BigQuery ML supports this use case with logistic regression.

  * Option A: Linear regression predicts continuous values, not probabilities for binary outcomes.
  * Option B: Matrix factorization is for recommendation systems, not binary prediction.
  * Option C: Logistic regression predicts probabilities for binary classification (e.g., click likelihood), ideal for this scenario and supported in BigQuery ML.

包括的で詳細な解説：
クリックの可能性（二値の結果：クリックまたは非クリック）を予測するには、分類モデルが必要です。BigQuery MLは、ロジスティック回帰でこのユースケースをサポートしています。

  * 選択肢A：線形回帰は連続値を予測するものであり、二値の結果の確率を予測するものではありません。
  * 選択肢B：行列因子分解は推薦システム用であり、二値予測用ではありません。
  * 選択肢C：ロジスティック回帰は二項分類（例：クリックの可能性）の確率を予測し、このシナリオに理想的であり、BigQuery MLでサポートされています。

-----

### <a name="no54"></a>**NO.54**

You are storing data in Cloud Storage for a machine learning project. The data is frequently accessed during the model training phase, minimally accessed after 30 days, and unlikely to be accessed after 90 days. You need to choose the appropriate storage class for the different stages of the project to minimize cost. What should you do?

あなたは機械学習プロジェクトのためにCloud Storageにデータを保存しています。データはモデルのトレーニング段階で頻繁にアクセスされ、30日後は最小限しかアクセスされず、90日後はアクセスされる可能性が低いです。コストを最小限に抑えるために、プロジェクトの異なる段階に適したストレージクラスを選択する必要があります。どうすべきですか？

A. Store the data in Nearline storage during the model training phase. Transition the data to Coldline storage 30 days after model deployment, and to Archive storage 90 days after model deployment.
B. Store the data in Standard storage during the model training phase. Transition the data to Nearline storage 30 days after model deployment, and to Coldline storage 90 days after model deployment.
C. Store the data in Nearline storage during the model training phase. Transition the data to Archive storage 30 days after model deployment, and to Coldline storage 90 days after model deployment.
D. Store the data in Standard storage during the model training phase. Transition the data to Durable Reduced Availability (DRA) storage 30 days after model deployment, and to Coldline storage 90 days after model deployment.

**正解: B**

**解説:**
Comprehensive and Detailed In-Depth Explanation:
Cost minimization requires matching storage classes to access patterns using lifecycle rules. Let's assess:

  * Option A: Nearline during training (frequent access) incurs high retrieval costs and latency, unsuitable for ML workloads. Coldline after 30 days and Archive after 90 days are reasonable but misaligned initially.
  * Option B: Standard storage (no retrieval fees, low latency) is ideal for frequent access during training. Transitioning to Nearline (30-day minimum, low access) after 30 days and Coldline (90-day minimum, rare access) after 90 days matches the pattern and minimizes costs effectively.
  * Option C: Nearline during training is costly for frequent access, and Archive to Coldline is illogical (Archive is cheaper than Coldline).

包括的で詳細な解説：
コストを最小化するには、ライフサイクルルールを使用してストレージクラスをアクセスパターンに一致させる必要があります。評価しましょう：

  * 選択肢A：トレーニング中のNearline（頻繁なアクセス）は、高い取得コストと遅延を伴うため、MLワークロードには不適切です。30日後のColdlineと90日後のArchiveは合理的ですが、初期の配置が間違っています。
  * 選択肢B：Standardストレージ（取得料なし、低遅延）は、トレーニング中の頻繁なアクセスに理想的です。30日後にNearline（最低30日間、低アクセス）、90日後にColdline（最低90日間、まれなアクセス）に移行することで、パターンに一致し、コストを効果的に最小化します。
  * 選択肢C：トレーニング中のNearlineは頻繁なアクセスにはコストがかかり、ArchiveからColdlineへの移行は論理的ではありません（Archiveの方がColdlineより安価です）。

-----

### <a name="no55"></a>**NO.55**

You are a Looker analyst. You need to add a new field to your Looker report that generates SQL that will run against your company's database. You do not have the Develop permission. What should you do?

あなたはLookerのアナリストです。会社のデータベースに対して実行されるSQLを生成する新しいフィールドをLookerレポートに追加する必要があります。あなたにはDevelop権限がありません。どうすべきですか？

A. Create a new field in the LookML layer, refresh your report, and select your new field from the field picker.
B. Create a calculated field using the Add a field option in Looker Studio, and add it to your report.
C. Create a table calculation from the field picker in Looker, and add it to your report.
D. Create a custom field from the field picker in Looker, and add it to your report.

**正解: D**

**解説:**
Creating acustom field from the field picker in Lookerallows you to add new fields to your report without requiring the Develop permission. Custom fields are created directly in the Looker UI, enabling you to define calculations or transformations that generate SQL for the database query. This approach is user-friendly and does not require access to the LookML layer, making it the appropriate choice for your situation.

Lookerのフィールドピッカーからカスタムフィールドを作成すると、Develop権限を必要とせずにレポートに新しいフィールドを追加できます。カスタムフィールドはLooker UIで直接作成され、データベースクエリ用のSQLを生成する計算や変換を定義できます。このアプローチはユーザーフレンドリーであり、LookMLレイヤーへのアクセスを必要としないため、あなたの状況に適した選択肢です。

-----

### <a name="no56"></a>**NO.56**

Your team is building several data pipelines that contain a collection of complex tasks and dependencies that you want to execute on a schedule, in a specific order. The tasks and dependencies consist of files in Cloud Storage, Apache Spark jobs, and data in BigQuery. You need to design a system that can schedule and automate these data processing tasks using a fully managed approach. What should you do?

あなたのチームは、特定の順序でスケジュールに従って実行したい、複雑なタスクと依存関係のコレクションを含むいくつかのデータパイプラインを構築しています。タスクと依存関係は、Cloud Storageのファイル、Apache Sparkジョブ、およびBigQueryのデータで構成されています。これらのデータ処理タスクをスケジュールし、自動化できるシステムを、フルマネージドのアプローチで設計する必要があります。どうすべきですか？

A. Use Cloud Scheduler to schedule the jobs to run.
B. Use Cloud Tasks to schedule and run the jobs asynchronously.
C. Create directed acyclic graphs (DAGs) in Cloud Composer. Use the appropriate operators to connect to Cloud Storage, Spark, and BigQuery.
D. Create directed acyclic graphs (DAGs) in Apache Airflow deployed on Google Kubernetes Engine. Use the appropriate operators to connect to Cloud Storage, Spark, and BigQuery.

**正解: C**

**解説:**
UsingCloud Composerto create Directed Acyclic Graphs (DAGs) is the best solution because it is a fully managed, scalable workflow orchestration service based on Apache Airflow. Cloud Composer allows you to define complex task dependencies and schedules while integrating seamlessly with Google Cloud services such as Cloud Storage, BigQuery, and Dataproc for Apache Spark jobs. This approach minimizes operational overhead, supports scheduling and automation, and provides an efficient and fully managed way to orchestrate your data pipelines.

Cloud Composerを使用して有向非巡回グラフ（DAG）を作成することが最善の解決策です。なぜなら、これはApache Airflowをベースにした、フルマネージドでスケーラブルなワークフローオーケストレーションサービスだからです。Cloud Composerを使用すると、Cloud Storage、BigQuery、Apache Sparkジョブ用のDataprocなどのGoogle Cloudサービスとシームレスに統合しながら、複雑なタスクの依存関係とスケジュールを定義できます。このアプローチは、運用オーバーヘッドを最小限に抑え、スケジューリングと自動化をサポートし、データパイプラインを効率的かつフルマネージドな方法で調整する手段を提供します。

-----

### <a name="no57"></a>**NO.57**

You are a data analyst at your organization. You have been given a BigQuery dataset that includes customer information. The dataset contains inconsistencies and errors, such as missing values, duplicates, and formatting issues. You need to effectively and quickly clean the data. What should you do?

あなたは組織のデータアナリストです。顧客情報を含むBigQueryデータセットを与えられました。データセットには、欠損値、重複、フォーマットの問題など、不整合やエラーが含まれています。データを効果的かつ迅速にクレンジングする必要があります。どうすべきですか？

A. Develop a Dataflow pipeline to read the data from BigQuery, perform data quality rules and transformations, and write the cleaned data back to BigQuery.
B. Use Cloud Data Fusion to create a data pipeline to read the data from BigQuery, perform data quality transformations, and write the clean data back to BigQuery.
C. Export the data from BigQuery to CSV files. Resolve the errors using a spreadsheet editor, and re- import the cleaned data into BigQuery.
D. Use BigQuery's built-in functions to perform data quality transformations.

**正解: D**

**解説:**
Using BigQuery's built-in functionsis the most effective and efficient way to clean the dataset directly within BigQuery. BigQuery provides powerful SQL capabilities to handle missing values, remove duplicates, and resolve formatting issues without needing to export data or create complex pipelines. This approach minimizes overhead and leverages the scalability of BigQuery for large datasets, making it an ideal solution for quickly addressing data quality issues.

BigQueryの組み込み関数を使用することが、データセットをBigQuery内で直接クレンジングするための最も効果的で効率的な方法です。BigQueryは、データをエクスポートしたり複雑なパイプラインを作成したりすることなく、欠損値の処理、重複の削除、フォーマットの問題の解決を行う強力なSQL機能を提供します。このアプローチはオーバーヘッドを最小限に抑え、大規模なデータセットに対するBigQueryのスケーラビリティを活用するため、データ品質の問題に迅速に対処するための理想的なソリューションです。

-----

### <a name="no58"></a>**NO.58**

You work for a retail company that collects customer data from various sources:

  * Online transactions: Stored in a MySQL database
  * Customer feedback: Stored as text files on a company server
  * Social media activity: Streamed in real-time from social media platformsYou need to design a data pipeline to extract and load the data into the appropriate Google Cloud storage system(s) for further analysis and ML model training. What should you do?

あなたは小売企業で働いており、さまざまなソースから顧客データを収集しています：

  * オンライン取引：MySQLデータベースに保存
  * 顧客フィードバック：会社のサーバーにテキストファイルとして保存
  * ソーシャルメディアアクティビティ：ソーシャルメディアプラットフォームからリアルタイムでストリーミング
    さらなる分析とMLモデルのトレーニングのために、データを抽出し、適切なGoogle Cloudストレージシステムにロードするデータパイプラインを設計する必要があります。どうすべきですか？

A. Copy the online transactions data into Cloud SQL for MySQL. Import the customer feedback into BigQuery. Stream the social media activity into Cloud Storage.
B. Extract and load the online transactions data into BigQuery. Load the customer feedback data into Cloud Storage. Stream the social media activity by using Pub/Sub and Dataflow, and store the data in BigQuery.
C. Extract and load the online transactions data, customer feedback data, and social media activity into Cloud Storage.
D. Extract and load the online transactions data into Bigtable. Import the customer feedback data into Cloud Storage. Store the social media activity in Cloud SQL for MySQL.

**正解: B**

**解説:**
Comprehensive and Detailed In-Depth Explanation:
The pipeline must extract diverse data types and load them into systems optimized for analysis and ML. Let's assess:

  * Option A: Cloud SQL for transactions keeps data relational but isn't ideal for analysis/ML (less scalable than BigQuery). BigQuery for feedback is fine but skips staging. Cloud Storage for streaming social media loses real-time context and requires extra steps for analysis.
  * Option B: BigQuery for transactions (via export from MySQL) supports analysis/ML with SQL. Cloud Storage stages feedback text files for preprocessing, then BigQuery ingestion. Pub/Sub and Dataflow stream social media into BigQuery, enabling real-time analysis-optimal for all sources.
  * Option C: Cloud Storage for all data is a staging step, not a final solution for analysis/ML, requiring additional pipelines.

包括的で詳細な解説：
パイプラインは多様なデータタイプを抽出し、分析とMLに最適化されたシステムにロードする必要があります。評価しましょう：

  * 選択肢A：トランザクション用のCloud SQLはデータをリレーショナルに保ちますが、分析/MLには理想的ではありません（BigQueryよりスケーラビリティが低い）。フィードバック用のBigQueryは良いですが、ステージングをスキップしています。ストリーミングソーシャルメディア用のCloud Storageはリアルタイムのコンテキストを失い、分析には追加のステップが必要です。
  * 選択肢B：トランザクション用のBigQuery（MySQLからのエクスポート経由）はSQLでの分析/MLをサポートします。Cloud Storageはフィードバックのテキストファイルを前処理のためにステージングし、その後BigQueryに取り込みます。Pub/SubとDataflowはソーシャルメディアをBigQueryにストリーミングし、リアルタイム分析を可能にします。これはすべてのソースにとって最適です。
  * 選択肢C：すべてのデータにCloud Storageを使用するのはステージングステップであり、分析/MLの最終的な解決策ではなく、追加のパイプラインが必要です。

-----

### <a name="no59"></a>**NO.59**

You need to transfer approximately 300 TB of data from your company's on-premises data center to Cloud Storage. You have 100 Mbps internet bandwidth, and the transfer needs to be completed as quickly as possible. What should you do?

会社のオンプレミスデータセンターからCloud Storageへ約300TBのデータを転送する必要があります。インターネット帯域幅は100Mbpsで、転送はできるだけ迅速に完了する必要があります。どうすべきですか？

A. Use Cloud Client Libraries to transfer the data over the internet.
B. Use the gcloud storage command to transfer the data over the internet.
C. Compress the data, upload it to multiple cloud storage providers, and then transfer the data to CloudStorage.
D. Request a Transfer Appliance, copy the data to the appliance, and ship it back to Google.

**正解: D**

**解説:**
Comprehensive and Detailed In-Depth Explanation:
Transferring 300 TB over a 100 Mbps connection would take an impractical amount of time (over 300 days at theoretical maximum speed, ignoring real-world constraints like latency). Google Cloud provides the Transfer Appliance for large-scale, time-sensitive transfers.

  * Option A: Cloud Client Libraries over the internet would be slow and unreliable for 300 TB due to bandwidth limitations.
  * Option B: The gcloud storage command is similarly constrained by internet speed and not designed for such large transfers.
  * Option C: Compressing and splitting across multiple providers adds complexity and isn't a Google- supported method for Cloud Storage ingestion.

包括的で詳細な解説：
100Mbpsの接続で300TBを転送するには、非現実的な時間がかかります（理論上の最大速度でも300日以上かかり、遅延などの現実世界の制約を無視しています）。Google Cloudは、大規模で時間に制約のある転送のためにTransfer Applianceを提供しています。

  * 選択肢A：インターネット経由のCloud Client Librariesは、帯域幅の制限により300TBには遅くて信頼性がありません。
  * 選択肢B：gcloud storageコマンドも同様にインターネット速度に制約され、このような大規模な転送には設計されていません。
  * 選択肢C：複数のプロバイダーに圧縮して分割するのは複雑で、Cloud Storageへの取り込みにはGoogleがサポートする方法ではありません。

-----

### <a name="no60"></a>**NO.60**

Your organization needs to store historical customer order data. The data will only be accessed once a month for analysis and must be readily available within a few seconds when it is accessed. You need to choose a storage class that minimizes storage costs while ensuring that the data can be retrieved quickly. What should you do?

あなたの組織は、過去の顧客注文データを保存する必要があります。データは分析のために月に一度だけアクセスされ、アクセス時には数秒以内に利用可能でなければなりません。データの迅速な取得を保証しつつ、ストレージコストを最小限に抑えるストレージクラスを選択する必要があります。どうすべきですか？

A. Store the data in Cloud Storage using Nearline storage.
B. Store the data in Cloud Storage using Coldline storage.
C. Store the data in Cloud Storage using Standard storage.
D. Store the data in Cloud Storage using Archive storage.

**正解: A**

**解説:**
Using Nearline storagein Cloud Storage is the best option for data that is accessed infrequently (such as once a month) but must be readily available within seconds when needed. Nearline offers a balance between low storage costs and quick retrieval times, making it ideal for scenarios like monthly analysis of historical data. It is specifically designed for infrequent access patterns while avoiding the higher retrieval costs and longer access times of Coldline or Archive storage.

Cloud StorageでNearlineストレージを使用することが、頻繁にはアクセスされない（月に一度など）が、必要なときには数秒以内に利用可能でなければならないデータに最適な選択肢です。Nearlineは、低いストレージコストと迅速な取得時間のバランスを提供し、過去のデータの月次分析のようなシナリオに理想的です。これは、ColdlineやArchiveストレージの高い取得コストや長いアクセス時間を避けつつ、頻繁でないアクセスパターン向けに特別に設計されています。

-----

### <a name="no61"></a>**NO.61**

Your company stores historical data in Cloud Storage. You need to ensure that all data is saved in a bucket for at least three years. What should you do?

あなたの会社は、過去のデータをCloud Storageに保存しています。すべてのデータが少なくとも3年間バケットに保存されるようにする必要があります。どうすべきですか？

A. Enable Object Versioning.
B. Change the bucket storage class to Archive.
C. Set a bucket retention policy.
D. Set temporary object holds.

**正解: C**

**解説:**
Comprehensive and Detailed in Depth Explanation:
Why C is correct: Bucket retention policies are specifically designed to enforce a minimum retention period for objects within a Cloud Storage bucket. This ensures that data cannot be deleted or overwritten before the specified period.
Why other options are incorrect:A: Object versioning allows you to keep multiple versions of an object, but it doesn't guarantee a minimum retention period.
B: Changing the storage class to Archive is for cost optimization, not data retention enforcement.
D: Object holds are for legal holds, not general retention.

包括的で詳細な解説：
Cが正しい理由：バケット保持ポリシーは、Cloud Storageバケット内のオブジェクトに最小保持期間を強制するために特別に設計されています。これにより、指定された期間が経過する前にデータが削除または上書きされないことが保証されます。
他の選択肢が不適切な理由：A：オブジェクトのバージョニングではオブジェクトの複数のバージョンを保持できますが、最小保持期間は保証されません。
B：ストレージクラスをArchiveに変更するのはコスト最適化のためであり、データ保持の強制のためではありません。
D：オブジェクトホールドは法的保持のためであり、一般的な保持のためではありません。

-----

### <a name="no62"></a>**NO.62**

You are responsible for managing Cloud Storage buckets for a research company. Your company has well- defined data tiering and retention rules. You need to optimize storage costs while achieving your data retention needs. What should you do?

あなたは研究会社のCloud Storageバケットを管理する責任者です。あなたの会社には、明確に定義されたデータ階層化と保持ルールがあります。データ保持のニーズを達成しながら、ストレージコストを最適化する必要があります。どうすべきですか？

A. Configure the buckets to use the Archive storage class.
B. Configure a lifecycle management policy on each bucket to downgrade the storage class and remove objects based on age.
C. Configure the buckets to use the Standard storage class and enable Object Versioning.
D. Configure the buckets to use the Autoclass feature.

**正解: B**

**解説:**
Configuring alifecycle management policyon each Cloud Storage bucket allows you to automatically transition objects to lower-cost storage classes (such as Nearline, Coldline, or Archive) based on their age or other criteria. Additionally, the policy can automate the removal of objects once they are no longer needed, ensuring compliance with retention rules and optimizing storage costs. This approach aligns well with well- defined data tiering and retention needs, providing cost efficiency and automation.

各Cloud Storageバケットにライフサイクル管理ポリシーを設定すると、オブジェクトをその年齢やその他の基準に基づいて、より低コストのストレージクラス（Nearline、Coldline、Archiveなど）に自動的に移行できます。さらに、ポリシーは不要になったオブジェクトの削除を自動化でき、保持ルールへの準拠を保証し、ストレージコストを最適化します。このアプローチは、明確に定義されたデータ階層化と保持のニーズによく合致しており、コスト効率と自動化を提供します。

-----

### <a name="no63"></a>**NO.63**

You need to create a weekly aggregated sales report based on a large volume of data. You want to use Python to design an efficient process for generating this report. What should you do?

大量のデータに基づいて、週次の集計済み売上レポートを作成する必要があります。このレポートを生成するための効率的なプロセスをPythonを使用して設計したいと考えています。どうすべきですか？

A. Create a Cloud Run function that uses NumPy. Use Cloud Scheduler to schedule the function to run once a week.
B. Create a Colab Enterprise notebook and use the bigframes.pandas library. Schedule the notebook to execute once a week.
C. Create a Cloud Data Fusion and Wrangler flow. Schedule the flow to run once a week.
D. Create a Dataflow directed acyclic graph (DAG) coded in Python. Use Cloud Scheduler to schedule the code to run once a week.

**正解: D**

**解説:**
Using Dataflow with a Python-coded Directed Acyclic Graph (DAG) is the most efficient solution for generating a weekly aggregated sales report based on a large volume of data. Dataflow is optimized for large-scale data processing and can handle aggregation efficiently. Python allows you to customize the pipeline logic, and Cloud Scheduler enables you to automate the process to run weekly. This approach ensures scalability, efficiency, and the ability to process large datasets in a cost-effective manner.

Pythonでコーディングされた有向非巡回グラフ（DAG）を持つDataflowを使用することが、大量のデータに基づいて週次の集計済み売上レポートを生成するための最も効率的なソリューションです。Dataflowは大規模なデータ処理に最適化されており、集計を効率的に処理できます。Pythonを使用するとパイプラインのロジックをカスタマイズでき、Cloud Schedulerを使用するとプロセスを週次で実行するように自動化できます。このアプローチは、スケーラビリティ、効率、および大規模なデータセットを費用対効果の高い方法で処理する能力を保証します。

-----

### <a name="no64"></a>**NO.64**

You work for a home insurance company. You are frequently asked to create and save risk reports with charts for specific areas using a publicly available storm event dataset. You want to be able to quickly create and re- run risk reports when new data becomes available. What should you do?

あなたは住宅保険会社で働いています。公開されている暴風イベントのデータセットを使用して、特定の地域のチャート付きリスクレポートを作成し、保存するよう頻繁に依頼されます。新しいデータが利用可能になったときに、リスクレポートを迅速に作成および再実行できるようにしたいと考えています。どうすべきですか？

A. Export the storm event dataset as a CSV file. Import the file to Google Sheets, and use cell data in the worksheets to create charts.
B. Copy the storm event dataset into your BigQuery project. Use BigQuery Studio to query and visualize the data in Looker Studio.
C. Reference and query the storm event dataset using SQL in BigQuery Studio. Export the results to Google Sheets, and use cell data in the worksheets to create charts.
D. Reference and query the storm event dataset using SQL in a Colab Enterprise notebook. Display the table results and document with Markdown, and use Matplotlib to create charts.

**正解: B**

**解説:**
Copying the storm event dataset into yourBigQuery projectand usingBigQuery Studioto query and visualize the data inLooker Studiois the best approach. This solution allows you to create reusable and automated workflows for generating risk reports. BigQuery handles the querying efficiently, and Looker Studio provides powerful tools for creating and sharing dynamic charts and dashboards. This setup ensures that reports can be easily re-run with updated data, minimizing manual effort and providing a scalable, interactive solution for visualizing risk reports.

暴風イベントのデータセットをあなたのBigQueryプロジェクトにコピーし、BigQuery Studioを使用してデータをクエリし、Looker Studioで視覚化することが最善のアプローチです。このソリューションにより、リスクレポートを生成するための再利用可能で自動化されたワークフローを作成できます。BigQueryはクエリを効率的に処理し、Looker Studioは動的なチャートやダッシュボードを作成・共有するための強力なツールを提供します。この設定により、更新されたデータでレポートを簡単に再実行でき、手作業を最小限に抑え、リスクレポートを視覚化するためのスケーラブルでインタラクティブなソリューションを提供します。

-----

### <a name="no65"></a>**NO.65**

You manage a BigQuery table that is used for critical end-of-month reports. The table is updated weekly with new sales data. You want to prevent data loss and reporting issues if the table is accidentally deleted. What should you do?

あなたは、重要な月末レポートに使用されるBigQueryテーブルを管理しています。テーブルは毎週新しい売上データで更新されます。テーブルが誤って削除された場合に、データ損失とレポートの問題を防ぎたいと考えています。どうすべきですか？

A. Configure the time travel duration on the table to be exactly seven days. On deletion, re-create the deleted table solely from the time travel data.
B. Schedule the creation of a new snapshot of the table once a week. On deletion, re-create the deleted table using the snapshot and time travel data.
C. Create a clone of the table. On deletion, re-create the deleted table by copying the content of the clone.
D. Create a view of the table. On deletion, re-create the deleted table from the view and time travel data.

**正解: B**

**解説:**
Scheduling the creation of asnapshot of the tableweekly ensures that you have a point-in-time backup of the table. In case of accidental deletion, you can re-create the table from the snapshot. Additionally, BigQuery'stime travelfeature allows you to recover data from up to seven days prior to deletion. Combining snapshots with time travel provides a robust solution for preventing data loss and ensuring reporting continuity for critical tables. This approach minimizes risks while offering flexibility for recovery.

週に一度テーブルのスナップショットを作成するようスケジュールすることで、テーブルのポイントインタイムバックアップを確保できます。誤って削除された場合、スナップショットからテーブルを再作成できます。さらに、BigQueryのタイムトラベル機能を使用すると、削除前の最大7日間のデータを回復できます。スナップショットとタイムトラベルを組み合わせることで、データ損失を防ぎ、重要なテーブルのレポートの継続性を確保するための堅牢なソリューションが提供されます。このアプローチは、リスクを最小限に抑えながら、回復のための柔軟性を提供します。

-----

### <a name="no66"></a>**NO.66**

Following a recent company acquisition, you inherited an on-premises data infrastructure that needs to move to Google Cloud. The acquired system has 250 Apache Airflow directed acyclic graphs (DAGs) orchestrating data pipelines. You need to migrate the pipelines to a Google Cloud managed service with minimal effort. What should you do?

最近の会社買収に伴い、Google Cloudに移行する必要があるオンプレミスのデータインフラストラクチャを引き継ぎました。買収したシステムには、データパイプラインを調整する250のApache Airflow有向非巡回グラフ（DAG）があります。最小限の労力でパイプラインをGoogle Cloudのマネージドサービスに移行する必要があります。どうすべきですか？

A. Convert each DAG to a Cloud Workflow and automate the execution with Cloud Scheduler.
B. Create a new Cloud Composer environment and copy DAGs to the Cloud Composer dags/folder.
C. Create a Google Kubernetes Engine (GKE) standard cluster and deploy Airflow as a workload. Migrate all DAGs to the new Airflow environment.
D. Create a Cloud Data Fusion instance. For each DAG, create a Cloud Data Fusion pipeline.

**正解: B**

**解説:**
Comprehensive and Detailed in Depth Explanation:
Why B is correct:Cloud Composer is a managed Apache Airflow service that provides a seamless migration path for existing Airflow DAGS. Simply copying the DAGs to the Cloud Composer folder allows them to run directly on Google Cloud.
Why other options are incorrect:A: Cloud Workflows is a different orchestration tool, not compatible with Airflow DAGS.
C: GKE deployment requires setting up and managing a Kubernetes cluster, which is more complex.
D: Cloud Data Fusion is a data integration tool, not suitable for orchestrating existing pipelines.

包括的で詳細な解説：
Bが正しい理由：Cloud ComposerはマネージドApache Airflowサービスであり、既存のAirflow DAGのシームレスな移行パスを提供します。DAGをCloud Composerフォルダにコピーするだけで、Google Cloud上で直接実行できます。
他の選択肢が不適切な理由：A：Cloud Workflowsは別のオーケストレーションツールであり、Airflow DAGと互換性がありません。
C：GKEのデプロイはKubernetesクラスタのセットアップと管理が必要で、より複雑です。
D：Cloud Data Fusionはデータ統合ツールであり、既存のパイプラインのオーケストレーションには適していません。

-----

### <a name="no67"></a>**NO.67**

You are working with a large dataset of customer reviews stored in Cloud Storage. The dataset contains several inconsistencies, such as missing values, incorrect data types, and duplicate entries. You need toclean the data to ensure that it is accurate and consistent before using it for analysis. What should you do?

あなたは、Cloud Storageに保存されている顧客レビューの大きなデータセットを扱っています。データセットには、欠損値、不正なデータ型、重複エントリなど、いくつかの不整合が含まれています。分析に使用する前に、データが正確で一貫性があることを保証するためにデータをクレンジングする必要があります。どうすべきですか？

A. Use the PythonOperator in Cloud Composer to clean the data and load it into BigQuery. Use SQL for analysis.
B. Use BigQuery to batch load the data into BigQuery. Use SQL for cleaning and analysis.
C. Use Storage Transfer Service to move the data to a different Cloud Storage bucket. Use event triggers to invoke Cloud Run functions to load the data into BigQuery. Use SQL for analysis.
D. Use Cloud Run functions to clean the data and load it into BigQuery. Use SQL for analysis.

**正解: B**

**解説:**
Using BigQuery to batch load the data and perform cleaning and analysis with SQL is the best approach for this scenario. BigQuery provides powerful SQL capabilities to handle missing values, enforce correct data types, and remove duplicates efficiently. This method simplifies the pipeline by leveraging BigQuery's built-in processing power for both cleaning and analysis, reducing the need for additional tools or services and minimizing complexity.

このシナリオでは、BigQueryを使用してデータをバッチロードし、SQLでクレンジングと分析を行うのが最善のアプローチです。BigQueryは、欠損値の処理、正しいデータ型の強制、重複の効率的な削除を行う強力なSQL機能を提供します。この方法は、クレンジングと分析の両方にBigQueryの組み込み処理能力を活用することでパイプラインを簡素化し、追加のツールやサービスの必要性を減らし、複雑さを最小限に抑えます。

-----

### <a name="no68"></a>**NO.68**

You recently inherited a task for managing Dataflow streaming pipelines in your organization and noticed that proper access had not been provisioned to you. You need to request a Google- provided IAM role so you can restart the pipelines. You need to follow the principle of least privilege. What should you do?

あなたは最近、組織のDataflowストリーミングパイプラインを管理するタスクを引き継ぎましたが、適切なアクセス権が付与されていないことに気づきました。パイプラインを再起動できるように、Google提供のIAMロールを要求する必要があります。最小権限の原則に従う必要があります。どうすべきですか？

A. Request the Dataflow Developer role.
B. Request the Dataflow Viewer role.
C. Request the Dataflow Worker role.
D. Request the Dataflow Admin role.

**正解: A**

**解説:**
TheDataflow Developerrole provides the necessary permissions to manage Dataflow streaming pipelines, including the ability to restart pipelines. This role adheres to the principle of least privilege, as it grants only the permissions required to manage and operate Dataflow jobs without unnecessary administrative access. Other roles, such as Dataflow Admin, would grant broader permissions, which are not needed in this scenario.

Dataflow開発者ロールは、パイプラインの再起動能力を含む、Dataflowストリーミングパイプラインを管理するために必要な権限を提供します。このロールは、不要な管理アクセスなしにDataflowジョブを管理・運用するために必要な権限のみを付与するため、最小権限の原則に準拠しています。Dataflow管理者などの他のロールは、このシナリオでは不要なより広範な権限を付与します。

-----

### <a name="no69"></a>**NO.69**

You need to create a new data pipeline. You want a serverless solution that meets the following requirements:

  * Data is streamed from Pub/Sub and is processed in real-time.
  * Data is transformed before being stored.
  * Data is stored in a location that will allow it to be analyzed with SQL using Looker.
    Which Google Cloud services should you recommend for the pipeline?

新しいデータパイプラインを作成する必要があります。以下の要件を満たすサーバーレスソリューションが必要です：

  * データはPub/Subからストリーミングされ、リアルタイムで処理される。
  * データは保存される前に変換される。
  * データはLookerを使用してSQLで分析できる場所に保存される。
    パイプラインにはどのGoogle Cloudサービスを推奨しますか？

A. 1. Dataproc Serverless 2. Bigtable
B. 1. Cloud Composer 2. Cloud SQL for MySQL
C. 1. BigQuery 2. Analytics Hub
D. 1. Dataflow 2. BigQuery

**正解: D**

**解説:**
To build a serverless data pipeline that processes data in real-time from Pub/Sub, transforms it, and stores it for SQL-based analysis using Looker, the best solution is to useDataflowandBigQuery.Dataflowis a fully managed service for real-time data processing and transformation, whileBigQueryis a serverless data warehouse that supports SQL-based querying and integrates seamlessly with Looker for data analysis and visualization. This combination meets the requirements for real-time streaming, transformation, and efficient storage for analytical queries.

Pub/Subからのデータをリアルタイムで処理し、変換し、Lookerを使用したSQLベースの分析のために保存するサーバーレスデータパイプラインを構築するには、DataflowとBigQueryを使用することが最善の解決策です。Dataflowはリアルタイムのデータ処理と変換のためのフルマネージドサービスであり、一方BigQueryはSQLベースのクエリをサポートし、データ分析と視覚化のためにLookerとシームレスに統合するサーバーレスデータウェアハウスです。この組み合わせは、リアルタイムストリーミング、変換、および分析クエリのための効率的なストレージの要件を満たします。

-----

### <a name="no70"></a>**NO.70**

Your company has an on-premises file server with 5 TB of data that needs to be migrated to Google Cloud. The network operations team has mandated that you can only use up to 250 Mbps of the total available bandwidth for the migration. You need to perform an online migration to Cloud Storage. What should you do?

あなたの会社には5TBのデータを持つオンプレミスのファイルサーバーがあり、Google Cloudに移行する必要があります。ネットワーク運用チームは、移行に利用可能な総帯域幅のうち最大250Mbpsしか使用できないと規定しています。Cloud Storageへのオンライン移行を実行する必要があります。どうすべきですか？

A. Use Storage Transfer Service to configure an agent-based transfer. Set the appropriate bandwidth limit for the agent pool.
B. Use the gcloud storage cp command to copy all files from on-premises to Cloud Storage using the - - daisy-chain option.
C. Request a Transfer Appliance, copy the data to the appliance, and ship it back to Google Cloud.
D. Use the gcloud storage cp command to copy all files from on-premises to Cloud Storage using the - -no- clobber option.

**正解: A**

**解説:**
Comprehensive and Detailed in Depth Explanation:
Why A is correct: Storage Transfer Service with agent-based transfer allows for online migrations and provides the ability to set bandwidth limits. Agents are installed on-premises and can be configured to respect network constraints.
Why other options are incorrect:B: The --daisy-chain option is not related to bandwidth control.
C: Transfer Appliance is for offline migrations and is not suitable for online transfers with bandwidth constraints.
D: The --no-clobber option prevents overwriting existing files but does not control bandwidth.

包括的で詳細な解説：
Aが正しい理由：エージェントベースの転送を伴うStorage Transfer Serviceは、オンライン移行を可能にし、帯域幅制限を設定する機能を提供します。エージェントはオンプレミスにインストールされ、ネットワークの制約を尊重するように構成できます。
他の選択肢が不適切な理由：B：--daisy-chainオプションは帯域幅制御とは関係ありません。
C：Transfer Applianceはオフライン移行用であり、帯域幅に制約のあるオンライン転送には適していません。
D：--no-clobberオプションは既存のファイルの上書きを防ぎますが、帯域幅を制御しません。

-----

### <a name="no71"></a>**NO.71**

Your organization uses Dataflow pipelines to process real-time financial transactions. You discover that one of your Dataflow jobs has failed. You need to troubleshoot the issue as quickly as possible. What should you do?

あなたの組織は、リアルタイムの金融取引を処理するためにDataflowパイプラインを使用しています。Dataflowジョブの1つが失敗したことを発見しました。できるだけ早く問題をトラブルシューティングする必要があります。どうすべきですか？

A. Set up a Cloud Monitoring dashboard to track key Dataflow metrics, such as data throughput, error rates, and resource utilization.
B. Create a custom script to periodically poll the Dataflow API for job status updates, and send email alerts if any errors are identified.
C. Navigate to the Dataflow Jobs page in the Google Cloud console. Use the job logs and worker logs to identify the error.
D. Use the gcloud CLI tool to retrieve job metrics and logs, and analyze them for errors and performance bottlenecks.

**正解: C**

**解説:**
To troubleshoot a failed Dataflow job as quickly as possible, you should navigate to theDataflow Jobs page in the Google Cloud console. The console provides access to detailed job logs and worker logs, which can help you identify the cause of the failure. The graphical interface also allows you to visualize pipeline stages, monitor performance metrics, and pinpoint where the error occurred, making it the most efficient way to diagnose and resolve the issue promptly.

失敗したDataflowジョブをできるだけ早くトラブルシューティングするには、Google CloudコンソールのDataflowジョブページに移動する必要があります。コンソールは詳細なジョブログとワーカロログへのアクセスを提供し、これらが障害の原因を特定するのに役立ちます。グラフィカルインターフェースでは、パイプラインのステージを視覚化し、パフォーマンスメトリクスを監視し、エラーが発生した場所を特定することもできるため、問題を迅速に診断・解決する最も効率的な方法です。

-----

### <a name="no72"></a>**NO.72**

You work for a financial services company that handles highly sensitive data. Due to regulatory requirements, your company is required to have complete and manual control of data encryption. Which type of keys should you recommend to use for data storage?

あなたは機密性の高いデータを扱う金融サービス会社で働いています。規制要件により、あなたの会社はデータ暗号化の完全かつ手動での制御を求められています。データストレージにはどのタイプのキーを使用することを推奨しますか？

A. Use customer-supplied encryption keys (CSEK).
B. Use a dedicated third-party key management system (KMS) chosen by the company.
C. Use Google-managed encryption keys (GMEK).
D. Use customer-managed encryption keys (CMEK).

**正解: A**

**解説:**
For regulatory requirements that mandate complete and manual control of data encryption, you should use customer-supplied encryption keys (CSEK). With CSEK, your company provides the encryption keys for data storage, and Google Cloud does not store or manage these keys. This approach ensures that your organization retains full control and responsibility over the encryption process, meeting strict regulatory compliance requirements.

データ暗号化の完全かつ手動での制御を義務付ける規制要件には、顧客指定の暗号化キー（CSEK）を使用すべきです。CSEKを使用すると、あなたの会社がデータストレージ用の暗号化キーを提供し、Google Cloudはこれらのキーを保存も管理もしません。このアプローチにより、あなたの組織は暗号化プロセスに対する完全な制御と責任を保持し、厳格な規制コンプライアンス要件を満たします。

-----

### <a name="no73"></a>**NO.73**

Your organization is conducting analysis on regional sales metrics. Data from each regional sales team is stored as separate tables in BigQuery and updated monthly. You need to create a solution that identifies the top three regions with the highest monthly sales for the next three months. You want the solution to automatically provide up-to-date results. What should you do?

あなたの組織は、地域の売上指標に関する分析を行っています。各地域の営業チームからのデータは、BigQueryに個別のテーブルとして保存され、毎月更新されます。今後3か月間の月間売上が最も高い上位3地域を特定するソリューションを作成する必要があります。ソリューションが自動的に最新の結果を提供するようにしたいと考えています。どうすべきですか？

A. Create a BigQuery table that performs a union across all of the regional sales tables. Use the row_number() window function to query the new table.
B. Create a BigQuery table that performs a cross join across all of the regional sales tables. Use the rank() window function to query the new table.
C. Create a BigQuery materialized view that performs a union across all of the regional sales tables. Use the rank() window function to query the new materialized view.
D. Create a BigQuery materialized view that performs a cross join across all of the regional sales tables. Use the row_number() window function to query the new materialized view.

**正解: C**

**解説:**
Comprehensive and Detailed in Depth Explanation:
Why C is correct: Materialized views in BigQuery are precomputed views that periodically cache the results of a query. This ensures up-to-date results automatically. A UNION is the correct operation to combine the data from multiple regional sales tables. RANK() function is correct to rank the sales regions. ROW_NUMBER() would create a unique number for each row, even if sales amount is the same, this is not the desired function.
Why other options are incorrect:A and B: Standard tables do not provide automatic updates.
D: A CROSS JOIN would produce a Cartesian product, which is not appropriate for combining regional sales data. Cross join is used when you want every combination of rows from tables, not a aggregation of data.

包括的で詳細な解説：
Cが正しい理由：BigQueryのマテリアライズドビューは、クエリの結果を定期的にキャッシュする事前計算済みビューです。これにより、自動的に最新の結果が保証されます。UNIONは、複数の地域の売上テーブルからデータを結合するための正しい操作です。RANK()関数は、売上地域をランク付けするのに正しいです。ROW_NUMBER()は、売上額が同じでも各行に一意の番号を作成するため、望ましい関数ではありません。
他の選択肢が不適切な理由：AとB：標準テーブルは自動更新を提供しません。
D：CROSS JOINはデカルト積を生成し、地域の売上データを結合するには不適切です。クロスジョインは、データの集計ではなく、テーブルからのすべての行の組み合わせが必要な場合に使用されます。

-----

### <a name="no74"></a>**NO.74**

You want to process and load a daily sales CSV file stored in Cloud Storage into BigQuery for downstream reporting. You need to quickly build a scalable data pipeline that transforms the data while providing insights into data quality issues. What should you do?

あなたは、下流のレポート作成のために、Cloud Storageに保存されている日次売上CSVファイルを処理してBigQueryにロードしたいと考えています。データ品質の問題に関する洞察を提供しながら、データを変換するスケーラブルなデータパイプラインを迅速に構築する必要があります。どうすべきですか？

A. Create a batch pipeline in Cloud Data Fusion by using a Cloud Storage source and a BigQuery sink.
B. Load the CSV file as a table in BigQuery, and use scheduled queries to run SQL transformation scripts.
C. Load the CSV file as a table in BigQuery. Create a batch pipeline in Cloud Data Fusion by using a BigQuery source and sink.
D. Create a batch pipeline in Dataflow by using the Cloud Storage CSV file to BigQuery batch template.

**正解: A**

**解説:**
Using Cloud Data Fusion to create a batch pipeline with a Cloud Storage source and a BigQuery sink is the best solution because:

  * Scalability: Cloud Data Fusion is a scalable, fully managed data integration service.
  * Data transformation: It provides a visual interface to design pipelines, enabling quick transformation of data.
  * Data quality insights: Cloud Data Fusion includes built-in tools for monitoring and addressing data quality issues during the pipeline creation and execution process.

Cloud StorageソースとBigQueryシンクを使用してCloud Data Fusionでバッチパイプラインを作成することが最善の解決策です。なぜなら：

  * スケーラビリティ：Cloud Data Fusionは、スケーラブルなフルマネージドのデータ統合サービスです。
  * データ変換：パイプラインを設計するための視覚的なインターフェースを提供し、迅速なデータ変換を可能にします。
  * データ品質の洞察：Cloud Data Fusionには、パイプラインの作成および実行プロセス中にデータ品質の問題を監視および対処するための組み込みツールが含まれています。

-----

### <a name="no75"></a>**NO.75**

You are designing a pipeline to process data files that arrive in Cloud Storage by 3:00 am each day. Data processing is performed in stages, where the output of one stage becomes the input of the next. Each stage takes a long time to run. Occasionally a stage fails, and you have to address the problem. You need to ensure that the final output is generated as quickly as possible. What should you do?

あなたは、毎日午前3時までにCloud Storageに到着するデータファイルを処理するパイプラインを設計しています。データ処理はステージごとに行われ、あるステージの出力が次のステージの入力となります。各ステージの実行には長い時間がかかります。時々ステージが失敗し、問題に対処する必要があります。最終的な出力ができるだけ早く生成されるようにする必要があります。どうすべきですか？

A. Design a Spark program that runs under Dataproc. Code the program to wait for user input when an error is detected. Rerun the last action after correcting any stage output data errors.
B. Design the pipeline as a set of PTransforms in Dataflow. Restart the pipeline after correcting any stage output data errors.
C. Design the workflow as a Cloud Workflow instance. Code the workflow to jump to a given stage based on an input parameter. Rerun the workflow after correcting any stage output data errors.
D. Design the processing as a directed acyclic graph (DAG) in Cloud Composer. Clear the state of the failed task after correcting any stage output data errors.

**正解: D**

**解説:**
UsingCloud Composerto design the processing pipeline as a Directed Acyclic Graph (DAG) is the most suitable approach because:

  * Fault tolerance: Cloud Composer (based on Apache Airflow) allows for handling failures at specific stages. You can clear the state of a failed task and rerun it without reprocessing the entire pipeline.
  * Stage-based processing: DAGs are ideal for workflows with interdependent stages where the output of one stage serves as input to the next.
  * Efficiency: This approach minimizes downtime and ensures that only failed stages are rerun, leading to faster final output generation.

Cloud Composerを使用して処理パイプラインを有向非巡回グラフ（DAG）として設計することが最も適切なアプローチです。なぜなら：

  * フォールトトレランス：Cloud Composer（Apache Airflowベース）は、特定のステージでの障害処理を可能にします。失敗したタスクの状態をクリアして、パイプライン全体を再処理することなく再実行できます。
  * ステージベースの処理：DAGは、あるステージの出力が次のステージの入力となる、相互に依存するステージを持つワークフローに理想的です。
  * 効率：このアプローチはダウンタイムを最小限に抑え、失敗したステージのみが再実行されることを保証し、最終的な出力の生成を高速化します。

-----

### <a name="no76"></a>**NO.76**

Your organization's website uses an on-premises MySQL as a backend database. You need to migrate the on- premises MySQL database to Google Cloud while maintaining MySQL features. You want to minimize administrative overhead and downtime. What should you do?

あなたの組織のウェブサイトは、バックエンドデータベースとしてオンプレミスのMySQLを使用しています。MySQLの機能を維持しながら、オンプレミスのMySQLデータベースをGoogle Cloudに移行する必要があります。管理オーバーヘッドとダウンタイムを最小限に抑えたいと考えています。どうすべきですか？

A. Install MySQL on a Compute Engine virtual machine. Export the database files using the mysqldump command. Upload the files to Cloud Storage, and import them into the MySQL instance on Compute Engine.
B. Use Database Migration Service to transfer the data to Cloud SQL for MySQL, and configure the on premises MySQL database as the source.
C. Use a Google-provided Dataflow template to replicate the MySQL database in BigQuery.
D. Export the database tables to CSV files, and upload the files to Cloud Storage. Convert the MySQL schema to a Spanner schema, create a JSON manifest file, and run a Google-provided Dataflow template to load the data into Spanner.

**正解: B**

**解説:**
Comprehensive and Detailed in Depth Explanation:
Why B is correct: Database Migration Service (DMS) is designed for migrating databases to Cloud SQL with minimal downtime and administrative overhead. Cloud SQL for MySQL is a fully managed MySQL service, which aligns with the requirement to minimize administrative overhead.
Why other options are incorrect:A: Installing MySQL on Compute Engine requires manual management of the database instance, which increases administrative overhead.
C: BigQuery is not a direct replacement for a relational MySQL database. It's an analytical data warehouse.
D: Spanner is a globally distributed, scalable database, but it requires schema conversion and is not a direct replacement for MySQL, and it is also much more complex than cloud SQL.

包括的で詳細な解説：
Bが正しい理由：Database Migration Service（DMS）は、最小限のダウンタイムと管理オーバーヘッドでデータベースをCloud SQLに移行するために設計されています。Cloud SQL for MySQLはフルマネージドのMySQLサービスであり、管理オーバーヘッドを最小限に抑えるという要件に合致します。
他の選択肢が不適切な理由：A：Compute EngineにMySQLをインストールすると、データベースインスタンスの手動管理が必要になり、管理オーバーヘッドが増加します。
C：BigQueryはリレーショナルなMySQLデータベースの直接的な代替品ではありません。分析用のデータウェアハウスです。
D：Spannerはグローバルに分散されたスケーラブルなデータベースですが、スキーマ変換が必要であり、MySQLの直接的な代替品ではなく、Cloud SQLよりもはるかに複雑です。

-----

### <a name="no77"></a>**NO.77**

Your company currently uses an on-premises network file system (NFS) and is migrating data to Google Cloud. You want to be able to control how much bandwidth is used by the data migration while capturing detailed reporting on the migration status. What should you do?

あなたの会社は現在、オンプレミスのネットワークファイルシステム（NFS）を使用しており、データをGoogle Cloudに移行中です。データ移行に使用される帯域幅を制御し、移行状況に関する詳細なレポートを取得できるようにしたいと考えています。どうすべきですか？

A. Use a Transfer Appliance.
B. Use Cloud Storage FUSE.
C. Use Storage Transfer Service.
D. Use gcloud storage commands

**正解: C**

**解説:**
Using theStorage Transfer Serviceis the best solution for migrating data from an on-premises NFS to Google Cloud. This service allows you to control bandwidth usage by configuring transfer speed limits and provides detailed reporting on the migration status. Storage Transfer Service is specifically designed for large-scale data migrations and supports scheduling, monitoring, and error handling, making it an efficient and reliable choice for your use case.

Storage Transfer Serviceを使用することが、オンプレミスのNFSからGoogle Cloudへデータを移行するための最善の解決策です。このサービスでは、転送速度制限を設定して帯域幅の使用量を制御し、移行状況に関する詳細なレポートを提供します。Storage Transfer Serviceは、大規模なデータ移行向けに特別に設計されており、スケジューリング、監視、エラー処理をサポートしているため、あなたのユースケースにとって効率的で信頼性の高い選択肢となります。

-----

### <a name="no78"></a>**NO.78**

You have millions of customer feedback records stored in BigQuery. You want to summarize the data by using the large language model (LLM) Gemini. You need to plan and execute this analysis using the most efficient approach. What should you do?

あなたは、BigQueryに保存されている数百万の顧客フィードバックレコードを持っています。大規模言語モデル（LLM）のGeminiを使用してデータを要約したいと考えています。最も効率的なアプローチを使用して、この分析を計画および実行する必要があります。どうすべきですか？

A. Query the BigQuery table from within a Python notebook, use the Gemini API to summarize the data within the notebook, and store the summaries in BigQuery.
B. Use a BigQuery ML model to pre-process the text data, export the results to Cloud Storage, and use the Gemini API to summarize the pre- processed data.
C. Create a BigQuery Cloud resource connection to a remote model in Vertex Al, and use Gemini to summarize the data.
D. Export the raw BigQuery data to a CSV file, upload it to Cloud Storage, and use the Gemini API to summarize the data.

**正解: C**

**解説:**
Creating aBigQuery Cloud resource connectionto a remote model inVertex Aland using Gemini to summarize the data is the most efficient approach. This method allows you to seamlessly integrate BigQuery with the Gemini model via Vertex Al, avoiding the need to export data or perform manual steps. It ensures scalability for large datasets and minimizes data movement, leveraging Google Cloud's ecosystem for efficient data summarization and storage.

Vertex AIのリモートモデルへのBigQuery Cloudリソース接続を作成し、Geminiを使用してデータを要約することが最も効率的なアプローチです。この方法により、データをエクスポートしたり手動の手順を実行したりする必要なく、Vertex AIを介してBigQueryをGeminiモデルとシームレスに統合できます。これにより、大規模なデータセットのスケーラビリティが確保され、データ移動が最小限に抑えられ、Google Cloudのエコシステムを活用して効率的なデータ要約と保存が可能になります。

-----

### <a name="no79"></a>**NO.79**

Your company uses Looker as its primary business intelligence platform. You want to use LookML to visualize the profit margin for each of your company's products in your Looker Explores and dashboards. You need to implement a solution quickly and efficiently. What should you do?

あなたの会社は、主要なビジネスインテリジェンスプラットフォームとしてLookerを使用しています。LookerのExploreとダッシュボードで、会社の各製品の利益率を視覚化するためにLookMLを使用したいと考えています。迅速かつ効率的にソリューションを実装する必要があります。どうすべきですか？

A. Create a derived table that pre-calculates the profit margin for each product, and include it in the Looker model.
B. Define a new measure that calculates the profit margin by using the existing revenue and cost fields.
C. Create a new dimension that categorizes products based on their profit margin ranges (e.g., high, medium, low).
D. Apply a filter to only show products with a positive profit margin.

**正解: B**

**解説:**
Defining a newmeasurein LookML to calculate the profit margin using the existing revenue and cost fields is the most efficient and straightforward solution. This approach allows you to dynamically compute the profit margin directly within your Looker Explores and dashboards without needing to pre-calculate or create additional tables.

既存の収益フィールドとコストフィールドを使用して利益率を計算する新しいメジャーをLookMLで定義することが、最も効率的で簡単な解決策です。このアプローチにより、事前に計算したり追加のテーブルを作成したりすることなく、LookerのExploreとダッシュボード内で直接利益率を動的に計算できます。

-----

### <a name="no80"></a>**NO.80**

You manage data at an ecommerce company. You have a Dataflow pipeline that processes order data from Pub/Sub, enriches the data with product information from Bigtable, and writes the processed data to BigQuery for analysis. The pipeline runs continuously and processes thousands of orders every minute. You need to monitor the pipeline's performance and be alerted if errors occur. What should you do?

あなたはeコマース企業でデータを管理しています。Pub/Subからの注文データを処理し、Bigtableからの製品情報でデータをエンリッチ化し、処理済みのデータを分析のためにBigQueryに書き込むDataflowパイプラインを持っています。パイプラインは継続的に実行され、毎分何千もの注文を処理します。パイプラインのパフォーマンスを監視し、エラーが発生した場合にアラートを受け取る必要があります。どうすべきですか？

A. Use Cloud Monitoring to track key metrics. Create alerting policies in Cloud Monitoring to trigger notifications when metrics exceed thresholds or when errors occur.
B. Use the Dataflow job monitoring interface to visually inspect the pipeline graph, check for errors, and configure notifications when critical errors occur.
C. Use BigQuery to analyze the processed data in Cloud Storage and identify anomalies or inconsistencies. Set up scheduled alerts based when anomalies or inconsistencies occur.
D. Use Cloud Logging to view the pipeline logs and check for errors. Set up alerts based on specific keywords in the logs.

**正解: A**

**解説:**
Comprehensive and Detailed in Depth Explanation:
Why A is correct: Cloud Monitoring is the recommended service for monitoring Google Cloud services, including Dataflow. It allows you to track key metrics like system lag, element throughput, and error rates. Alerting policies in Cloud Monitoring can trigger notifications based on metric thresholds.
Why other options are incorrect:B: The Dataflow job monitoring interface is useful for visualization, but Cloud Monitoring provides more comprehensive alerting.
C: BigQuery is for analyzing the processed data, not monitoring the pipeline itself. Also Cloud Storage is not where the data resides during processing.
D: Cloud Logging is useful for viewing logs, but Cloud Monitoring is better for metric-based alerting.

包括的で詳細な解説：
Aが正しい理由：Cloud Monitoringは、Dataflowを含むGoogle Cloudサービスを監視するための推奨サービスです。システムラグ、要素スループット、エラーレートなどの主要なメトリクスを追跡できます。Cloud Monitoringのアラートポリシーは、メトリクスのしきい値に基づいて通知をトリガーできます。
他の選択肢が不適切な理由：B：Dataflowジョブ監視インターフェースは視覚化に役立ちますが、Cloud Monitoringはより包括的なアラートを提供します。
C：BigQueryは処理済みデータを分析するためのものであり、パイプライン自体を監視するためのものではありません。また、データは処理中にCloud Storageに存在しません。
D：Cloud Loggingはログの表示に役立ちますが、メトリクスベースのアラートにはCloud Monitoringの方が優れています。

-----

### <a name="no81"></a>**NO.81**

Your organization is building a new application on Google Cloud. Several data files will need to be stored in Cloud Storage. Your organization has approved only two specific cloud regions where these data files can reside. You need to determine a Cloud Storage bucket strategy that includes automated high availability. What should you do?

あなたの組織は、Google Cloud上で新しいアプリケーションを構築しています。いくつかのデータファイルをCloud Storageに保存する必要があります。組織は、これらのデータファイルが常駐できる2つの特定のクラウドリジョンのみを承認しています。自動化された高可用性を含むCloud Storageバケット戦略を決定する必要があります。どうすべきですか？

A. Create a dual-region bucket, and upload the files to this bucket.
B. Create a single-region bucket in each of the two regions, and use the gcloud storage command to replicate the data across the buckets in both regions.
C. Create a multi-region bucket, and upload the files to this bucket.
D. Create a single-region bucket in each of the two regions, and use Storage Transfer Service to replicate the data across the buckets in both regions.

**正解: A**

**解説:**
Comprehensive and Detailed In-Depth Explanation:
The strategy requires storage in two specific regions with automated high availability (HA). Cloud Storage location options dictate the solution:

  * Option A: A dual-region bucket (e.g., us-west1 and us-east1) replicates data synchronously across two user-specified regions, ensuring HA without manual intervention. It's fully automated and meets the requirement.
  * Option B: Two single-region buckets with gcloud storage replication is manual, not automated, and lacks real-time HA (requires scripting and monitoring).
  * Option C: Multi-region buckets (e.g., us) span multiple regions within a geography but don't let you specify exactly two regions, potentially violating the restriction.

包括的で詳細な解説：
戦略は、2つの特定のリージョンでのストレージと自動化された高可用性（HA）を必要とします。Cloud Storageのロケーションオプションが解決策を決定します：

  * 選択肢A：デュアルリージョンバケット（例：us-west1とus-east1）は、ユーザーが指定した2つのリージョン間でデータを同期的に複製し、手動介入なしでHAを保証します。完全に自動化されており、要件を満たしています。
  * 選択肢B：gcloud storageレプリケーションを使用した2つのシングルリージョンバケットは手動であり、自動化されておらず、リアルタイムのHAがありません（スクリプト作成と監視が必要）。
  * 選択肢C：マルチリージョンバケット（例：us）は地理内の複数のリージョンにまたがりますが、正確に2つのリージョンを指定できないため、制限に違反する可能性があります。

-----

### <a name="no82"></a>**NO.82**

Your retail company wants to analyze customer reviews to understand sentiment and identify areas for improvement. Your company has a large dataset of customer feedback text stored in BigQuery that includes diverse language patterns, emojis, and slang. You want to build a solution to classify customer sentiment from the feedback text. What should you do?

あなたの小売会社は、顧客レビューを分析して感情を理解し、改善点を特定したいと考えています。BigQueryに保存されている顧客フィードバックのテキストの大きなデータセットがあり、多様な言語パターン、絵文字、スラングが含まれています。フィードバックのテキストから顧客の感情を分類するソリューションを構築したいと考えています。どうすべきですか？

A. Preprocess the text data in BigQuery using SQL functions. Export the processed data to AutoML Natural Language for model training and deployment.
B. Export the raw data from BigQuery. Use AutoML Natural Language to train a custom sentiment analysis model.
C. Use Dataproc to create a Spark cluster, perform text preprocessing using Spark NLP, and build a sentiment analysis model with Spark MLlib.
D. Develop a custom sentiment analysis model using TensorFlow. Deploy it on a Compute Engine instance.

**正解: B**

**解説:**
Comprehensive and Detailed in Depth Explanation:
Why B is correct: AutoML Natural Language is designed for text classification tasks, including sentiment analysis, and can handle diverse language patterns without extensive preprocessing. AutoML can train a custom model with minimal coding.
Why other options are incorrect:A: Unnecessary extra preprocessing. AutoML can handle the raw data.
C: Dataproc and Spark are overkill for this task. AutoML is more efficient and easier to use.
D: Developing a custom TensorFlow model requires significant expertise and time, which is not efficient for this scenario.

包括的で詳細な解説：
Bが正しい理由：AutoML Natural Languageは、感情分析を含むテキスト分類タスク用に設計されており、広範な前処理なしで多様な言語パターンを処理できます。AutoMLは最小限のコーディングでカスタムモデルをトレーニングできます。
他の選択肢が不適切な理由：A：不要な追加の前処理です。AutoMLは生データを処理できます。
C：このタスクにはDataprocとSparkは過剰です。AutoMLの方が効率的で使いやすいです。
D：カスタムのTensorFlowモデルを開発するには、かなりの専門知識と時間が必要であり、このシナリオでは効率的ではありません。

-----

### <a name="no83"></a>**NO.83**

You created a customer support application that sends several forms of data to Google Cloud. Your application is sending:

1.  Audio files from phone interactions with support agents that will be accessed during trainings.
2.  CSV files of users' personally identifiable information (PII) that will be analyzed with SQL.
3.  A large volume of small document files that will power other applications.
    You need to select the appropriate tool for each data type given the required use case, while following Google- recommended practices. Which should you choose?

あなたは、いくつかの形式のデータをGoogle Cloudに送信するカスタマーサポートアプリケーションを作成しました。アプリケーションは以下を送信しています：

1.  トレーニング中にアクセスされる、サポートエージェントとの電話対話の音声ファイル。
2.  SQLで分析される、ユーザーの個人を特定できる情報（PII）のCSVファイル。
3.  他のアプリケーションを動かすための、大量の小さなドキュメントファイル。
    必要なユースケースを考慮し、Googleが推奨するプラクティスに従いながら、各データタイプに適切なツールを選択する必要があります。どれを選択すべきですか？

A. 1. Cloud Storage 2. CloudSQL for PostgreSQL 3. Bigtable
B. 1. Filestore 2. Cloud SQL for PostgreSQL 3. Datastore
C. 1. Cloud Storage 2. BigQuery 3. Firestore
D. 1. Filestore 2. Bigtable 3. BigQuery

**正解: C**

**解説:**
Audio files from phone interactions: Use Cloud Storage. Cloud Storage is ideal for storing large binary objects like audio files, offering scalability and easy accessibility for training purposes.
CSV files of users' personally identifiable information (PII): Use BigQuery. BigQuery is a serverless data warehouse optimized for analyzing structured data, such as CSV files, using SQL. It ensures compliance with PIl handling through access controls and data encryption.
A large volume of small document files: Use Firestore. Firestore is a scalable NoSQL database designed for applications requiring fast, real-time interactions and structured document storage, making it suitable for powering other applications.

電話対話の音声ファイル：Cloud Storageを使用します。Cloud Storageは音声ファイルのような大きなバイナリオブジェクトを保存するのに理想的で、スケーラビリティとトレーニング目的での簡単なアクセス性を提供します。
ユーザーの個人を特定できる情報（PII）のCSVファイル：BigQueryを使用します。BigQueryは、SQLを使用してCSVファイルのような構造化データを分析するために最適化されたサーバーレスのデータウェアハウスです。アクセス制御とデータ暗号化を通じてPIIの取り扱いに関するコンプライアンスを保証します。
大量の小さなドキュメントファイル：Firestoreを使用します。Firestoreは、高速なリアルタイムの対話と構造化されたドキュメントストレージを必要とするアプリケーション向けに設計されたスケーラブルなNoSQLデータベースであり、他のアプリケーションを動かすのに適しています。

-----

### <a name="no84"></a>**NO.84**

You have a Dataproc cluster that performs batch processing on data stored in Cloud Storage. You need to schedule a daily Spark job to generate a report that will be emailed to stakeholders. You need a fully-managed solution that is easy to implement and minimizes complexity. What should you do?

あなたは、Cloud Storageに保存されているデータに対してバッチ処理を実行するDataprocクラスタを持っています。利害関係者にメールで送信されるレポートを生成するための日次Sparkジョブをスケジュールする必要があります。実装が簡単で複雑さを最小限に抑える、フルマネージドのソリューションが必要です。どうすべきですか？

A. Use Cloud Composer to orchestrate the Spark job and email the report.
B. Use Dataproc workflow templates to define and schedule the Spark job, and to email the report.
C. Use Cloud Run functions to trigger the Spark job and email the report.
D. Use Cloud Scheduler to trigger the Spark job. and use Cloud Run functions to email the report.

**正解: B**

**解説:**
Using Dataproc workflow templatesis a fully-managed and straightforward solution for defining and scheduling your Spark job on a Dataproc cluster. Workflow templates allow you to automate the execution of Spark jobs with predefined steps, including data processing and report generation. You can integrate email notifications by adding a step to the workflow that sends the report using tools like a Cloud Function or external email service. This approach minimizes complexity while leveraging Dataproc's managed capabilities for batch processing.

Dataprocワークフローテンプレートを使用することが、Dataprocクラスタ上のSparkジョブを定義・スケジュールするための、フルマネージドで簡単なソリューションです。ワークフローテンプレートを使用すると、データ処理やレポート生成を含む事前定義されたステップでSparkジョブの実行を自動化できます。Cloud Functionや外部メールサービスのようなツールを使用してレポートを送信するステップをワークフローに追加することで、メール通知を統合できます。このアプローチは、Dataprocのマネージド機能をバッチ処理に活用しつつ、複雑さを最小限に抑えます。

-----

### <a name="no85"></a>**NO.85**

You have millions of customer feedback records stored in BigQuery. You want to summarize the data by using the large language model (LLM) Gemini. You need to plan and execute this analysis using the most efficient approach. What should you do?

あなたは、BigQueryに保存されている数百万の顧客フィードバックレコードを持っています。大規模言語モデル（LLM）のGeminiを使用してデータを要約したいと考えています。最も効率的なアプローチを使用して、この分析を計画および実行する必要があります。どうすべきですか？

A. Use a BigQuery ML model to pre-process the text data, export the results to Cloud Storage, and use the Gemini API to summarize the pre- processed data.
B. Query the BigQuery table from within a Python notebook, use the Gemini API to summarize the data within the notebook, and store the summaries in BigQuery.
C. Create a BigQuery Cloud resource connection to a remote model in Vertex Al, and use Gemini to summarize the data.
D. Export the raw BigQuery data to a CSV file, upload it to Cloud Storage, and use the Gemini API to summarize the data.

**正解: C**

**解説:**
Creating aBigQuery Cloud resource connectionto a remote model inVertex Aland using Gemini to summarize the data is the most efficient approach. This method allows you to seamlessly integrate BigQuery with the Gemini model via Vertex Al, avoiding the need to export data or perform manual steps. It ensures scalability for large datasets and minimizes data movement, leveraging Google Cloud's ecosystem for efficient data summarization and storage.

Vertex AIのリモートモデルへのBigQuery Cloudリソース接続を作成し、Geminiを使用してデータを要約することが最も効率的なアプローチです。この方法により、データをエクスポートしたり手動の手順を実行したりする必要なく、Vertex AIを介してBigQueryをGeminiモデルとシームレスに統合できます。これにより、大規模なデータセットのスケーラビリティが確保され、データ移動が最小限に抑えられ、Google Cloudのエコシステムを活用して効率的なデータ要約と保存が可能になります。

-----

### <a name="no86"></a>**NO.86**

You are working on a data pipeline that will validate and clean incoming data before loading it into BigQuery for real-time analysis. You want to ensure that the data validation and cleaning is performed efficiently and can handle high volumes of data. What should you do?

あなたは、リアルタイム分析のためにBigQueryにロードする前に、受信データを検証・クレンジングするデータパイプラインに取り組んでいます。データ検証とクレンジングが効率的に実行され、大量のデータを処理できることを保証したいと考えています。どうすべきですか？

A. Write custom scripts in Python to validate and clean the data outside of Google Cloud. Load the cleaned data into BigQuery.
B. Use Cloud Run functions to trigger data validation and cleaning routines when new data arrives in Cloud Storage.
C. Use Dataflow to create a streaming pipeline that includes validation and transformation steps.
D. Load the raw data into BigQuery using Cloud Storage as a staging area, and use SQL queries in BigQuery to validate and clean the data.

**正解: C**

**解説:**
Using Dataflowto create a streaming pipeline that includes validation and transformation steps is the most efficient and scalable approach for real-time analysis. Dataflow is optimized for high-volume data processing and allows you to apply validation and cleaning logic as the data flows through the pipeline. This ensures that only clean, validated data is loaded into BigQuery, supporting real-time analysis while handling high data volumes effectively.

検証と変換のステップを含むストリーミングパイプラインをDataflowで作成することが、リアルタイム分析のための最も効率的でスケーラブルなアプローチです。Dataflowは大量のデータ処理に最適化されており、データがパイプラインを流れる際に検証とクレンジングのロジックを適用できます。これにより、クリーンで検証済みのデータのみがBigQueryにロードされ、大量のデータを効果的に処理しながらリアルタイム分析をサポートします。

-----

### <a name="no87"></a>**NO.87**

Your organization uses a BigQuery table that is partitioned by ingestion time. You need to remove data that is older than one year to reduce your organization's storage costs. You want to use the most efficient approach while minimizing cost. What should you do?

あなたの組織は、取り込み時間でパーティション分割されたBigQueryテーブルを使用しています。組織のストレージコストを削減するために、1年以上古いデータを削除する必要があります。コストを最小限に抑えながら、最も効率的なアプローチを使用したいと考えています。どうすべきですか？

A. Create a scheduled query that periodically runs an update statement in SQL that sets the "deleted" column to "yes" for data that is more than one year old. Create a view that filters out rows that have been marked deleted.
B. Create a view that filters out rows that are older than one year.
C. Require users to specify a partition filter using the alter table statement in SQL.
D. Set the table partition expiration period to one year using the ALTER TABLE statement in SQL.

**正解: D**

**解説:**
Setting thetable partition expiration periodto one year using theALTER TABLEstatement is the most efficient and cost-effective approach. This automatically deletes data in partitions older than one year, reducing storage costs without requiring manual intervention or additional queries. It minimizes administrative overhead and ensures compliance with your data retention policy while optimizing storage usage in BigQuery.

ALTER TABLEステートメントを使用してテーブルのパーティション有効期限を1年に設定することが、最も効率的で費用対効果の高いアプローチです。これにより、1年以上古いパーティションのデータが自動的に削除され、手動介入や追加のクエリを必要とせずにストレージコストが削減されます。管理オーバーヘッドを最小限に抑え、データ保持ポリシーへの準拠を保証しながら、BigQueryのストレージ使用量を最適化します。

-----

### <a name="no88"></a>**NO.88**

You need to design a data pipeline to process large volumes of raw server log data stored in Cloud Storage. The data needs to be cleaned, transformed, and aggregated before being loaded into BigQuery for analysis. The transformation involves complex data manipulation using Spark scripts that your team developed. You need to implement a solution that leverages your team's existing skillset, processes data at scale, and minimizes cost. What should you do?

あなたは、Cloud Storageに保存されている大量の生サーバーログデータを処理するデータパイプラインを設計する必要があります。データは、分析のためにBigQueryにロードされる前に、クレンジング、変換、集計される必要があります。変換には、あなたのチームが開発したSparkスクリプトを使用した複雑なデータ操作が含まれます。チームの既存のスキルセットを活用し、大規模にデータを処理し、コストを最小限に抑えるソリューションを実装する必要があります。どうすべきですか？

A. Use Dataflow with a custom template for the transformation logic.
B. Use Cloud Data Fusion to visually design and manage the pipeline.
C. Use Dataform to define the transformations in SQLX.
D. Use Dataproc to run the transformations on a cluster.

**正解: D**

**解説:**
Comprehensive and Detailed In-Depth Explanation:
The pipeline must handle large-scale log processing with existing Spark scripts, prioritizing skillset reuse, scalability, and cost. Let's break it down:

  * Option A: Dataflow uses Apache Beam, not Spark, requiring script rewrites (losing skillset leverage). Custom templates scale well but increase development cost and effort.
  * Option B: Cloud Data Fusion is a visual ETL tool, not Spark-based. It doesn't reuse existing scripts, requiring redesign, and is less cost-efficient for complex, code-driven transformations.
  * Option C: Dataform uses SQLX for BigQuery ELT, not Spark. It's unsuitable for pre-load transformations of raw logs and doesn't leverage Spark skills.

包括的で詳細な解説：
パイプラインは、既存のSparkスクリプトを使用した大規模なログ処理を処理し、スキルセットの再利用、スケーラビリティ、コストを優先する必要があります。分析しましょう：

  * 選択肢A：DataflowはSparkではなくApache Beamを使用するため、スクリプトの書き換えが必要となり、スキルセットの活用が失われます。カスタムテンプレートはスケーラビリティに優れていますが、開発コストと労力が増加します。
  * 選択肢B：Cloud Data Fusionは視覚的なETLツールであり、Sparkベースではありません。既存のスクリプトを再利用せず、再設計が必要で、複雑なコード駆動の変換には費用対効果が低いです。
  * 選択肢C：DataformはBigQuery ELTにSQLXを使用し、Sparkではありません。生ログのロード前変換には不向きで、Sparkスキルを活用できません。

-----

### <a name="no89"></a>**NO.89**

Your team uses Google Sheets to track budget data that is updated daily. The team wants to compare budget data against actual cost data, which is stored in a BigQuery table. You need to create a solution that calculates the difference between each day's budget and actual costs. You want to ensure that your team has access to daily-updated results in Google Sheets. What should you do?

あなたのチームは、毎日更新される予算データを追跡するためにGoogle Sheetsを使用しています。チームは、BigQueryテーブルに保存されている実績コストデータと予算データを比較したいと考えています。各日の予算と実績コストの差額を計算するソリューションを作成する必要があります。チームがGoogle Sheetsで毎日更新される結果にアクセスできるようにしたいと考えています。どうすべきですか？

A. Create a BigQuery external table by using the Drive URI of the Google sheet, and join the actual cost table with it. Save the joined table as a CSV file and open the file in Google Sheets.
B. Download the budget data as a CSV file and upload the CSV file to a Cloud Storage bucket. Create a new BigQuery table from Cloud Storage, and join the actual cost table with it. Open the joined BigQuery table by using Connected Sheets.
C. Download the budget data as a CSV file, and upload the CSV file to create a new BigQuery table. Join the actual cost table with the new BigQuery table, and save the results as a CSV file. Open the CSV file in Google Sheets.
D. Create a BigQuery external table by using the Drive URI of the Google sheet, and join the actual cost table with it. Save the joined table, and open it by using Connected Sheets.

**正解: D**

**解説:**
Comprehensive and Detailed in Depth Explanation:
Why D is correct: Creating a BigQuery external table directly from the Google Sheet allows for real- time updates. Joining the external table with the actual cost table in BigQuery performs the calculation. Connected Sheets allows the team to access and analyze the results directly in Google Sheets, with the data being updated.
Why other options are incorrect:A: Saving as a CSV file loses the live connection and daily updates.
B: Downloading and uploading as a CSV file adds unnecessary steps and loses the live connection.
C: Same issue as B, losing the live connection.

包括的で詳細な解説：
Dが正しい理由：Google Sheetから直接BigQuery外部テーブルを作成すると、リアルタイムの更新が可能になります。外部テーブルと実績コストテーブルをBigQueryで結合すると、計算が実行されます。Connected Sheetsを使用すると、チームはデータが更新された状態で、Google Sheetsで直接結果にアクセスして分析できます。
他の選択肢が不適切な理由：A：CSVファイルとして保存すると、ライブ接続と日々の更新が失われます。
B：CSVファイルとしてダウンロードしてアップロードすると、不要な手順が追加され、ライブ接続が失われます。
C：Bと同じ問題で、ライブ接続が失われます。

-----

### <a name="no90"></a>**NO.90**

You need to create a data pipeline that streams event information from applications in multiple Google Cloud regions into BigQuery for near real-time analysis. The data requires transformation before loading. You want to create the pipeline using a visual interface. What should you do?

あなたは、複数のGoogle Cloudリージョンにあるアプリケーションからのイベント情報を、ほぼリアルタイム分析のためにBigQueryにストリーミングするデータパイプラインを作成する必要があります。データはロード前に変換が必要です。視覚的なインターフェースを使用してパイプラインを作成したいと考えています。どうすべきですか？

A. Push event information to a Pub/Sub topic. Create a Dataflow job using the Dataflow job builder.
B. Push event information to a Pub/Sub topic. Create a Cloud Run function to subscribe to the Pub/Sub topic, apply transformations, and insert the data into BigQuery.
C. Push event information to a Pub/Sub topic. Create a BigQuery subscription in Pub/Sub.
D. Push event information to Cloud Storage, and create an external table in BigQuery. Create a BigQuery scheduled job that executes once each day to apply transformations.

**正解: A**

**解説:**
Pushing event information to aPub/Sub topicand then creating aDataflow job using the Dataflow job builderis the most suitable solution. The Dataflow job builder provides a visual interface to design pipelines, allowing you to define transformations and load data into BigQuery. This approach is ideal for streaming data pipelines that require near real-time transformations and analysis. It ensures scalability across multiple regions and integrates seamlessly with Pub/Sub for event ingestion and BigQuery for analysis.

イベント情報をPub/Subトピックにプッシュし、その後Dataflowジョブビルダーを使用してDataflowジョブを作成することが最も適切な解決策です。Dataflowジョブビルダーはパイプラインを設計するための視覚的なインターフェースを提供し、変換を定義してデータをBigQueryにロードすることができます。このアプローチは、ほぼリアルタイムの変換と分析を必要とするストリーミングデータパイプラインに理想的です。複数のリージョンにわたるスケーラビリティを保証し、イベント取り込みのためのPub/Subと分析のためのBigQueryとシームレスに統合します。

-----

### <a name="no91"></a>**NO.91**

Your company is migrating their batch transformation pipelines to Google Cloud. You need to choose a solution that supports programmatic transformations using only SQL. You also want the technology to support Git integration for version control of your pipelines. What should you do?

あなたの会社は、バッチ変換パイプラインをGoogle Cloudに移行しています。SQLのみを使用してプログラムによる変換をサポートするソリューションを選択する必要があります。また、パイプラインのバージョン管理のためにGit統合をサポートする技術も求めています。どうすべきですか？

A. Use Cloud Data Fusion pipelines.
B. Use Dataform workflows.
C. Use Dataflow pipelines.
D. Use Cloud Composer operators.

**正解: B**

**解説:**
Dataform workflowsare the ideal solution for migrating batch transformation pipelines to Google Cloud when you want to perform programmatic transformations using only SQL. Dataform allows you to define SQL- based workflows for data transformations and supports Git integration for version control, enabling collaboration and version tracking of your pipelines. This approach is purpose-built for SQL-driven data pipeline management and aligns perfectly with your requirements.

SQLのみを使用してプログラムによる変換を実行したい場合、バッチ変換パイプラインをGoogle Cloudに移行するにはDataformワークフローが理想的なソリューションです。Dataformを使用すると、データ変換のためのSQLベースのワークフローを定義でき、バージョン管理のためにGit統合をサポートしているため、パイプラインの共同作業とバージョン追跡が可能になります。このアプローチは、SQL駆動のデータパイプライン管理のために特別に構築されており、あなたの要件に完全に合致しています。

-----

### <a name="no92"></a>**NO.92**

You are working with a small dataset in Cloud Storage that needs to be transformed and loaded into BigQuery for analysis. The transformation involves simple filtering and aggregation operations. You want to use the most efficient and cost-effective data manipulation approach. What should you do?

あなたは、Cloud Storageにある小さなデータセットを扱っており、それを変換して分析のためにBigQueryにロードする必要があります。変換には、単純なフィルタリングと集計操作が含まれます。最も効率的で費用対効果の高いデータ操作アプローチを使用したいと考えています。どうすべきですか？

A. Use Dataproc to create an Apache Hadoop cluster, perform the ETL process using Apache Spark, and load the results into BigQuery.
B. Use BigQuery's SQL capabilities to load the data from Cloud Storage, transform it, and store the results in a new BigQuery table.
C. Create a Cloud Data Fusion instance and visually design an ETL pipeline that reads data from Cloud Storage, transforms it using built-in transformations, and loads the results into BigQuery.
D. Use Dataflow to perform the ETL process that reads the data from Cloud Storage, transforms it using Apache Beam, and writes the results to BigQuery.

**正解: B**

**解説:**
Comprehensive and Detailed In-Depth Explanation:
For a small dataset with simple transformations (filtering, aggregation), Google recommends leveraging BigQuery's native SQL capabilities to minimize cost and complexity.

  * Option A: Dataproc with Spark is overkill for a small dataset, incurring cluster management costs and setup time.
  * Option B: BigQuery can load data directly from Cloud Storage (e.g., CSV, JSON) and perform transformations using SQL in a serverless manner, avoiding additional service costs. This is the most efficient and cost-effective approach.
  * Option C: Cloud Data Fusion is suited for complex ETL but adds overhead (instance setup, Ul design) unnecessary for simple tasks.

包括的で詳細な解説：
単純な変換（フィルタリング、集計）を伴う小さなデータセットの場合、Googleはコストと複雑さを最小限に抑えるためにBigQueryのネイティブSQL機能を活用することを推奨しています。

  * 選択肢A：小さなデータセットにDataprocとSparkを使用するのは過剰であり、クラスタ管理コストとセットアップ時間が発生します。
  * 選択肢B：BigQueryはCloud Storageから直接データをロードし（例：CSV、JSON）、追加のサービスコストを回避しながらサーバーレスでSQLを使用して変換を実行できます。これが最も効率的で費用対効果の高いアプローチです。
  * 選択肢C：Cloud Data Fusionは複雑なETLに適していますが、単純なタスクには不要なオーバーヘッド（インスタンスのセットアップ、UIデザイン）が加わります。

-----

### <a name="no93"></a>**NO.93**

Your organization's ecommerce website collects user activity logs using a Pub/Sub topic. Your organization's leadership team wants a dashboard that contains aggregated user engagement metrics. You need to create a solution that transforms the user activity logs into aggregated metrics, while ensuring that the raw data can be easily queried. What should you do?

あなたの組織のeコマースウェブサイトは、Pub/Subトピックを使用してユーザーアクティビティログを収集しています。組織のリーダーシップチームは、集計されたユーザーエンゲージメント指標を含むダッシュボードを求めています。ユーザーアクティビティログを集計指標に変換し、かつ生データも簡単にクエリできるようにするソリューションを作成する必要があります。どうすべきですか？

A. Create a Dataflow subscription to the Pub/Sub topic, and transform the activity logs. Load the transformed data into a BigQuery table for reporting.
B. Create an event-driven Cloud Run function to trigger a data transformation pipeline to run. Load the transformed activity logs into a BigQuery table for reporting.
C. Create a Cloud Storage subscription to the Pub/Sub topic. Load the activity logs into a bucket using the Avro file format. Use Dataflow to transform the data, and load it into a BigQuery table for reporting.
D. Create a BigQuery subscription to the Pub/Sub topic, and load the activity logs into the table. Create a materialized view in BigQuery using SQL to transform the data for reporting

**正解: A**

**解説:**
Using Dataflowto subscribe to the Pub/Sub topic and transform the activity logs is the best approach for this scenario. Dataflow is a managed service designed for processing and transforming streaming data in real time. It allows you to aggregate metrics from the raw activity logs efficiently and load the transformed data into a BigQuery table for reporting. This solution ensures scalability, supports real-time processing, and enables querying of both raw and aggregated data in BigQuery, providing the flexibility and insights needed for the dashboard.

このシナリオでは、Dataflowを使用してPub/Subトピックをサブスクライブし、アクティビティログを変換するのが最善のアプローチです。Dataflowは、ストリーミングデータをリアルタイムで処理および変換するために設計されたマネージドサービスです。これにより、生のアクティビティログから効率的にメトリクスを集計し、変換されたデータをレポート作成のためにBigQueryテーブルにロードできます。このソリューションはスケーラビリティを確保し、リアルタイム処理をサポートし、BigQueryで生データと集計データの両方をクエリできるため、ダッシュボードに必要な柔軟性と洞察を提供します。

-----

### <a name="no94"></a>**NO.94**

Your retail organization stores sensitive application usage data in Cloud Storage. You need to encrypt the data without the operational overhead of managing encryption keys. What should you do?

あなたの小売組織は、機密性の高いアプリケーション利用データをCloud Storageに保存しています。暗号化キーを管理する運用オーバーヘッドなしでデータを暗号化する必要があります。どうすべきですか？

A. Use Google-managed encryption keys (GMEK).
B. Use customer-managed encryption keys (CMEK).
C. Use customer-supplied encryption keys (CSEK).
D. Use customer-supplied encryption keys (CSEK) for the sensitive data and customer-managed encryption keys (CMEK) for the less sensitive date

**正解: A**

**解説:**
Using Google-managed encryption keys (GMEK) is the best choice when you want to encrypt sensitive data in Cloud Storage without the operational overhead of managing encryption keys. GMEK is the default encryption mechanism in Google Cloud, and it ensures that data is automatically encrypted at rest with no additional setup or maintenance required. It provides strong security while eliminating the need for manual key management.

暗号化キーを管理する運用オーバーヘッドなしで機密データをCloud Storageに暗号化したい場合、Google管理の暗号化キー（GMEK）を使用することが最善の選択です。GMEKはGoogle Cloudのデフォルトの暗号化メカニズムであり、追加の設定やメンテナンスを必要とせずにデータが自動的に保存時に暗号化されることを保証します。手動でのキー管理の必要性をなくしつつ、強力なセキュリティを提供します。

-----

### <a name="no95"></a>**NO.95**

You are building a batch data pipeline to process 100 GB of structured data from multiple sources for daily reporting. You need to transform and standardize the data prior to loading the data to ensure that it is stored in a single dataset. You want to use a low-code solution that can be easily built and managed. What should you do?

あなたは、日次レポート作成のために複数のソースからの100GBの構造化データを処理するバッチデータパイプラインを構築しています。データをロードする前に変換および標準化して、単一のデータセットに保存されるようにする必要があります。簡単に構築・管理できるローコードソリューションを使用したいと考えています。どうすべきですか？

A. Use Cloud Data Fusion to ingest data and load the data into BigQuery. Use Looker Studio to perform data cleaning and transformation.
B. Use Cloud Data Fusion to ingest the data, perform data cleaning and transformation, and load the data into BigQuery.
C. Use Cloud Data Fusion to ingest the data, perform data cleaning and transformation, and load the data into Cloud SQL for PostgreSQL.
D. Use Cloud Storage to store the data. Use Cloud Run functions to perform data cleaning and transformation, and load the data into BigQuery.

**正解: B**

**解説:**
Comprehensive and Detailed in Depth Explanation:
Why B is correct:Cloud Data Fusion is a fully managed, cloud-native data integration service for building and managing ETL/ELT data pipelines. It provides a graphical interface for building pipelines without coding, making it a low-code solution. Cloud data fusion is perfect for the ingestion, transformation and loading of data into BigQuery.
Why other options are incorrect:A: Looker studio is for visualization, not data transformation.
C: Cloud SQL is a relational database, not ideal for large-scale analytical data.
D: Cloud run is for stateless applications, not batch data processing.

包括的で詳細な解説：
Bが正しい理由：Cloud Data Fusionは、ETL/ELTデータパイプラインを構築・管理するための、フルマネージドでクラウドネイティブなデータ統合サービスです。コーディングなしでパイプラインを構築するためのグラフィカルインターフェースを提供するため、ローコードソリューションとなります。Cloud Data Fusionは、データの取り込み、変換、BigQueryへのロードに最適です。
他の選択肢が不適切な理由：A：Looker Studioは視覚化用であり、データ変換用ではありません。
C：Cloud SQLはリレーショナルデータベースであり、大規模な分析データには理想的ではありません。
D：Cloud Runはステートレスアプリケーション用であり、バッチデータ処理用ではありません。

-----

### <a name="no96"></a>**NO.96**

You work for a global financial services company that trades stocks 24/7. You have a Cloud SGL for PostgreSQL user database. You need to identify a solution that ensures that the database is continuously operational, minimizes downtime, and will not lose any data in the event of a zonal outage. What should you do?

あなたは24時間365日株式取引を行うグローバルな金融サービス会社で働いています。Cloud SQL for PostgreSQLのユーザーデータベースを持っています。データベースが継続的に稼働し、ダウンタイムを最小限に抑え、ゾーン障害が発生した場合でもデータが失われないことを保証するソリューションを特定する必要があります。どうすべきですか？

A. Continuously back up the Cloud SGL instance to Cloud Storage. Create a Compute Engine instance with PostgreSCL in a different region. Restore the backup in the Compute Engine instance if a failure occurs.
B. Create a read replica in another region. Promote the replica to primary if a failure occurs.
C. Configure and create a high-availability Cloud SQL instance with the primary instance in zone A and a secondary instance in any zone other than zone A.
D. Create a read replica in the same region but in a different zone.

**正解: C**

**解説:**
Configuring a high-availability (HA) Cloud SQL instance ensures continuous operation, minimizes downtime, and prevents data loss in the event of a zonal outage. In this setup, the primary instance is located in one zone (e.g., zone A), and a synchronous secondary instance is located in a different zone within the same region. This configuration ensures that all data is replicated to the secondary instance in real-time. In the event of a failure in the primary zone, the system automatically promotes the secondary instance to primary, ensuring seamless failover with no data loss and minimal downtime. This is the recommended approach for mission-critical, highly available databases.

高可用性（HA）Cloud SQLインスタンスを構成することで、継続的な運用を保証し、ダウンタイムを最小限に抑え、ゾーン障害が発生した場合のデータ損失を防ぎます。この設定では、プライマリインスタンスが1つのゾーン（例：ゾーンA）にあり、同期セカンダリインスタンスが同じリージョン内の別のゾーンにあります。この構成により、すべてのデータがリアルタイムでセカンダリインスタンスに複製されることが保証されます。プライマリゾーンで障害が発生した場合、システムは自動的にセカンダリインスタンスをプライマリに昇格させ、データ損失なしで最小限のダウンタイムでシームレスなフェイルオーバーを保証します。これは、ミッションクリティカルで高可用性が求められるデータベースに推奨されるアプローチです。

-----

### <a name="no97"></a>**NO.97**

Your company wants to implement a data transformation (ETL) pipeline for their BigQuery data warehouse. You need to identify a managed transformation solution that allows users to develop with SQL and JavaScript, has version control, allows for modular code, and has data quality checks. What should you do?

あなたの会社は、BigQueryデータウェアハウスのためのデータ変換（ETL）パイプラインを実装したいと考えています。ユーザーがSQLとJavaScriptで開発でき、バージョン管理、モジュール化されたコード、データ品質チェック機能を備えた、マネージド変換ソリューションを特定する必要があります。どうすべきですか？

A. Create a Cloud Composer environment, and orchestrate the transformations by using the BigQueryinsertJob operator.
B. Create BigQuery scheduled queries to define the transformations in SQL.
C. Use Dataform to define the transformations in SQLX.
D. Use Dataproc to create an Apache Spark cluster and implement the transformations by using PySpark SQL.

**正解: C**

**解説:**
Comprehensive and Detailed in Depth Explanation:
Why C is correct: Dataform is a managed data transformation service that allows you to define data pipelines using SQL and JavaScript. It provides version control, modular code development, and data quality checks.
Why other options are incorrect:A: Cloud Composer is an orchestration tool, not a data transformation tool.
B: Scheduled queries are not suitable for complex ETL pipelines.
D: Dataproc requires setting up a Spark cluster and writing code, which is more complex than using Dataform.

包括的で詳細な解説：
Cが正しい理由：Dataformは、SQLとJavaScriptを使用してデータパイプラインを定義できるマネージドデータ変換サービスです。バージョン管理、モジュール化されたコード開発、データ品質チェックを提供します。
他の選択肢が不適切な理由：A：Cloud Composerはオーケストレーションツールであり、データ変換ツールではありません。
B：スケジュールされたクエリは、複雑なETLパイプラインには適していません。
C：DataprocはSparkクラスタのセットアップとコード記述が必要で、Dataformを使用するより複雑です。

-----

### <a name="no98"></a>**NO.98**

You have a Cloud SQL for PostgreSQL database that stores sensitive historical financial data. You need to ensure that the data is uncorrupted and recoverable in the event that the primary region is destroyed. The data is valuable, so you need to prioritize recovery point objective (RPO) over recovery time objective (RTO). You want to recommend a solution that minimizes latency for primary read and write operations. What should you do?

あなたは、機密性の高い過去の財務データを保存するCloud SQL for PostgreSQLデータベースを持っています。プライマリリージョンが破壊された場合に、データが破損せず、回復可能であることを保証する必要があります。データは貴重であるため、回復時間目標（RTO）よりも回復ポイント目標（RPO）を優先する必要があります。プライマリの読み取りおよび書き込み操作の遅延を最小限に抑えるソリューションを推奨したいと考えています。どうすべきですか？

A. Configure the Cloud SQL for PostgreSQL instance for regional availability (HA) with asynchronous replication to a secondary instance in a different region.
B. Configure the Cloud SQL for PostgreSQL instance for multi-region backup locations.
C. Configure the Cloud SQL for PostgreSQL instance for regional availability (HA). Back up the Cloud SQL for PostgreSQL database hourly to a Cloud Storage bucket in a different region.
D. Configure the Cloud SQL for PostgreSQL instance for regional availability (HA) with synchronous replication to a secondary instance in a different zone.

**正解: D**

**解説:**
Comprehensive and Detailed in Depth Explanation:
Why D is correct:Synchronous replication ensures that data is written to both the primary and secondary instances at the same time, minimizing data loss (RPO). Regional availability (HA) within different zones provides redundancy within the same region, minimizing latency.
Why other options are incorrect:A: Asynchronous replication has a potential for data loss.
B: Multiregion backups are for disaster recovery, not minimizing latency.
C: Hourly backups do not provide the lowest possible RPO.

包括的で詳細な解説：
Dが正しい理由：同期レプリケーションは、データがプライマリインスタンスとセカンダリインスタンスの両方に同時に書き込まれることを保証し、データ損失（RPO）を最小限に抑えます。異なるゾーン内でのリージョン可用性（HA）は、同じリージョン内での冗長性を提供し、遅延を最小限に抑えます。
他の選択肢が不適切な理由：A：非同期レプリケーションにはデータ損失の可能性があります。
B：マルチリージョンバックアップは災害復旧用であり、遅延を最小限に抑えるためではありません。
C：時間ごとのバックアップは、可能な限り低いRPOを提供しません。

-----

### <a name="no99"></a>**NO.99**

Your organization has decided to migrate their existing enterprise data warehouse to BigQuery. The existing data pipeline tools already support connectors to BigQuery. You need to identify a data migration approach that optimizes migration speed. What should you do?

あなたの組織は、既存のエンタープライズデータウェアハウスをBigQueryに移行することを決定しました。既存のデータパイプラインツールは、すでにBigQueryへのコネクタをサポートしています。移行速度を最適化するデータ移行アプローチを特定する必要があります。どうすべきですか？

A. Create a temporary file system to facilitate data transfer from the existing environment to Cloud Storage. Use Storage Transfer Service to migrate the data into BigQuery.
B. Use the Cloud Data Fusion web interface to build data pipelines. Create a directed acyclic graph (DAG) that facilitates pipeline orchestration.
C. Use the BigQuery Data Transfer Service to recreate the data pipeline and migrate the data into BigQuery.
D. Use the existing data pipeline tool's BigQuery connector to reconfigure the data mapping.

**正解: D**

**解説:**
Since your existing data pipeline tools already support connectors to BigQuery, the most efficient approach is touse the existing data pipeline tool's BigQuery connectorto reconfigure the data mapping. This leverages your current tools, reducing migration complexity and setup time, while optimizing migration speed. By reconfiguring the data mapping within the existing pipeline, you can seamlessly direct the data into BigQuery without needing additional services or intermediary steps.

既存のデータパイプラインツールはすでにBigQueryへのコネクタをサポートしているため、最も効率的なアプローチは、既存のデータパイプラインツールのBigQueryコネクタを使用してデータマッピングを再構成することです。これにより、現在のツールを活用し、移行の複雑さと設定時間を短縮し、移行速度を最適化します。既存のパイプライン内でデータマッピングを再構成することにより、追加のサービスや中間ステップを必要とせずに、データをシームレスにBigQueryに送ることができます。

-----

### <a name="no100"></a>**NO.100**

Your company uses Looker as its primary business intelligence platform. You want to use LookML to visualize the profit margin for each of your company's products in your Looker Explores and dashboards. You need to implement a solution quickly and efficiently. What should you do?

あなたの会社は、主要なビジネスインテリジェンスプラットフォームとしてLookerを使用しています。LookerのExploreとダッシュボードで、会社の各製品の利益率を視覚化するためにLookMLを使用したいと考えています。迅速かつ効率的にソリューションを実装する必要があります。どうすべきですか？

A. Apply a filter to only show products with a positive profit margin.
B. Define a new measure that calculates the profit margin by using the existing revenue and cost fields.
C. Create a new dimension that categorizes products based on their profit margin ranges (e.g., high, medium, low).
D. Create a derived table that pre-calculates the he profit margin for each product, and include it in the Looker model.

**正解: B**

**解説:**
Comprehensive and Detailed in Depth Explanation:
Why B is correct:Defining a new measure in LookML is the most efficient and direct way to calculate and visualize aggregated metrics like profit margin. Measures are designed for calculations based on existing fields.
Why other options are incorrect:A: Filtering doesn't calculate or visualize the profit margin itself.
C: Dimensions are for categorizing data, not calculating aggregated metrics.
D: Derived tables are more complex and unnecessary for a simple calculation like profit margin, which can be done using a measure.

包括的で詳細な解説：
Bが正しい理由：LookMLで新しいメジャーを定義することが、利益率のような集計メトリクスを計算し視覚化するための最も効率的で直接的な方法です。メジャーは既存のフィールドに基づく計算のために設計されています。
他の選択肢が不適切な理由：A：フィルタリングは利益率自体を計算したり視覚化したりしません。
C：ディメンションはデータを分類するためのものであり、集計メトリクスを計算するためではありません。
D：派生テーブルはより複雑で、メジャーを使用して行える利益率のような単純な計算には不要です。

-----

### <a name="no101"></a>**NO.101**

Your company's ecommerce website collects product reviews from customers. The reviews are loaded as CSV files daily to a Cloud Storage bucket. The reviews are in multiple languages and need to be translated to Spanish. You need to configure a pipeline that is serverless, efficient, and requires minimal maintenance. What should you do?

あなたの会社のeコマースウェブサイトは、顧客から製品レビューを収集します。レビューは毎日CSVファイルとしてCloud Storageバケットにロードされます。レビューは多言語であり、スペイン語に翻訳する必要があります。サーバーレスで効率的、かつ最小限のメンテナンスで済むパイプラインを構成する必要があります。どうすべきですか？

A. Load the data into BigQuery using Dataproc. Use Apache Spark to translate the reviews by invoking the Cloud Translation API. Set BigQuery as the sink.
B. Use a Dataflow templates pipeline to translate the reviews using the Cloud Translation API. Set BigQuery as the sink.
C. Load the data into BigQuery using a Cloud Run function. Use the BigQuery ML create model statement to train a translation model. Use the model to translate the product reviews within BigQuery.
D. Load the data into BigQuery using a Cloud Run function. Create a BigQuery remote function that invokes the Cloud Translation API. Use a scheduled query to translate new reviews.

**正解: D**

**解説:**
Loading the data into BigQuery using aCloud Run functionand creating aBigQuery remote functionthat invokes theCloud Translation APlis a serverless and efficient approach. With this setup, you can use ascheduled queryin BigQuery to invoke the remote function and translate new product reviews on a regular basis. This solution requires minimal maintenance, as BigQuery handles storage and querying, and the Cloud Translation API provides accurate translations without the need for custom ML model development.

Cloud Run関数を使用してデータをBigQueryにロードし、Cloud Translation APIを呼び出すBigQueryリモート関数を作成することは、サーバーレスで効率的なアプローチです。この設定により、BigQueryでスケジュールされたクエリを使用してリモート関数を呼び出し、新しい製品レビューを定期的に翻訳できます。BigQueryがストレージとクエリを処理し、Cloud Translation APIがカスタムMLモデル開発の必要なく正確な翻訳を提供するため、このソリューションは最小限のメンテナンスで済みます。

-----

### <a name="no102"></a>**NO.102**

Your company has several retail locations. Your company tracks the total number of sales made at each location each day. You want to use SQL to calculate the weekly moving average of sales by location to identify trends for each store. Which query should you use?

あなたの会社にはいくつかの小売店があります。会社は、各店舗で毎日行われる総売上数を追跡しています。SQLを使用して、店舗ごとのトレンドを特定するために、場所別の売上の週次移動平均を計算したいと考えています。どのクエリを使用すべきですか？

A. SELECT store_id, date, total sales, AVG(total_sales) OVER ( PARTITION BY store_id ORDER BY total_sales RANGE BETWEEN 6 PRECEDING AND CURRENT ROW) as rolling_avg FROM store_sales_daily
B. SELECT store id, date, total sales, AVG(total sales) OVER ( PARTITION BY date ORDER BY store_id ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as rolling_avg FROM store sales_daily
C. SELECT store_id, date, total_sales, AVG(total_sales) OVER ( PARTITION BY store_id ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as rolling_avg FROM store sales daily
D. SELECT store id, date, total sales, AVG(total sales) OVER ( PARTITION BY total sales ORDER BY date RANGE BETWEEN 6 PRECEDING AND CURRENT ROW) as rolling_avg FROM store_sales_daily

**正解: C**

**解説:**
To calculate the weekly moving average of sales by location:

  * The query must group bystore_id(partitioning the calculation by each store).
  * TheORDER BY dateensures the sales are evaluated chronologically.
  * TheROWS BETWEEN 6 PRECEDING AND CURRENT ROWspecifies a rolling window of 7 rows (1 week if each row represents daily data).
  * TheAVG(total_sales)computes the average sales over the defined rolling window.
    Chosen querymeets these requirements:
    PARTITION BY store_idgroups the calculation by each store.

場所別の売上の週次移動平均を計算するには：

  * クエリはstore_idでグループ化する必要があります（各店舗ごとに計算をパーティション化）。
  * ORDER BY dateは、売上が時系列で評価されることを保証します。
  * ROWS BETWEEN 6 PRECEDING AND CURRENT ROWは、7行（各行が日次データを表す場合は1週間）のローリングウィンドウを指定します。
  * AVG(total_sales)は、定義されたローリングウィンドウ内の平均売上を計算します。
    選択されたクエリはこれらの要件を満たしています：
    PARTITION BY store_idは、各店舗ごとに計算をグループ化します。

-----

### <a name="no103"></a>**NO.103**

Your organization has decided to move their on-premises Apache Spark-based workload to Google Cloud. You want to be able to manage the code without needing to provision and manage your own cluster. What should you do?

あなたの組織は、オンプレミスのApache SparkベースのワークロードをGoogle Cloudに移行することを決定しました。独自のクラスタをプロビジョニングおよび管理する必要なく、コードを管理できるようにしたいと考えています。どうすべきですか？

A. Migrate the Spark jobs to Dataproc Serverless.
B. Configure a Google Kubernetes Engine cluster with Spark operators, and deploy the Spark jobs.
C. Migrate the Spark jobs to Dataproc on Google Kubernetes Engine.
D. Migrate the Spark jobs to Dataproc on Compute Engine.

**正解: A**

**解説:**
Migrating the Spark jobs toDataproc Serverlessis the best approach because it allows you to run Spark workloads without the need to provision or manage clusters. Dataproc Serverless automatically scales resources based on workload requirements, simplifying operations and reducing administrative overhead. This solution is ideal for organizations that want to focus on managing their Spark code without worrying about the underlying infrastructure. It is cost-effective and fully managed, aligning well with the goal of minimizing cluster management.

SparkジョブをDataproc Serverlessに移行することが最善のアプローチです。なぜなら、クラスタをプロビジョニングまたは管理する必要なくSparkワークロードを実行できるからです。Dataproc Serverlessはワークロードの要件に基づいてリソースを自動的にスケーリングし、運用を簡素化し、管理オーバーヘッドを削減します。このソリューションは、基盤となるインフラストラクチャを気にすることなくSparkコードの管理に集中したい組織に最適です。費用対効果が高く、完全に管理されており、クラスタ管理を最小限に抑えるという目標によく合致しています。

-----

### <a name="no104"></a>**NO.104**

You are predicting customer churn for a subscription-based service. You have a 50 PB historical customer dataset in BigQuery that includes demographics, subscription information, and engagement metrics. You want to build a churn prediction model with minimal overhead. You want to follow the Google-recommended approach. What should you do?

あなたは、サブスクリプションベースのサービスの顧客離反を予測しています。BigQueryには、人口統計、サブスクリプション情報、エンゲージメント指標を含む50PBの過去の顧客データセットがあります。最小限のオーバーヘッドで離反予測モデルを構築したいと考えています。Googleが推奨するアプローチに従いたいと考えています。どうすべきですか？

A. Export the data from BigQuery to a local machine. Use scikit-learn in a Jupyter notebook to build the churn prediction model.
B. Use Dataproc to create a Spark cluster. Use the Spark MLlib within the cluster to build the churn prediction model.
C. Create a Looker dashboard that is connected to BigQuery. Use LookML to predict churn.
D. Use the BigQuery Python client library in a Jupyter notebook to query and preprocess the data in BigQuery. Use the CREATE MODEL statement in BigQueryML to train the churn prediction model.

**正解: D**

**解説:**
Using theBigQuery Python client libraryto query and preprocess data directly in BigQuery and then leveraging BigQueryMLto train the churn prediction model is the Google-recommended approach for this scenario. BigQueryML allows you to build machine learning models directly within BigQuery using SQL, eliminating the need to export data or manage additional infrastructure. This minimizes overhead, scales effectively for a dataset as large as 50 PB, and simplifies the end-to-end process of building and training the churn prediction model.

このシナリオでは、BigQuery Pythonクライアントライブラリを使用してBigQueryで直接データをクエリ・前処理し、その後BigQueryMLを活用して離反予測モデルをトレーニングすることが、Googleが推奨するアプローチです。BigQueryMLを使用すると、SQLを使用してBigQuery内で直接機械学習モデルを構築でき、データをエクスポートしたり追加のインフラを管理したりする必要がなくなります。これにより、オーバーヘッドが最小限に抑えられ、50PBもの大規模なデータセットに対して効果的にスケーリングし、離反予測モデルの構築とトレーニングのエンドツーエンドのプロセスが簡素化されます。

-----

### <a name="no105"></a>**NO.105**

You are a database administrator managing sales transaction data by region stored in a BigQuery table. You need to ensure that each sales representative can only see the transactions in their region. What should you do?

あなたは、BigQueryテーブルに保存されている地域別の売上トランザクションデータを管理するデータベース管理者です。各営業担当者が自分の地域のトランザクションのみを閲覧できるようにする必要があります。どうすべきですか？

A. Add a policy tag in BigQuery.
B. Create a row-level access policy.
C. Create a data masking rule.
D. Grant the appropriate 1AM permissions on the dataset.

**正解: B**

**解説:**
Creating arow-level access policyin BigQuery ensures that each sales representative can see only the transactions relevant to their region. Row-level access policies allow you to define fine-grained access control by filtering rows based on specific conditions, such as matching the sales representative's region. This approach enforces security while providing tailored data access, aligning with the principle of least privilege.

BigQueryで行レベルのアクセスポリシーを作成すると、各営業担当者が自分の地域に関連するトランザクションのみを閲覧できるようになります。行レベルのアクセスポリシーを使用すると、営業担当者の地域と一致させるなどの特定の条件に基づいて行をフィルタリングすることで、詳細なアクセス制御を定義できます。このアプローチは、セキュリティを強化し、カスタマイズされたデータアクセスを提供し、最小権限の原則に沿っています。

-----

### <a name="no106"></a>**NO.106**

You work for a healthcare company. You have a daily ETL pipeline that extracts patient data from a legacy system, transforms it, and loads it into BigQuery for analysis. The pipeline currently runs manually using a shell script. You want to automate this process and add monitoring to ensure pipeline observability and troubleshooting insights. You want one centralized solution, using open- source tooling, without rewriting the ETL code. What should you do?

あなたはヘルスケア企業で働いています。レガシーシステムから患者データを抽出し、変換し、分析のためにBigQueryにロードする日次のETLパイプラインがあります。パイプラインは現在、シェルスクリプトを使用して手動で実行されています。このプロセスを自動化し、パイプラインの可観測性とトラブルシューティングの洞察を確保するために監視を追加したいと考えています。ETLコードを書き直さずに、オープンソースのツールを使用した一元化されたソリューションを求めています。どうすべきですか？

A. Create a direct acyclic graph (DAG) in Cloud Composer to orchestrate a pipeline trigger daily. Monitor the pipeline's execution using the Apache Airflow web interface and Cloud Monitoring.
B. Configure Cloud Dataflow to implement the ETL pipeline, and use Cloud Scheduler to trigger the Dataflow pipeline daily. Monitor the pipelines execution using the Dataflow job monitoring interface and Cloud Monitoring.
C. Use Cloud Scheduler to trigger a Dataproc job to execute the pipeline daily. Monitor the job's progress using the Dataproc job web interface and Cloud Monitoring.
D. Create a Cloud Run function that runs the pipeline daily. Monitor the functions execution using Cloud Monitoring.

**正解: A**

**解説:**
Comprehensive and Detailed in Depth Explanation:
Why A is correct: Cloud Composer is a managed Apache Airflow service, which is a popular open- source workflow orchestration tool. DAGs in Airflow can be used to automate ETL pipelines. Airflow's web interface and Cloud Monitoring provide comprehensive monitoring capabilities. It also allows you to run existing shell scripts.
Why other options are incorrect:B: Dataflow requires rewriting the ETL pipeline using its SDK.
C: Dataproc is for big data processing, not orchestration.
D: Cloud Run functions are for stateless applications, not long-running ETL pipelines.

包括的で詳細な解説：
Aが正しい理由：Cloud Composerは、人気のオープンソースワークフローオーケストレーションツールであるApache Airflowのマネージドサービスです。AirflowのDAGはETLパイプラインの自動化に使用できます。AirflowのWebインターフェースとCloud Monitoringは、包括的な監視機能を提供します。また、既存のシェルスクリプトを実行することもできます。
他の選択肢が不適切な理由：B：Dataflowは、そのSDKを使用してETLパイプラインを書き直す必要があります。
C：Dataprocはビッグデータ処理用であり、オーケストレーション用ではありません。
D：Cloud Run関数はステートレスアプリケーション用であり、長時間のETLパイプライン用ではありません。

-----

### <a name="no107"></a>**NO.107**

Your organization plans to move their on-premises environment to Google Cloud. Your organization's network bandwidth is less than 1 Gbps. You need to move over 500 ## of data to Cloud Storage securely, and only have a few days to move the data. What should you do?

あなたの組織は、オンプレミス環境をGoogle Cloudに移行することを計画しています。組織のネットワーク帯域幅は1Gbps未満です。500TB以上のデータを安全にCloud Storageに移動する必要があり、データの移動には数日しかありません。どうすべきですか？

A. Request multiple Transfer Appliances, copy the data to the appliances, and ship the appliances back to Google Cloud to upload the data to Cloud Storage.
B. Connect to Google Cloud using VPN. Use Storage Transfer Service to move the data to Cloud Storage.
C. Connect to Google Cloud using VPN. Use the gcloud storage command to move the data to Cloud Storage.
D. Connect to Google Cloud using Dedicated Interconnect. Use the gcloud storage command to move the data to Cloud Storage.

**正解: A**

**解説:**
Using Transfer Appliancesis the best solution for securely and efficiently moving over 500 TB of data to Cloud Storage within a limited timeframe, especially with network bandwidth below 1 Gbps. Transfer Appliances are physical devices provided by Google Cloud to securely transfer large amounts of data. After copying the data to the appliances, they are shipped back to Google, where the data is uploaded to Cloud Storage. This approach bypasses bandwidth limitations and ensures the data is migrated quickly and securely.

特にネットワーク帯域幅が1Gbps未満の場合、限られた時間枠内で500TB以上のデータを安全かつ効率的にCloud Storageに移動するには、Transfer Applianceを使用することが最善の解決策です。Transfer Applianceは、大量のデータを安全に転送するためにGoogle Cloudが提供する物理デバイスです。アプライアンスにデータをコピーした後、それらはGoogleに返送され、そこでデータがCloud Storageにアップロードされます。このアプローチは帯域幅の制限を回避し、データが迅速かつ安全に移行されることを保証します。

-----

### <a name="no108"></a>**NO.108**

Your company is adopting BigQuery as their data warehouse platform. Your team has experienced Python developers. You need to recommend a fully-managed tool to build batch ETL processes that extract data from various source systems, transform the data using a variety of Google Cloud services, and load the transformed data into BigQuery. You want this tool to leverage your team's Python skills. What should you do?

あなたの会社は、データウェアハウスプラットフォームとしてBigQueryを採用しています。あなたのチームには経験豊富なPython開発者がいます。様々なソースシステムからデータを抽出し、さまざまなGoogle Cloudサービスを使用してデータを変換し、変換されたデータをBigQueryにロードするバッチETLプロセスを構築するための、フルマネージドツールを推奨する必要があります。このツールがチームのPythonスキルを活用できるようにしたいと考えています。どうすべきですか？

A. Use Dataform with assertions.
B. Deploy Cloud Data Fusion and included plugins.
C. Use Cloud Composer with pre-built operators.
D. Use Dataflow and pre-built templates.

**正解: C**

**解説:**
Comprehensive and Detailed In-Depth Explanation:
The tool must be fully managed, support batch ETL, integrate with multiple Google Cloud services, and leverage Python skills.

  * Option A: Dataform is SQL-focused for ELT within BigQuery, not Python-centric, and lacks broad service integration for extraction.
  * Option B: Cloud Data Fusion is a visual ETL tool, not Python-focused, and requires more Ul-based configuration than coding.
  * Option C: Cloud Composer (managed Apache Airflow) is fully managed, supports batch ETL via DAGs, integrates with various Google Cloud services (e.g., BigQuery, GCS) through operators, and allows custom Python code in tasks. It's ideal for Python developers per the "Cloud Composer" documentation.

包括的で詳細な解説：
ツールはフルマネージドであり、バッチETLをサポートし、複数のGoogle Cloudサービスと統合し、Pythonスキルを活用する必要があります。

  * 選択肢A：DataformはBigQuery内でのELTにSQL中心であり、Python中心ではなく、抽出のための広範なサービス統合がありません。
  * 選択肢B：Cloud Data Fusionは視覚的なETLツールであり、Python中心ではなく、コーディングよりもUIベースの構成が多く必要です。
  * 選択肢C：Cloud Composer（マネージドApache Airflow）はフルマネージドで、DAGを介してバッチETLをサポートし、オペレーターを通じてさまざまなGoogle Cloudサービス（例：BigQuery、GCS）と統合し、タスク内でカスタムPythonコードを許可します。「Cloud Composer」のドキュメントによれば、Python開発者にとって理想的です。

-----