### <a name="no1"></a>**NO.1**

この問題については、TerramEarthのケーススタディを参照してください。

TerramEarthの各車両では、環境条件に応じて効率を高めるために、油圧などの運用パラメータが調整可能です。あなたの主な目標は、現場にある2,000万台のセルラー接続および非接続車両すべての運用効率を向上させることです。この目標をどのように達成できますか？

A. エンジニアにデータのパターンを調査させ、運用調整を自動的に行うルールを持つアルゴリズムを作成させる。
B. すべての運用データをキャプチャし、理想的な運用を特定する機械学習モデルをトレーニングし、ローカルで実行して運用調整を自動的に行う。
C. Google Cloud Dataflowのストリーミングジョブをスライディングウィンドウで使用し、Google Cloud Messaging（GCM）を使用して運用調整を自動的に行う。
D. すべての運用データをキャプチャし、理想的な運用を特定する機械学習モデルをトレーニングし、Google Cloud Machine Learning（ML）Platformでホストして運用調整を自動的に行う。

**正解: B**

**解説:**
ケーススタディには、2000万台の車両のうち大部分が**ネットワークに接続されていない**と明記されています。したがって、クラウド上のサービス（D）からリアルタイムで調整を行うことは不可能です。解決策は、クラウドの強力な計算能力を利用して機械学習モデルを**トレーニング**し、その完成したモデルを各車両に配布（メンテナンス時など）して**ローカルで実行**させることです。これにより、ネットワーク接続がない車両でも自律的に運用調整を行えるようになります。

-----

### <a name="no2"></a>**NO.2**

この問題については、TerramEarthのケーススタディを参照してください。

TerramEarthは、テレメトリデータを収集するために、非接続のトラックにサーバーとセンサーを装備しました。来年、彼らはそのデータを使って機械学習モデルをトレーニングしたいと考えています。コストを削減しつつ、このデータをクラウドに保存したいと考えています。どうすべきですか？

A. 車両のコンピュータに1時間ごとのスナップショットでデータを圧縮させ、Google Cloud Storage（GCS）のNearlineバケットに保存する。
B. テレメトリデータをリアルタイムでストリーミングDataflowジョブにプッシュし、データを圧縮してGoogle BigQueryに保存する。
C. テレメトリデータをリアルタイムでストリーミングDataflowジョブにプッシュし、データを圧縮してCloud Bigtableに保存する。
D. 車両のコンピュータに1時間ごとのスナップショットでデータを圧縮させ、GCS Coldlineバケットに保存する。

**正解: D**

**解説:**
問題の要件は、**来年**（つまり、すぐにはアクセスしない）の機械学習モデルのトレーニング用にデータを**最も低コスト**で保存することです。Cloud Storageのストレージクラスの中で、**Coldline**は年に1回以下のアクセス頻度のデータ向けに設計されており、ストレージ料金が最も安価です。Nearline（A）よりもさらに低コストであり、この要件に最適です。

-----

### <a name="no3"></a>**NO.3**

この問題については、TerramEarthのケーススタディを参照してください。

TerramEarthは、現場にある2,000万台すべての車両をクラウドに接続する計画です。これにより、データ量は1秒あたり2,000万件の600バイトレコード、1時間あたり40TBに増加します。データ取り込みはどのように設計すべきですか？

A. 車両はデータを直接GCSに書き込む。
B. 車両はデータを直接Google Cloud Pub/Subに書き込む。
C. 車両はデータを直接Google BigQueryにストリーミングする。
D. 車両は既存のシステム（FTP）を使用してデータを書き込み続ける。

**正解: B**

**解説:**
**Cloud Pub/Sub**は、大量のストリーミングデータ（テレメトリデータなど）を大規模かつ確実に、低レイテンシで取り込むために設計されたフルマネージドサービスです。1秒あたり数百万のメッセージを処理でき、後続の処理（Dataflowなど）へのバッファとして機能し、システム全体の信頼性とスケーラビリティを高めます。このような大規模なIoTデータ取り込みシナリオのデファクトスタンダードです。

-----

### <a name="no4"></a>**NO.4**

この問題については、TerramEarthのケーススタディを参照してください。

あなたはTerramEarthのダウンタイム削減というビジネス要件を分析し、部品の待ち時間を短縮することで時間節約の大部分を達成できることを見出しました。あなたは3週間の集計レポート時間を短縮することに焦点を当てることにしました。会社のプロセスにどのような変更を推奨すべきですか？

A. CSVからバイナリ形式への移行、FTPからSFTPトランスポートへの移行、およびメトリクスの機械学習分析の開発。
B. FTPからストリーミングトランスポートへの移行、CSVからバイナリ形式への移行、およびメトリクスの機械学習分析の開発。
C. フリートのセルラー接続を80%に増加、FTPからストリーミングトランスポートへの移行、およびメトリクスの機械学習分析の開発。
D. FTPからSFTPトランスポートへの移行、メトリクスの機械学習分析の開発、およびディーラーの現地在庫を固定係数で増加させる。

**正解: C**

**解説:**
レポートが3週間遅れる根本原因は、データがメンテナンス時にFTPで**バッチアップロード**されることです。この遅延を解決するには、リアルタイムに近いデータ収集が不可欠です。そのためには、まず**セルラー接続を持つ車両を増やす**ことが前提となり、その上でデータ転送方法をバッチ式のFTPから**ストリーミングトランスポート**（例: Pub/Sub）に移行することが最も効果的です。これにより、ほぼリアルタイムでデータ分析（機械学習など）が可能になり、レポートの遅延が劇的に改善されます。

-----

### <a name="no5"></a>**NO.5**

この問題については、TerramEarthのケーススタディを参照してください。

TerramEarthの開発チームは、会社のビジネス要件を満たすためのAPIを作成したいと考えています。あなたは開発チームに、カスタムフレームワークの作成ではなく、ビジネス価値に開発努力を集中させたいと考えています。どの方法を使用すべきですか？

A. Google App EngineとGoogle Cloud Endpointsを使用する。ディーラーとパートナー向けのAPIに焦点を当てる。
B. Google App EngineとJAX-RS Jersey Javaベースのフレームワークを使用する。一般向けのAPIに焦点を当てる。
C. Google App EngineとSwagger（Open API Specification）フレームワークを使用する。一般向けのAPIに焦点を当てる。
D. Google Container EngineとDjango Pythonコンテナを使用する。一般向けのAPIに焦点を当てる。
E. Google Container EngineとSwagger（Open API Specification）フレームワークを備えたTomcatコンテナを使用する。ディーラーとパートナー向けのAPIに焦点を当てる。

**正解: A**

**解説:**
「ビジネス価値に開発努力を集中させる」という要件は、インフラ管理のオーバーヘッドが少ない**サーバーレス**や**マネージドサービス**の利用を示唆しています。**App Engine**はサーバーレスプラットフォームであり、インフラ管理をGoogleに任せられます。**Cloud Endpoints**はAPIのデプロイ、保護、監視を容易にするマネージドなAPIフレームワークです。ケーススタディのビジネス要件には「ディーラーネットワークのサポート」と「他社とのパートナーシップ」が挙げられているため、APIの対象はディーラーとパートナーが適切です。

-----

### <a name="no6"></a>**NO.6**

あなたはKubernetes Engine上のクラスタでWebアプリケーションを運用しています。ユーザーから、アプリケーションの特定の部分が応答しなくなったと報告されています。デプロイメントのすべてのPodが2秒後に再起動を繰り返していることに気づきました。アプリケーションはログを標準出力に書き込みます。問題の原因を見つけるためにログを調査したいです。どのアプローチを取ることができますか？

A. gcloud credentialsを使用してクラスタに接続し、いずれかのPod内のコンテナに接続してログを読み取る。
B. 応答しないアプリケーション部分を提供している特定のKubernetes EngineコンテナのStackdriverログを確認する。
C. クラスタ内のノードとして機能している各Compute Engineインスタンスのシリアルポートログを確認する。
D. クラスタ内のノードとして機能している各Compute EngineインスタンスのStackdriverログを確認する。

**正解: B**

**解説:**
Podが短時間で再起動を繰り返す「CrashLoopBackOff」状態は、通常、コンテナ内のアプリケーションが起動直後にエラーで終了していることが原因です。Kubernetes Engineでは、コンテナが標準出力（stdout）や標準エラー出力（stderr）に書き込んだログは、**Cloud Logging (旧Stackdriver)** に自動的に集約されます。したがって、Cloud Loggingで該当コンテナのログを確認することが、エラーの原因を特定する最も直接的で標準的な方法です。

-----

### <a name="no7"></a>**NO.7**

この問題については、TerramEarthのケーススタディを参照してください。

TerramEarthのCTOは、接続された車両からの生データを使用して、車両が故障するおおよその時期を特定するのを支援したいと考えています。あなたはアナリストが車両データを中央でクエリできるようにしたいです。どのアーキテクチャを推奨すべきですか？

（注：この問題には4つのアーキテクチャ図が含まれています。以下に各選択肢の要点を記述します。）

A. IoT -> FTP -> Google Cloud Load Balancing -> Google Container Engine -> Cloud Pub/Sub -> Cloud Dataflow -> BigQuery -> Analysts
B. IoT -> FTP -> App Engine Flexible Environment -> Cloud Pub/Sub -> Cloud Dataflow -> BigQuery -> Analysts
C. IoT -> FTP -> Google Cloud Load Balancing -> Google Container Engine -> Cloud Pub/Sub -> Cloud Dataflow -> Cloud SQL -> Analysts
D. IoT -> FTP -> App Engine Flexible Environment -> Cloud Pub/Sub -> Cloud Dataflow -> Cloud SQL -> Analysts

**正解: A**

**解説:**
これは大規模なIoTデータ分析におけるGoogle Cloudの典型的なリファレンスアーキテクチャです。

  * **Load Balancing + Container Engine**: FTPサーバーのようなイングレス（入口）ポイントをスケーラブルかつ高可用にするための構成です。
  * **Cloud Pub/Sub**: 大量のストリーミングデータを確実に取り込むためのバッファとして機能します。
  * **Cloud Dataflow**: Pub/Subからのデータをストリーム処理（変換、集約など）します。
  * **BigQuery**: アナリストがペタバイト級の大規模データセットに対してアドホックなSQLクエリを実行するためのデータウェアハウスとして最適です。
    Cloud SQL (C, D) は、この規模のデータ分析（OLAP）には不向きです。

-----

### <a name="no8"></a>**NO.8**

あなたの会社は、外部ユーザーがファイルをアップロードして共有できるようにするためのモノリシックな3層アプリケーションを開発しました。このソリューションは簡単に拡張できず、信頼性に欠けています。開発チームは、マイクロサービスとフルマネージドサービスアプローチを採用するためにアプリケーションを再設計したいと考えていますが、その努力が価値あるものであることを経営陣に納得させる必要があります。経営陣にどの利点を強調すべきですか？

A. 新しいアプローチは、インフラストラクチャとアプリケーションの分離、新機能の開発とリリース、基盤となるインフラストラクチャの管理、CI/CDパイプラインの管理、A/Bテストの実施、および必要に応じたソリューションのスケーリングを容易にする。
B. モノリシックソリューションはDockerでコンテナに変換できる。生成されたコンテナはKubernetesクラスタにデプロイできる。
C. 新しいアプローチは大幅にコストが安くなり、基盤となるインフラストラクチャの管理が容易になり、CI/CDパイプラインを自動的に管理する。
D. プロセスはMigrate for Compute Engineで自動化できる。

**正解: A**

**解説:**
マイクロサービスアーキテクチャとマネージドサービスへの移行は、単なる技術的な流行ではなく、ビジネスの俊敏性と信頼性を向上させるための具体的な利点をもたらします。選択肢Aは、それらの利点を最も包括的に説明しています。各サービスを**独立して開発・デプロイ**できるため、新機能のリリースサイクルが速くなります。また、サービスごとに最適な技術を選択し、**個別にスケーリング**できるため、リソース効率と耐障害性が向上します。これらは経営陣にとって説得力のあるビジネス上のメリットです。

-----

### <a name="no9"></a>**NO.9**

この問題については、TerramEarthのケーススタディを参照してください。

Google Cloud Platformの採用が増加した結果、TerramEarthのどのレガシーエンタープライズプロセスが大幅な変更を経験するでしょうか？

A. Opex/capexの配分、LANの変更、キャパシティプランニング
B. キャパシティプランニング、TCO計算、opex/capexの配分
C. キャパシティプランニング、使用率測定、データセンターの拡張
D. データセンターの拡張、TCO計算、使用率測定

**正解: B**

**解説:**
クラウドへの移行は、ITインフラの財務と計画に関する考え方を根本から変えます。

  * **キャパシティプランニング**: オンプレミスでは物理サーバーの購入と設置を数ヶ月～数年単位で計画する必要がありましたが、クラウドでは需要に応じてリソースを分単位で動的に増減させるモデルに変わります。
  * **TCO計算（総所有コスト）**: ハードウェア、電力、冷却、物理的なセキュリティなどのコスト要因が、クラウドサービスの利用料という形で再計算されます。
  * **Opex/Capexの配分**: 物理サーバーへの先行設備投資（Capex）が、サービス利用料という継続的な運用費（Opex）に変わります。これは企業の財務モデルに大きな影響を与えます。

-----

### <a name="no10"></a>**NO.10**

この問題については、TerramEarthのケーススタディを参照してください。

データ検索を高速化するために、より多くの車両がセルラー接続にアップグレードされ、ETLプロセスにデータを送信できるようになります。現在のFTPプロセスはエラーが発生しやすく、接続が失敗するとファイルの最初からデータ転送を再開しますが、これは頻繁に発生します。ソリューションの信頼性を向上させ、セルラー接続でのデータ転送時間を最小限に抑えたいです。どうすべきですか？

A. FTPサーバーのGoogle Container Engineクラスタを1つ使用する。データをマルチリージョンバケットに保存する。バケット内のデータを使用してETLプロセスを実行する。
B. 異なるリージョンにあるFTPサーバーを実行する複数のGoogle Container Engineクラスタを使用する。データをus、eu、asiaのマルチリージョンバケットに保存する。バケット内のデータを使用してETLプロセスを実行する。
C. Google APIを介してHTTP(S)でファイルをus、eu、asiaの異なるGoogle Cloudマルチリージョンストレージバケットの場所に直接転送する。バケット内のデータを使用してETLプロセスを実行する。
D. Google APIを介してHTTP(S)でファイルをus、eu、asiaの異なるGoogle Cloudリージョンストレージバケットの場所に直接転送する。各リージョンバケットからデータを取得するためにETLプロセスを実行する。

**正解: D**

**解説:**
信頼性の低いFTPの問題を解決するため、Google Cloud StorageのAPIは**再開可能なアップロード**をサポートしており、接続が中断しても中断点から再開できます。これにより信頼性が大幅に向上します。また、車両は世界中に分散しているため、データ転送時間を最小化するには、物理的に最も近い**リージョン**のバケットにアップロードするのが最適です。アップロード後、各リージョンのデータをETLプロセスで処理します。これにより、高レイテンシでコストのかかるリージョン間データ転送を最小限に抑えられます。

-----

### <a name="no11"></a>**NO.11**

あなたの農業部門は、完全自律走行車を実験しています。車両の運用中に強力なセキュリティを促進するアーキテクチャを望んでいます。どの2つのアーキテクチャを検討すべきですか？ （2つ選択）

A. 車両上のモジュール間のすべてのマイクロサービスコールを信頼できないものとして扱う。
B. 安全なアドレス空間を確保するためにIPv6接続を要求する。
C. トラステッドプラットフォームモジュール（TPM）を使用し、起動時にファームウェアとバイナリを検証する。
D. コード実行サイクルを分離するために、関数型プログラミング言語を使用する。
E. 冗長性のために複数の接続サブシステムを使用する。
F. 車両の駆動電子機器をファラデーケージで囲み、チップを分離する。

**正解: A, C**

**解説:**

  * **A (ゼロトラスト)**: ネットワークの内部か外部かを問わず、すべての通信を信頼できないものとして扱い、都度認証・認可を行う「ゼロトラスト」の考え方です。自律走行車のように多数のモジュールが連携するシステムでは、一部が侵害されても被害を拡大させないためにこのアプローチが不可欠です。
  * **C (セキュアブート)**: TPMは、ハードウェアレベルで暗号化キーを安全に保管し、システムの起動プロセスを検証するための専用チップです。これにより、起動時に読み込まれるファームウェアやソフトウェアが改ざんされていないことを確認でき、根本的なレベルでのセキュリティ（Root of Trust）を確保します。

-----

### <a name="no12"></a>**NO.12**

この問題については、TerramEarthのケーススタディを参照してください。

TerramEarthの2,000万台の車両は世界中に散らばっています。車両の位置に基づいて、そのテレメトリデータはGoogle Cloud Storage（GCS）のリージョンバケット（US、ヨーロッパ、またはアジア）に保存されます。CTOから、車両が10万マイル後に故障する理由を判断するために、生のテレメトリデータに関するレポートを実行するように依頼されました。このジョブをすべてのデータで実行したいです。このジョブを実行する最も費用対効果の高い方法は何ですか？

A. すべてのデータを1つのゾーンに移動し、Cloud Dataprocクラスタを起動してジョブを実行する。
B. すべてのデータを1つのリージョンに移動し、Google Cloud Dataprocクラスタを起動してジョブを実行する。
C. 各リージョンでクラスタを起動して生データを前処理および圧縮し、データをマルチリージョンバケットに移動して、Dataprocクラスタを使用してジョブを完了する。
D. 各リージョンでクラスタを起動して生データを前処理および圧縮し、データをリージョンバケットに移動して、Cloud Dataprocクラスタを使用してジョブを完了する。

**正解: D**

**解説:**
Google Cloudではリージョンをまたぐネットワーク転送（下り）にコストがかかります。最も費用対効果の高いアプローチは、データが配置されている**各リージョン内で**Cloud Dataprocクラスタなどを起動し、そこでデータの初期処理（前処理、フィルタリング、圧縮など）を行うことです。これにより、ネットワーク転送が必要なデータ量を大幅に削減できます。その後、処理済みの小さなデータを1つのリージョンバケットに集約し、最終的な集計ジョブを実行することで、高額なリージョン間データ転送コストを最小限に抑えることができます。

-----

### <a name="no13"></a>**NO.13**

この問題については、TerramEarthのケーススタディを参照してください。

あなたの開発チームは、車両データを取得するための構造化APIを作成しました。彼らは、サードパーティがこの車両イベントデータを使用してディーラー向けのツールを開発できるようにしたいと考えています。このデータに対する委任された承認をサポートしたいです。どうすべきですか？

A. OAuth互換のアクセス制御システムを構築または活用する。
B. SAML 2.0 SSO互換性を認証システムに組み込む。
C. パートナーシステムのソースIPアドレスに基づいてデータアクセスを制限する。
D. 信頼できるサードパーティに渡すことができる、各ディーラー用のセカンダリ資格情報を作成する。

**正解: A**

**解説:**
**OAuth 2.0**は、ユーザー（この場合はディーラー）が自身の認証情報（パスワードなど）をサードパーティアプリケーションに直接渡すことなく、特定のリソースへの限定的なアクセス権を**委任（delegate）**するための標準プロトコルです。これにより、サードパーティはディーラーの代理として安全にAPIにアクセスでき、要件である「委任された承認」を安全かつ標準的な方法で実現できます。SAML（B）は主にシングルサインオン（SSO）に使われます。

-----

### <a name="no14"></a>**NO.14**

この問題については、Mountkirk Gamesのケーススタディを参照してください。

Mountkirk Gamesは、新しいテスト戦略の設計をあなたに依頼しました。他のプラットフォーム上の既存のバックエンドと比べて、テストカバレッジはどのように異なるべきですか？

A. テストは、以前のアプローチをはるかに超えてスケーラブルであるべきだ。
B. ユニットテストはもはや不要で、エンドツーエンドテストのみが必要だ。
C. テストは、リリースが本番環境で行われた後に適用されるべきだ。
D. テストには、Google Cloud Platform（GCP）インフラストラクチャを直接テストすることを含めるべきだ。

**正解: A**

**解説:**
ケーススタディには、以前のクラウドプロバイダーで「**スケーリングに問題があった**」ことが明確に記載されています。新しいプラットフォームの主要な技術要件の一つは「**ゲームのアクティビティに基づいて動的にスケールアップまたはダウンする**」ことです。したがって、新しいテスト戦略では、このスケーラビリティを検証することが最重要課題となります。つまり、テスト自体が、予想される大規模な負荷を生成し、システムのスケール性能を正確に検証できるものでなければなりません。

-----

### <a name="no15"></a>**NO.15**

この問題については、Mountkirk Gamesのケーススタディを参照してください。

Mountkirk Gamesは、新しいゲームのためにリアルタイム分析プラットフォームを設定したいと考えています。新しいプラットフォームは、彼らの技術要件を満たす必要があります。どのGoogleテクノロジーの組み合わせがすべての要件を満たしますか？

A. Container Engine, Cloud Pub/Sub, and Cloud SQL
B. Cloud Dataflow, Cloud Storage, Cloud Pub/Sub, and BigQuery
C. Cloud SQL, Cloud Storage, Cloud Pub/Sub, and Cloud Dataflow
D. Cloud Dataproc, Cloud Pub/Sub, Cloud SQL, and Cloud Dataflow
E. Cloud Pub/Sub, Compute Engine, Cloud Storage, and Cloud Dataproc

**正解: B**

**解説:**
技術要件を各サービスにマッピングすると、Bが最適であることがわかります。

  * **Cloud Pub/Sub**: 「ゲームサーバーから直接データをオンザフライで処理する」ためのストリーミングデータ取り込み。
  * **Cloud Dataflow**: 「遅れて到着するデータを処理する」能力を持つ、信頼性の高いストリーム処理。
  * **BigQuery**: 「少なくとも10TBの履歴データへのSQLクエリ」を可能にする、スケーラブルなデータウェアハウス。
  * **Cloud Storage**: 「ユーザーのモバイルデバイスから定期的にアップロードされるファイル」を処理するためのステージングエリア。
    これらはすべて「フルマネージドサービス」であり、技術要件を完全に満たしています。

-----

### <a name="no16"></a>**NO.16**

この問題については、Mountkirk Gamesのケーススタディを参照してください。

Mountkirk Gamesは、隔離されたアプリケーション環境をデプロイするための、再現可能で設定可能なメカニズムを作成する必要があります。開発者とテスターは互いの環境とリソースにアクセスできますが、ステージングまたは本番リソースにはアクセスできません。ステージング環境は、本番環境からいくつかのサービスにアクセスする必要があります。開発環境をステージングおよび本番環境から隔離するために、何をすべきですか？

A. 開発とテスト用に1つのプロジェクトを作成し、ステージングと本番用に別のプロジェクトを作成する。
B. 開発とテスト用に1つのネットワークを作成し、ステージングと本番用に別のネットワークを作成する。
C. 開発用に1つのサブネットワークを作成し、ステージングと本番用に別のサブネットワークを作成する。
D. 開発用に1つのプロジェクト、ステージング用に2つ目のプロジェクト、本番用に3つ目のプロジェクトを作成する。

**正解: D**

**解説:**
Google Cloudにおけるリソース分離と権限管理のベストプラクティスは、環境（開発、ステージング、本番など）ごとに**個別のプロジェクト**を作成することです。プロジェクトは、IAMポリシー、課金、API、リソースの管理単位となります。環境ごとにプロジェクトを分けることで、IAMポリシーを明確に分離でき、「開発者は本番リソースにアクセスできない」といった要件を確実に満たすことができます。また、「ステージングは本番の一部のサービスにアクセスする」といった要件も、サービスアカウントとプロジェクト間のIAM設定で安全に実現できます。

-----

### <a name="no17"></a>**NO.17**

この問題については、Mountkirk Gamesのケーススタディを参照してください。

Mountkirk Gamesは、新しいバックエンドをGoogle Cloud Platform（GCP）にデプロイしました。バックエンドの新しいバージョンが一般にリリースされる前に、徹底的なテストプロセスを作成したいと考えています。テスト環境が経済的な方法でスケールできるようにしたいです。プロセスはどのように設計すべきですか？

A. 本番負荷をシミュレートするために、GCPにスケーラブルな環境を作成する。
B. 既存のインフラストラクチャを使用して、GCPベースのバックエンドを大規模にテストする。
C. 負荷をシミュレートするためにGCP内部のリソースを使用して、アプリケーションの各コンポーネントにストレステストを組み込む。
D. 高、中、低など、さまざまな負荷レベルをテストするために、GCPに一連の静的環境を作成する。

**正解: A**

**解説:**
ケーススタディの要件は、本番環境が「動的にスケールアップまたはダウン」することです。このスケーラビリティを正確にテストするためには、テスト環境自体も同様にスケーラブルである必要があります。GCPの利点は、必要なときに必要なだけリソース（負荷生成用のインスタンスなど）を確保し、テスト終了後には破棄できるため、**経済的**に大規模な負荷テストを実施できる点です。静的な環境（D）では、ピーク時の負荷をシミュレートできず、本番のスケーリング動作を検証できません。

-----

### <a name="no18"></a>**NO.18**

この問題については、Mountkirk Gamesのケーススタディを参照してください。

Mountkirk Gamesのゲームサーバーが適切に自動スケーリングしていません。先月、彼らは新機能を展開しましたが、それが突然非常に人気になりました。記録的な数のユーザーがサービスを利用しようとしていますが、多くが503エラーと非常に遅い応答時間を受け取っています。最初に何を調査すべきですか？

A. データベースがオンラインであることを確認する。
B. プロジェクトの割り当て（クォータ）が超えられていないことを確認する。
C. 新機能のコードにパフォーマンスのバグが導入されていないことを確認する。
D. 負荷テストチームが本番環境に対してツールを実行していないことを確認する。

**正解: B**

**解説:**
503 Service Unavailableエラーは、サーバーがリクエストを処理できない状態を示します。自動スケーリングが設定されているにもかかわらずサーバーが増えず、高負荷に対応できていない状況です。このような場合、最も一般的な原因の一つが、プロジェクトに設定されたリソースの**クォータ（上限）**に達してしまったことです。例えば、「リージョンあたりのCPUコア数」や「IPアドレス数」などのクォータ上限に達すると、オートスケーラーは新しいVMインスタンスを作成できず、結果として既存のサーバーが過負荷になり503エラーを返します。したがって、最初にクォータを確認することが最も論理的なトラブルシューティングのステップです。

-----

### <a name="no19"></a>**NO.19**

この問題については、Mountkirk Gamesのケーススタディを参照してください。

Mountkirk Gamesは、継続的デリバリーパイプラインを設定したいと考えています。彼らのアーキテクチャには、迅速に更新およびロールバックできるようにしたい多くの小さなサービスが含まれています。Mountkirk Gamesには以下の要件があります：

  * サービスは、米国とヨーロッパの複数のリージョンにわたって冗長にデプロイされる。
  * フロントエンドサービスのみがパブリックインターネットに公開される。
  * サービスのフリートに対して単一のフロントエンドIPを提供できる。
  * デプロイメントアーティファクトは不変である。
    どの製品セットを使用すべきですか？

A. Google Cloud Storage, Google Cloud Dataflow, Google Compute Engine
B. Google Cloud Storage, Google App Engine, Google Network Load Balancer
C. Google Kubernetes Registry, Google Container Engine, Google HTTP(S) Load Balancer
D. Google Cloud Functions, Google Cloud Pub/Sub, Google Cloud Deployment Manager

**正解: C**

**解説:**
各要件に対応する最適なGoogle Cloudサービスは以下の通りです。

  * **不変のアーティファクト**: **Google Kubernetes Registry** (現 Artifact Registry) は、Dockerコンテナイメージ（不変のアーティファクト）を保存するためのレジストリです。
  * **複数リージョンへの冗長デプロイ**: **Google Container Engine (GKE)** は、コンテナ化されたアプリケーションを複数のリージョンにまたがるクラスタで管理・オーケストレーションするのに最適です。
  * **単一IPでのグローバル公開**: **Google HTTP(S) Load Balancer** は、単一のエニーキャストIPアドレスで、最も近いリージョンのバックエンド（この場合はGKEサービス）にトラフィックをインテリジェントにルーティングできるグローバルなL7ロードバランサです。

-----

### <a name="no20"></a>**NO.20**

この問題については、JencoMartのケーススタディを参照してください。

JencoMartのセキュリティチームは、すべてのGoogle Cloud Platformインフラストラクチャが、本番リソースと開発リソースの間の職務分離を伴う最小権限モデルを使用してデプロイされることを要求しています。どのようなGoogleドメインとプロジェクト構造を推奨すべきですか？

A. ユーザーを管理するために2つのG Suiteアカウントを作成する：1つは開発/テスト/ステージング用、もう1つは本番用。各アカウントには、アプリケーションごとに1つのプロジェクトを含めるべきだ。
B. ユーザーを管理するために2つのG Suiteアカウントを作成する：1つはすべての開発アプリケーション用の単一プロジェクト、もう1つはすべての本番アプリケーション用の単一プロジェクト。
C. 各アプリケーションの各ステージが独自のプロジェクトにある単一のG Suiteアカウントを作成してユーザーを管理する。
D. 開発/テスト/ステージング環境用に1つのプロジェクト、本番環境用に1つのプロジェクトを持つ単一のG Suiteアカウントを作成してユーザーを管理する。

**正解: D**

**解説:**
Google Cloudのベストプラクティスでは、通常、単一の**G Suite**（現 Google Workspace）または**Cloud Identity**アカウントを使用して、組織全体のユーザーIDを一元管理します。そして、環境の分離は**プロジェクト**レベルで行います。本番環境と非本番（開発/テスト/ステージング）環境を別々のプロジェクトに分けることで、IAMポリシーを明確に分離できます。例えば、開発者は開発プロジェクトでは幅広い権限を持つことができますが、本番プロジェクトでは閲覧権限のみ、あるいは全く権限を持たないように設定でき、これにより「最小権限」と「職務分離」の要件を満たすことができます。

-----

### <a name="no21"></a>**NO.21**

この問題については、JencoMartのケーススタディを参照してください。

JencoMartがユーザー認証情報データベースをGoogle Cloud Platformに移行し、旧サーバーを停止してから数日後、新しいデータベースサーバーがSSH接続に応答しなくなりました。アプリケーションサーバーからのデータベースリクエストは正常に処理されています。問題の診断のために、どの3つのステップを実行すべきですか？ （3つ選択）

A. 仮想マシン（VM）とディスクを削除し、新しいものを作成する。
B. インスタンスを削除し、ディスクを新しいVMにアタッチして調査する。
C. ディスクのスナップショットを作成し、新しいマシンに接続して調査する。
D. マシンが接続されているネットワークの受信ファイアウォールルールを確認する。
E. マシンを非常に単純なファイアウォールルールを持つ別のネットワークに接続して調査する。
F. インスタンスのシリアルコンソール出力を表示してトラブルシューティングし、インタラクティブコンソールを有効にして調査する。

**正解: C, D, F**

**解説:**
データベースサービスは稼働しているがSSHができないという状況では、VM自体を破壊するAやBは最終手段です。まずは破壊せずに調査すべきです。

  * **D (ファイアウォール確認)**: 最も一般的な原因は、SSHポート（TCP/22）を許可するファイアウォールルールがない、または誤って変更されたことです。最初に確認すべき点です。
  * **F (シリアルコンソール)**: ネットワーク設定に関わらず、VMの最も低レベルな出力にアクセスする方法がシリアルコンソールです。起動時のエラーやOSレベルの問題を確認するのに非常に有効です。
  * **C (スナップショット)**: VMを稼働させたままディスクの完全なコピー（スナップショット）を作成し、それを別の新しいVMにアタッチして安全に内部を調査することができます。これにより、本番VMに影響を与えることなく、ログファイルなどを詳しく調べることが可能です。

-----

### <a name="no22"></a>**NO.22**

この問題については、JencoMartのケーススタディを参照してください。

JencoMartは、アジア向けにトラフィックを供給するアプリケーションのバージョンをGoogle Cloud Platform上に構築しました。あなたは、ビジネスおよび技術的な目標に対する成功を測定したいと考えています。どのメトリクスを追跡すべきですか？

A. アジアからのリクエストのエラー率
B. 米国とアジアのレイテンシの違い
C. アジアからの総訪問数、エラー率、レイテンシ
D. アジアのユーザーの総訪問数と平均レイテンシ
E. データベースに存在する文字セットの数

**正解: D**

**解説:**
ケーススタディにおけるCEOの声明には「アジアでのレイテンシを減少させる」という明確な技術的目標と、「より多くの人々がウェブにアクセスするにつれて顧客との関係を築く」というビジネス目標が述べられています。したがって、この取り組みの成功を測定するには、

  * **平均レイテンシ**: ユーザー体験の質と技術目標の達成度を直接測定します。
  * **総訪問数**: アジア市場でのビジネスの成長と顧客エンゲージメントを示します。
    この2つのメトリクスが最も重要です。

-----

### <a name="no23"></a>**NO.23**

この問題については、JencoMartのケーススタディを参照してください。

JencoMartのアプリケーションのGoogle Cloud Platform（GCP）への移行が遅すぎます。インフラは図に示されています。スループットを最大化したいです。潜在的なボトルネックはどれですか？（3つ選択）

（注：図にはオンプレミスのラック、エッジルーター、Cloud VPN、GCP上のマネージドグループとCloud Storageが示されています。）

A. スループットを制限する単一のVPNトンネル
B. このタスクに適していないGoogle Cloud Storageの階層
C. 長距離での操作に適していないコピーコマンド
D. オンプレミスのマシンよりもGCPの仮想マシン（VM）が少ないこと
E. VMの外部にある、このタスクに適していない別のストレージ層
F. オンプレミスインフラストラクチャとGCP間の複雑なインターネット接続

**正解: A, D, F**

**解説:**

  * **A (単一VPNトンネル)**: Cloud VPNの各トンネルにはスループットの上限（約1.5〜3.0 Gbps）があります。大規模なデータ移行では、単一のトンネルがボトルネックになる可能性があります。
  * **D (VM数)**: データ移行プロセスにVMが関わっている場合（例：データの変換や転送の中継）、GCP側のVMの数がオンプレミス側より少ないと、処理能力が追いつかずにボトルネックになります。
  * **F (インターネット接続)**: Cloud VPNはパブリックなインターネットを経由します。インターネットの経路は複雑で、帯域幅、レイテンシ、パケットロスなどが変動しやすく、安定した高いスループットを維持する上でのボトルネックとなり得ます。

-----

### <a name="no24"></a>**NO.24**

この問題については、JencoMartのケーススタディを参照してください。

JencoMartは、ユーザープロファイルのストレージをGoogle Cloud Datastoreに、アプリケーションサーバーをGoogle Compute Engine（GCE）に移行することを決定しました。移行中、既存のオンプレミスインフラストラクチャはデータをアップロードするためにDatastoreにアクセスする必要があります。どのようなサービスアカウントキー管理戦略を推奨すべきですか？

A. オンプレミスインフラストラクチャ用とGCE仮想マシン（VM）用の両方にサービスアカウントキーをプロビジョニングする。
B. オンプレミスインフラストラクチャはユーザーアカウントで認証し、VMにはサービスアカウントキーをプロビジョニングする。
C. オンプレミスインフラストラクチャ用にサービスアカウントキーをプロビジョニングし、VMにはGoogle Cloud Platform（GCP）管理のキーを使用する。
D. GCE/Google Kubernetes Engine（GKE）上にオンプレミスインフラストラクチャ用のカスタム認証サービスを展開し、VMにはGCP管理のキーを使用する。

**正解: C**

**解説:**
Google Cloudのベストプラクティスは、**GCPリソース（この場合はGCE VM）上では、サービスアカウントキー（JSONファイル）を直接管理しない**ことです。GCEインスタンスにはサービスアカウントを直接アタッチでき、GCPがキーの管理とローテーションを自動的に行います（GCP管理のキー）。一方、**GCPの外部（オンプレミスなど）**からGCPサービスにアクセスするには、サービスアカウントキーを作成し、ダウンロードして使用する必要があります。したがって、オンプレミスにはキーをプロビジョニングし、GCE VMにはGCP管理のキー（アタッチされたサービスアカウント）を使用するのが最も安全で推奨される方法です。

-----

### <a name="no25"></a>**NO.25**

この問題については、JencoMartのケーススタディを参照してください。

JencoMartは、ユーザープロファイルデータベースをGoogle Cloud Platformに移行したいと考えています。どのGoogleデータベースを使用すべきですか？

A. Cloud Spanner
B. Google BigQuery
C. Google Cloud SQL
D. Google Cloud Datastore

**正解: D**

**解説:**
[cite_start]Google Cloudの公式ドキュメントでは、**Cloud Datastore（現在はFirestore in Datastore mode）**の一般的なワークロードとして「**ユーザープロファイル**」や「商品カタログ」が明記されています [cite: 595, 596]。Datastoreは、スケーラブルでスキーマレスなNoSQLデータベースであり、構造化されたオブジェクトを保存するのに適しているため、ユーザープロファイルの格納に最適です。

-----

### <a name="no26"></a>**NO.26**

あなたは、Compute Engine仮想マシンインスタンスでアプリケーションをホストする、新しく作成されたGoogle Cloudプロジェクトのクラウドネットワークアーキテクチャを構成しています。インスタンスは、単一リージョン内の2つの異なるサブネット（sub-aおよびsub-b）に作成されます。

  * sub-aのインスタンスはパブリックIPアドレスを持ちます。
  * sub-bのインスタンスはプライベートIPアドレスのみを持ちます。
    更新されたパッケージをダウンロードするために、インスタンスはGoogle Cloudの外部にあるパブリックリポジトリに接続する必要があります。sub-bが外部リポジトリにアクセスできるようにする必要があります。どうすべきですか？

A. sub-bでプライベートGoogleアクセスを有効にする。
B. Cloud NATを構成し、NATマッピングセクションでsub-bを選択する。
C. sub-aに踏み台ホストインスタンスを構成して、sub-bのインスタンスに接続する。
D. sub-bのインスタンスに対してTCP転送用のIdentity-Aware Proxyを有効にする。

**正解: B**

**解説:**
[cite_start]プライベートIPアドレスしか持たないVMインスタンスが、インターネット上の外部リソース（パブリックリポジトリなど）に**外向き（アウトバウンド）のアクセスを行うための標準的なソリューションがCloud NAT**です [cite: 676][cite_start]。Cloud NATを構成し、対象のサブネット（sub-b）を指定することで、sub-b内のインスタンスは外部へのリクエストを送信でき、その応答を受け取ることができます。この際、インスタンス自体がパブリックIPを持つ必要はありません [cite: 677]。

-----

### <a name="no27"></a>**NO.27**

リクエストされるURLパスに基づいてグローバルな負荷分散を行うソリューションを設計する必要があります。Googleのベストプラクティスに基づき、運用の信頼性とエンドツーエンドの転送中暗号化を保証する必要があります。どうすべきですか？

A. URLマップを使用してクロスリージョンロードバランサを作成する。
B. URLマップを使用してHTTPSロードバランサを作成する。
C. 適切なインスタンスグループとインスタンスを作成する。SSLプロキシロードバランシングを構成する。
D. グローバル転送ルールを作成する。SSLプロキシバランシングを構成する。

**正解: B**

**解説:**
要件は以下の通りです。

  * **グローバル負荷分散**: 世界中のユーザーからのトラフィックを分散させる。
  * **URLパスベース**: `/images/*` や `/api/*` のようなパスに応じてトラフィックを異なるバックエンドにルーティングする。
  * **エンドツーエンド暗号化**: ユーザーからロードバランサ、ロードバランサからバックエンドまで暗号化する。

[cite_start]これらの要件をすべて満たすのは**外部HTTPSロードバランサ（Global External HTTPS Load Balancer）**です [cite: 691]。このロードバランサはグローバルであり、「URLマップ」機能を使用してパスベースのルーティングをサポートしています。また、SSL証明書を設定することで、クライアントとの通信を暗号化し、バックエンドとの通信も暗号化できます。

-----

### <a name="no28"></a>**NO.28**

あなたはGoogle Cloudにアプリケーションをデプロイしています。このアプリケーションは、Google Cloud以外の環境にあるアプリケーションとプライベートネットワークを介して通信する必要があります。予想される平均スループットは200kbpsです。ビジネス要件は次のとおりです：

  * 99.99%のシステム可用性
  * コスト最適化
    ビジネス要件を満たすために、ロケーション間の接続を設計する必要があります。何をプロビジョニングすべきですか？

A. 1つのトンネルでオンプレミスのVPNゲートウェイに接続されたClassic Cloud VPNゲートウェイ。
B. 2つのトンネルでオンプレミスのVPNゲートウェイに接続されたClassic Cloud VPNゲートウェイ。
C. 2つのトンネルでオンプレミスのVPNゲートウェイに接続されたHA Cloud VPNゲートウェイ。
D. 2つのHA Cloud VPNゲートウェイを2つのオンプレミスVPNゲートウェイに接続する。各HA Cloud VPNゲートウェイに2つのトンネルを設定し、それぞれを異なるオンプレミスVPNゲートウェイに接続する。

**正解: B**

**解説:**
スループット要件は200kbpsと非常に低いため、高価なHA VPNやDedicated Interconnectは「コスト最適化」の観点から過剰です。**Classic VPN**は、低スループットのユースケースに適した、より安価なオプションです。99.99%の可用性を目指すには、単一のトンネル(A)では不十分です。単一のVPNゲートウェイに**2つのトンネル**を設定することで、トンネルの障害に対する冗長性を確保でき、可用性を高めることができます。これがコストと可用性のバランスを取った構成です。

-----

### <a name="no29"></a>**NO.29**

この問題については、Dress4Winのケーススタディを参照してください。

あなたは、Dress4Winの販売および税務記録を、監査人による**稀な閲覧**のために少なくとも**10年間**利用可能な状態に保つ必要があります。**コスト最適化**が最優先事項です。どのクラウドサービスを選択すべきですか？

A. データを保存するためにGoogle Cloud Storage Coldlineを使用し、データにアクセスするためにgsutilを使用する。
B. データを保存するためにGoogle Cloud Storage Nearlineを使用し、データにアクセスするためにgsutilを使用する。
C. データを保存するために米国またはEUをロケーションとするGoogle Bigtableを使用し、データにアクセスするためにgcloudを使用する。
D. データを保存するためにBigQueryを使用し、データにアクセスするためにマネージドインスタンスグループ内のWebサーバークラスターを使用する。

**正解: A**

**解説:**
[cite_start]要件は「10年間の長期保存」「稀なアクセス」「コスト最適化」です。Google Cloud Storageのストレージクラスの中で、**Coldline Storage**は、年に1回以下のアクセス頻度を想定したアーカイブデータ用に設計されており、ストレージ料金が最も安価です [cite: 715]。 Nearline（B）よりもさらに長期間の保存に適しており、このユースケースに完全に一致します。gsutilはCloud Storageを操作するための標準的なコマンドラインツールです。

-----

### <a name="no30"></a>**NO.30**

あなたのカスタマーサポートツールは、すべてのメールとチャットの会話を保持と分析のためにCloud Bigtableに記録します。初期保存の前に、このデータから個人識別情報（PII）や支払いカード情報をサニタイズ（無害化）するための推奨されるアプローチは何ですか？

A. SHA256を使用してすべてのデータをハッシュ化する。
B. 楕円曲線暗号を使用してすべてのデータを暗号化する。
C. Cloud Data Loss Prevention APIを使用してデータを非識別化する。
D. 正規表現を使用して電話番号、メールアドレス、クレジットカード番号を見つけて編集する。

**正解: C**

**解説:**
この問題は、PDFの正解(A)が誤っている可能性が高い典型例です。ハッシュ化(A)はデータを不可逆的に変換するため、元の情報を復元できなくなり「分析」の目的を果たせません。暗号化(B)はデータを保護しますが、分析のためには復号する必要があり、キー管理が複雑になります。**Cloud Data Loss Prevention (DLP) API**は、まさにこの目的のために設計されたサービスです。DLPは、マスキング、トークン化、仮名化などの技術を用いて、データの有用性を保ちながら機密部分（PIIなど）を**非識別化**することができます。これにより、安全にデータを分析・活用できます。

-----

### <a name="no31"></a>**NO.31**

この問題については、Dress4Winのケーススタディを参照してください。

Dress4Winのセキュリティチームは、Google Cloud Platform（GCP）上の本番仮想マシン（VM）への外部からのSSHアクセスを無効にしました。運用チームはVMをリモート管理し、Dockerコンテナをビルド・プッシュし、Google Cloud Storageオブジェクトを管理する必要があります。彼らは何をすることができますか？

A. 運用エンジニアにGoogle Cloud Shellを使用するアクセス権を付与する。
B. GCPへのVPN接続を構成して、クラウドVMへのSSHアクセスを許可する。
C. 運用エンジニアがタスクを実行する必要があるときに、クラウドVMへの一時的なSSHアクセスを許可する新しいアクセス要求プロセスを開発する。
D. 運用チームが特定のRPCを実行してタスクを達成できるようにするAPIサービスを開発チームに構築させる。

**正解: A**

**解説:**
**Google Cloud Shell**は、ブラウザからアクセスできるコマンドライン環境で、gcloud CLI、kubectl、Dockerなどのツールがプリインストールされています。Cloud ShellはGCP環境内から実行されるため、VM自体に外部IPやSSHアクセスがなくても、`gcloud compute ssh`コマンド（IAPトンネリング経由）で内部的にVMに接続したり、Dockerコマンドを実行したり、gsutilでCloud Storageを操作したりできます。これは、外部SSHアクセスを禁止するというセキュリティ要件を満たしつつ、運用タスクを実行するための最も簡単で安全な方法です。

-----

### <a name="no32"></a>**NO.32**

あなたは、クラスタ内部に留まるべき異なるマイクロサービスを使用してアプリケーションを開発しています。各マイクロサービスを特定のレプリカ数で構成できるようにしたいと考えています。また、マイクロサービスがスケールするレプリカ数に関係なく、クラスタ内の他のマイクロサービスから統一された方法で特定のマイクロサービスにアドレス指定できるようにしたいです。このソリューションをGoogle Kubernetes Engineに実装する必要があります。どうすべきですか？

A. 各マイクロサービスをDeploymentとしてデプロイする。Serviceを使用してクラスタ内でDeploymentを公開し、クラスタ内の他のマイクロサービスからアドレス指定するためにServiceのDNS名を使用する。
B. 各マイクロサービスをDeploymentとしてデプロイする。Ingressを使用してクラスタ内でDeploymentを公開し、クラスタ内の他のマイクロサービスからDeploymentをアドレス指定するためにIngressのIPアドレスを使用する。
C. 各マイクロサービスをPodとしてデプロイする。Serviceを使用してクラスタ内でPodを公開し、クラスタ内の他のマイクロサービスからマイクロサービスをアドレス指定するためにServiceのDNS名を使用する。
D. 各マイクロサービスをPodとしてデプロイする。Ingressを使用してクラスタ内でPodを公開し、クラスタ内の他のマイクロサービスからPodをアドレス指定するためにIngressのIPアドレス名を使用する。

**正解: A**

**解説:**
Kubernetesにおけるベストプラクティスは以下の通りです。

  * **レプリカの管理**: **Deployment**リソースは、Podのレプリカ数を宣言的に管理し、ローリングアップデートなどを可能にするための標準的な方法です。Podを直接管理する(C, D)のは一般的ではありません。
  * [cite_start]**安定したアクセスポイント**: PodのIPアドレスは変動するため、直接アクセスするべきではありません。**Service**リソースは、一連のPodに対して安定した単一のアクセスポイント（クラスタ内DNS名と仮想IP）を提供します。これにより、他のマイクロサービスはPodの数や状態に関わらず、常に同じDNS名でターゲットのサービスに接続できます [cite: 739]。
  * **内部通信**: Ingress(B)は主に**外部**からのHTTP(S)トラフィックをクラスタ内にルーティングするためのものであり、内部マイクロサービス間の通信にはServiceのDNS名を使用するのが標準です。

-----

### <a name="no33"></a>**NO.33**

あなたはCompute Engine上でデプロイされたいくつかの内部アプリケーションを管理しています。ビジネスユーザーから、あるアプリケーションが過去数日間で非常に遅くなったと報告がありました。問題を解決するために、根本的な原因を見つけたいです。最初に何をすべきですか？

A. Cloud LoggingとCloud Monitoringでインスタンスからのログとメトリクスを検査する。
B. アプリケーションが遅くなる前の時点からアプリケーションデータベースのバックアップを復元する。
C. アプリケーションを自動スケーリングが有効なマネージドインスタンスグループにデプロイする。マネージドインスタンスグループの前にロードバランサを追加し、ユーザーにロードバランサのIPに接続させる。
D. アプリケーションの背後にあるCompute Engineインスタンスを、より多くのCPUとメモリを持つマシンタイプに変更する。

**正解: A**

**解説:**
[cite_start]パフォーマンス問題が発生した場合、最初にすべきことは**現状の把握と原因の特定**です [cite: 757][cite_start]。B、C、Dはすべて具体的な対策ですが、原因がわからないまま実行すると、問題を悪化させたり、不要なコストを発生させたりする可能性があります。**Cloud Logging**でアプリケーションやシステムのエラーログを確認し、**Cloud Monitoring**でCPU使用率、メモリ使用率、ディスクI/Oなどの**メトリクス**を調べることで、リソースの枯渇、コードのエラー、外部サービスの遅延など、問題の根本原因を特定するための手がかりを得ることができます [cite: 758, 760]。

-----

### <a name="no34"></a>**NO.34**

あなたの会社は、単一のMySQLインスタンス上で複数のデータベースを実行しています。特定のデータベースのバックアップを定期的に取得する必要があります。バックアップアクティビティは可能な限り迅速に完了し、ディスクのパフォーマンスに影響を与えないようにする必要があります。ストレージはどのように構成すべきですか？

A. cronジョブを設定して、gcloudツールを使用して永続ディスクスナップショットによる定期的なバックアップを取得する。
B. ローカルSSDボリュームをバックアップ場所としてマウントする。バックアップ完了後、gsutilを使用してバックアップをGoogle Cloud Storageに移動する。
C. gcsfuseを使用してGoogle Cloud Storageバケットをインスタンスに直接ボリュームとしてマウントし、mysqldumpを使用してマウントされた場所にバックアップを書き込む。
D. 追加の永続ディスクボリュームを各仮想マシン（VM）インスタンスにRAID10アレイでマウントし、LVMを使用してスナップショットを作成しCloud Storageに送信する。

**正解: B**

**解説:**
要件は「迅速な完了」と「ディスクパフォーマンスへの影響なし」です。

  * **ローカルSSD**は、VMホストに物理的に接続された非常に高速なストレージであり、バックアップのようなI/O集中型の操作を非常に迅速に完了させることができます。
  * バックアップをローカルSSDに書き込むことで、データベースが稼働しているプライマリの永続ディスクへのI/O負荷を回避でき、パフォーマンスへの影響を最小限に抑えられます。
  * [cite_start]バックアップ完了後、長期保存のために安価な**Cloud Storage**に非同期で移動させるのがベストプラクティスです [cite: 767]。
    永続ディスクスナップショット(A)はディスク全体のコピーであり、ディスクパフォーマンスに影響を与える可能性があります。

-----

### <a name="no35"></a>**NO.35**

あなたは、特定のゾーンからアプリケーションにサービスを提供するために、単一のCloud SQLインスタンスを使用しています。高可用性を導入したいです。どうすべきですか？

A. 異なるリージョンにリードレプリカインスタンスを作成する。
B. 異なるリージョンにフェイルオーバーレプリカインスタンスを作成する。
C. 同じリージョン内の異なるゾーンにリードレプリカインスタンスを作成する。
D. 同じリージョン内の異なるゾーンにフェイルオーバーレプリカインスタンスを作成する。

**正解: D**

**解説:**
[cite_start]PDFの回答Bは誤りです。Cloud SQLの**高可用性（HA）構成**は、プライマリインスタンスとは**同じリージョン内の異なるゾーン**に**フェイルオーバーレプリカ**を配置することで実現されます [cite: 780]。これにより、ゾーン障害が発生した場合、Cloud SQLは自動的にフェイルオーバーレプリカに切り替えます。リードレプリカ(A, C)は読み取り性能のスケールアウト用であり、自動フェイルオーバーの機能はありません。異なるリージョン(B)にフェイルオーバーレプリカを構成するのは、リージョン障害に備えるディザスタリカバリ（DR）目的であり、単一ゾーン障害に対するHAとは目的が異なります。

-----

### <a name="no36"></a>**NO.36**

あなたの組織は、すべてのアプリケーションからのメトリクスを、将来の法的手続きでの分析のために5年間保持する必要があります。どのアプローチを使用すべきですか？

A. セキュリティチームに各プロジェクトのログへのアクセス権を付与する。
B. すべてのプロジェクトに対してStackdriver Monitoringを構成し、BigQueryにエクスポートする。
C. すべてのプロジェクトに対してデフォルトの保持ポリシーを持つStackdriver Monitoringを構成する。
D. すべてのプロジェクトに対してStackdriver Monitoringを構成し、Google Cloud Storageにエクスポートする。

**正解: D**

**解説:**
[cite_start]**Cloud Monitoring**のデフォルトのメトリクス保持期間は数週間から数ヶ月であり、5年間という要件を満たしません。そのため、長期保存のためには外部に**エクスポート**する必要があります。エクスポート先として**Google Cloud Storage**は、低コストなストレージクラス（Nearline, Coldline, Archive）を利用できるため、長期間のデータアーカイブに最適です。BigQuery(B)も可能ですが、主に分析クエリを頻繁に行う用途に向いており、純粋な長期アーカイブ目的ではCloud Storageの方がコスト効率が良いです [cite: 791]。

-----

### <a name="no37"></a>**NO.37**

この問題については、Dress4Winのケーススタディを参照してください。

Dress4Winは、エンドポイントの100%をカバーするエンドツーエンドテストを持っています。彼らは、クラウドへの移行が新たなバグを導入しないことを確実にしたいと考えています。停止を防ぐために、開発者はどの追加のテスト方法を採用すべきですか？

A. コードが意図した通りに動作しているか判断するために、クラウドのステージング環境でエンドツーエンドテストを実行する。
B. コード内のエラーを表示するために、アプリケーションコードでGoogle Stackdriver Debuggerを有効にする。
C. クラウドのステージング環境で、追加のユニットテストと本番規模の負荷テストを追加する。
D. 新しいリリースがレイテンシにどれだけの影響を与えるかを開発者が測定できるように、カナリアテストを追加する。

**正解: C**

**解説:**
PDFの回答Bは不適切です。Stackdriver Debuggerは本番環境でコードの状態を調査するためのツールであり、バグを防ぐための「テスト」方法ではありません。
エンドツーエンドテストは既に存在しますが、クラウド移行では環境が大きく変わるため、以下のテストが重要になります。

  * **追加のユニットテスト**: 既存のテストがクラウド特有の動作（例: SDKの挙動）をカバーしているか確認し、必要に応じて拡充します。
  * **本番規模の負荷テスト**: クラウド環境のスケーラビリティやパフォーマンスの特性はオンプレミスと全く異なります。オートスケーリングが正しく機能するか、データベースのパフォーマンスが十分かなどを確認するために、本番と同等規模の負荷をかけたテストが不可欠です。これにより、オンプレミスでは現れなかったパフォーマンス関連のバグを発見できます。

-----

### <a name="no38"></a>**NO.38**

あなたはカスタムJavaアプリケーションをGoogle App Engineにデプロイします。デプロイに失敗し、以下のスタックトレースが表示されます。

`Java.lang.securityException: SHA1 digest error for com/Altostrat/CloakedServlet.class`
...(スタックトレースの続き)

どうすべきですか？

A. CloakedServletクラスをMD5ハッシュの代わりにSHA1を使用して再コンパイルする。
B. すべてのJARファイルにデジタル署名し、アプリケーションを再デプロイする。
C. 不足しているJARファイルをアップロードし、アプリケーションを再デプロイする。

**正解: B**

**解説:**
`SecurityException: SHA1 digest error` というエラーメッセージは、JARファイルのマニフェスト（`MANIFEST.MF`）に記録されているファイルのハッシュ値と、実際に展開されたファイルのハッシュ値が一致しないことを示しています。これは、JARファイルが破損しているか、署名後に変更された場合に発生します。App Engine環境のセキュリティ要件を満たすために、すべてのJARファイルに**デジタル署名**を正しく行い、ファイルが改ざんされていないことを保証してから再デプロイすることで、この問題を解決できます。

-----

### <a name="no39"></a>**NO.39**

あなたの会社は、開発者体験を向上させるためにAPIの大幅な改訂を決定しました。古いバージョンのAPIを利用可能かつデプロイ可能な状態に保ちつつ、新しい顧客やテスターが新しいAPIを試せるようにする必要があります。同じSSL証明書とDNSレコードを維持して両方のAPIを提供したいです。どうすべきですか？

A. 新しいバージョンのAPI用に新しいロードバランサを構成する。
B. 古いクライアントを再構成して、新しいAPI用に新しいエンドポイントを使用させる。
C. 古いAPIがパスに基づいて新しいAPIにトラフィックを転送するようにする。
D. ロードバランサの背後で、APIパスごとに別々のバックエンドプールを使用する。

**正解: D**

**解説:**
[cite_start]単一のロードバランサ（同じDNSとSSL）を維持しつつ、異なるバージョンのAPIを共存させるには、**パスベースのルーティング**が最適な方法です。外部HTTPSロードバランサでは、URLマップを構成して、リクエストのパス（例: `/v1/*` と `/v2/*`）に応じてトラフィックを異なる**バックエンドサービス（バックエンドプール）**に転送できます [cite: 828]。これにより、`api.example.com/v1/` へのリクエストは古いAPIのバックエンドに、`api.example.com/v2/` へのリクエストは新しいAPIのバックエンドに振り分けることができ、シームレスな移行とテストが可能になります。

-----

### <a name="no40"></a>**NO.40**

あなたの顧客は、eコマースサイトでユーザーに商品推薦を提供するために使用されるWebサービスを運営しています。同社は、結果の質を向上させるためにGoogle Cloud Platform上で機械学習モデルの実験を開始しました。時間をかけてモデルの結果を改善するために、顧客は何をすべきですか？

A. StackdriverからCloud Machine Learning EngineのパフォーマンスメトリクスをBigQueryにエクスポートし、モデルの効率を分析するために使用する。
B. Cloud GPUからCloud TPUへの機械学習モデルのトレーニング移行ロードマップを構築する。Cloud TPUはより良い結果を提供する。
C. Compute Engineの発表を監視し、新しいCPUアーキテクチャが利用可能になり次第、追加のパフォーマンスを得るためにモデルをデプロイする。
D. 推薦の履歴と推薦結果をBigQueryに保存し、トレーニングデータとして使用する。

**正解: D**

**解説:**
[cite_start]機械学習モデルの性能を継続的に改善するための最も基本的かつ重要なアプローチは、**新しいデータで再トレーニング**することです。ユーザーに提示した推薦（predictions）と、それに対するユーザーの反応（クリックした、購入したなど）を**結果（outcomes）**として収集し、それらを新しい**トレーニングデータ（フィードバックループ）**として利用することで、モデルは時間とともにより正確でパーソナライズされた推薦を行えるようになります [cite: 840]。BigQueryは、このような大規模なインタラクションログを保存・分析し、トレーニングデータセットを準備するのに最適な場所です。

-----

### <a name="no41"></a>**NO.41**

あなたはGoogle Cloudにアプリケーションをデプロイする必要があります。アプリケーションはTCP経由でトラフィックを受信し、ファイルシステムへの読み書きを行います。アプリケーションは水平スケーリングをサポートしていません。同時アクセスは破損を引き起こすため、アプリケーションプロセスはファイルシステム上のデータを完全に制御する必要があります。ビジネスはインシデント発生時のダウンタイムを受け入れますが、アプリケーションはビジネス運営をサポートするために24時間365日利用可能である必要があります。Google Cloud上でこのアプリケーションのアーキテクチャを設計する必要があります。どうすべきですか？

A. 複数ゾーンにインスタンスを持つマネージドインスタンスグループを使用し、Cloud Filestoreを使用し、インスタンスの前にHTTPロードバランサを使用する。
B. 複数ゾーンにインスタンスを持つマネージドインスタンスグループを使用し、Cloud Filestoreを使用し、インスタンスの前にネットワークロードバランサを使用する。
C. 異なるゾーンにアクティブインスタンスとスタンバイインスタンスを持つ非マネージドインスタンスグループを使用し、リージョン永続ディスクを使用し、インスタンスの前にHTTPロードバランサを使用する。
D. 異なるゾーンにアクティブインスタンスとスタンバイインスタンスを持つ非マネージドインスタンスグループを使用し、リージョン永続ディスクを使用し、インスタンスの前にネットワークロードバランサを使用する。

**正解: D**

**解説:**

  * **水平スケーリング不可**: マネージドインスタンスグループ(A, B)は水平スケーリングを前提としているため不適切です。アクティブ/スタンバイ構成が適しています。
  * **単一ファイルシステムへのアクセス**: 複数のインスタンスが同時に同じファイルシステムに書き込むと破損するため、共有ファイルシステム（Cloud Filestore）は使えません。ゾーン障害時にデータを引き継ぐ必要があるため、**リージョン永続ディスク**が最適です。これは、2つのゾーン間でデータを同期的に複製し、一方のゾーンの障害時にはもう一方のゾーンのインスタンスにアタッチできます。
  * **TCPトラフィック**: HTTPロードバランサ(A, C)はHTTP(S)トラフィックしか扱えません。TCPトラフィックを扱うには**ネットワークロードバランサ**が必要です。
    これらの要件を組み合わせると、Dが唯一の正しい選択肢となります。

-----

### <a name="no42"></a>**NO.42**

あなたの会社は、バックエンドとしていくつかの公開APIを持つ、高信頼性のWebアプリケーションを構築したいと考えています。ユーザーのトラフィックは多くないと予想されますが、時折急増する可能性があります。Cloud Load Balancingを活用し、ソリューションはユーザーにとって費用対効果が高いものでなければなりません。どうすべきですか？

A. HTMLや画像などの静的コンテンツをCloud CDNに保存する。APIをApp Engineでホストし、ユーザーデータをCloud SQLに保存する。
B. HTMLや画像などの静的コンテンツをCloud Storageバケットに保存する。APIを複数ゾーンにワーカーノードを持つゾーン別Google Kubernetes Engineクラスタでホストし、ユーザーデータをCloud Spannerに保存する。
C. HTMLや画像などの静的コンテンツをCloud CDNに保存する。Cloud Runを使用してAPIをホストし、ユーザーデータをCloud SQLに保存する。
D. HTMLや画像などの静的コンテンツをCloud Storageバケットに保存する。Cloud Functionsを使用してAPIをホストし、ユーザーデータをFirestoreに保存する。

**正解: D**

**解説:**
要件は「高信頼性」「トラフィックの急増に対応」「費用対効果」です。

  * **静的コンテンツ**: **Cloud Storage**は静的コンテンツをホストするための最も安価でスケーラブルな方法です。
  * **APIホスティング**: トラフィックが少ない時やない時にはコストがかからず（スケール・トゥ・ゼロ）、急増時には自動的にスケールする**Cloud Functions**のようなサーバーレスコンピューティングが最も費用対効果が高いです。
  * **データストア**: **Firestore**は、サーバーレスアプリケーションとの親和性が高く、スケーラブルなNoSQLデータベースです。
    この組み合わせは、完全にサーバーレスであり、運用オーバーヘッドが最小限で、使用量に応じた課金体系のため、トラフィックが変動するシナリオで最も費用対効果が高くなります。

-----

### <a name="no43"></a>**NO.43**

コスト削減のため、エンジニアリング部長はすべての開発者に、開発インフラリソースをオンプレミスの仮想マシン（VM）からGoogle Cloud Platformに移動するよう要求しました。これらのリソースは日中に複数回の起動/停止イベントを経て、状態を永続化させる必要があります。あなたは、財務部門にコストの可視性を提供しながら、Google Cloudで開発環境を実行するプロセスを設計するように依頼されました。どの2つのステップを実行すべきですか？ （2つ選択）

A. すべての永続ディスクに`--no-auto-delete`フラグを使用し、VMを停止する。
B. すべての永続ディスクに`-auto-delete`フラグを使用し、VMを終了する。
C. VMのCPU使用率ラベルを適用し、BigQueryの課金エクスポートに含める。
D. Google BigQueryの課金エクスポートとラベルを使用して、コストをグループに関連付ける。
E. すべての状態をローカルSSDに保存し、永続ディスクをスナップショットして、VMを終了する。
F. すべての状態をGoogle Cloud Storageに保存し、永続ディスクをスナップショットして、VMを終了する。

**正解: A, D**

**解説:**

  * **A (状態の永続化とコスト削減)**: 開発者は作業を中断・再開するため、VMを**停止（stop）**するだけで、**終了（terminate）**はしません。停止したVMはCPUとメモリの料金はかかりませんが、ディスクの料金は発生し続けます。状態を保持した永続ディスクをVMの停止後も残すには、ディスクが自動で削除されないように`--no-auto-delete`フラグを設定しておく必要があります。
  * [cite_start]**D (コストの可視性)**: 財務部門がコストを把握するためには、リソースに**ラベル**を付け（例: `env: dev`, `team: alpha`）、**課金データをBigQueryにエクスポート**するのがベストプラクティスです [cite: 886]。BigQueryにエクスポートされた詳細な課金データに対して、ラベルを使ってクエリを実行することで、チームやプロジェクトごとのコストを正確に分析・可視化できます。

-----

### <a name="no44"></a>**NO.44**

あなたは、GCPを使用してリモートリカバリのためのディザスタリカバリの回復力を検証する手順を開発する必要があります。あなたの本番環境はオンプレミスでホストされています。オンプレミスネットワークとGCPネットワーク間に安全で冗長な接続を確立する必要があります。どうすべきですか？

A. Dedicated InterconnectがGCPにファイルを複製できることを確認する。Dedicated Interconnectが失敗した場合に、ダイレクトピアリングがネットワーク間に安全な接続を確立できることを確認する。
B. Dedicated InterconnectがGCPにファイルを複製できることを確認する。Dedicated Interconnectが失敗した場合に、Cloud VPNがネットワーク間に安全な接続を確立できることを確認する。
C. Transfer ApplianceがGCPにファイルを複製できることを確認する。Transfer Applianceが失敗した場合に、ダイレクトピアリングがネットワーク間に安全な接続を確立できることを確認する。
D. Transfer ApplianceがGCPにファイルを複製できることを確認する。Transfer Applianceが失敗した場合に、Cloud VPNがネットワーク間に安全な接続を確立できることを確認する。

**正解: B**

**解説:**
[cite_start]**Dedicated Interconnect**は、オンプレミスとGCP間の高帯域幅でプライベートな物理接続を提供し、本番環境の主要な接続として適しています。ディザスタリカバリ計画には、このプライマリ接続が失敗した場合のバックアップパスが必要です。**Cloud VPN**は、パブリックインターネットを介したIPsec VPN接続を提供し、Dedicated Interconnectの冗長（バックアップ）接続として構成するのがGoogleの推奨する高可用性パターンです [cite: 893]。これにより、Interconnectに障害が発生しても、VPN経由で安全な接続が維持されます。

-----

### <a name="no45"></a>**NO.45**

あなたはGitソースリポジトリに保存されているプロジェクトの継続的デプロイメントパイプラインを構築しており、本番環境にデプロイする前にコードの変更が検証されることを確実にしたいです。どうすべきですか？

A. Spinnakerを使用して、レッド/ブラック（Blue/Green）デプロイメント戦略でビルドを本番環境にデプロイし、変更を簡単にロールバックできるようにする。
B. Spinnakerを使用して、ビルドを本番環境にデプロイし、本番デプロイメントでテストを実行する。
C. Jenkinsを使用してステージングブランチとマスターブランチをビルドする。完全なロールアウトの前に、10%のユーザーに対して変更をビルドし本番環境にデプロイする。
D. Jenkinsを使用してリポジトリのタグを監視する。ステージングタグをテスト用のステージング環境にデプロイする。テスト後、リポジトリに本番用のタグを付け、それを本番環境にデプロイする。

**正解: D**

**解説:**
これは、Gitのタグを活用した典型的な**GitFlow**またはそれに類するブランチング戦略に基づくCI/CDパイプラインです。

1.  **Jenkinsがタグを監視**: `staging-v1.2`のようなタグがプッシュされると、Jenkinsがそれを検知します。
2.  **ステージング環境へデプロイ**: Jenkinsは、そのタグのコードをビルドし、自動的に**ステージング環境**にデプロイします。
3.  **検証**: ステージング環境でQAチームなどがテストを行い、デプロイが安全であることを**検証**します。
4.  **本番用タグ付けとデプロイ**: テストに合格したら、同じコードコミットに対して`production-v1.2`のような本番用のタグを付けます。Jenkinsはこの新しいタグを検知し、本番環境へのデプロイをトリガーします。
    [cite_start]このプロセスにより、「本番デプロイ前にコードの変更が検証される」という要件が確実に満たされます [cite: 911]。

-----

### <a name="no46"></a>**NO.46**

あなたはGoogle Cloud上でWebサービスのインフラを実装しています。Webサービスは毎秒50万リクエストのデータを受信・保存する必要があります。データは後で、既知の属性セットの完全一致に基づいてリアルタイムでクエリされます。Webサービスがリクエストを全く受信しない期間もあります。ビジネスはコストを低く抑えたいと考えています。アプリケーションにはどのWebサービスプラットフォームとデータベースを使用すべきですか？

A. Cloud RunとBigQuery
B. Cloud RunとCloud Bigtable
C. Compute Engineの自動スケーリングマネージドインスタンスグループとBigQuery
D. Compute Engineの自動スケーリングマネージドインスタンスグループとCloud Bigtable

**正解: B**

**解説:**

  * **Webサービスプラットフォーム**: リクエストがない期間がある（トラフィックがゼロになる可能性がある）ため、リクエストがない時にはコストがかからない**Cloud Run**のようなサーバーレスプラットフォームが最もコスト効率が良いです。Compute Engine（C, D）はインスタンスが1台でも起動していればコストが発生します。
  * [cite_start]**データベース**: 「毎秒50万リクエスト」という非常に高い書き込みスループットと、「完全一致に基づくリアルタイムクエリ」という要件は、低レイテンシでの大規模な読み書きを得意とするNoSQLデータベースである**Cloud Bigtable**に最適です [cite: 924]。BigQuery（A, C）は分析クエリには強いですが、このような高スループットのリアルタイムなキーベースの読み書きには設計されていません。

-----

### <a name="no47"></a>**NO.47**

あなたは、実行中のGoogle Kubernetes Engineクラスタが、アプリケーションの需要の変化に応じてスケーリングできるようにしたいです。どうすべきですか？

A. `gcloud container clusters resize`コマンドを使用して、Kubernetes Engineクラスタにノードを追加する。
B. `gcloud compute instances add-tags`コマンドを使用して、クラスタ内のインスタンスにタグを追加する。
C. `gcloud alpha container clusters update mycluster --enable-autoscaling --min-nodes=1 --max-nodes=10`コマンドを使用して、既存のKubernetes Engineクラスタを更新する。
D. `gcloud alpha container clusters create mycluster --enable-autoscaling --min-nodes=1 --max-nodes=10`コマンドを使用して新しいKubernetes Engineクラスタを作成し、アプリケーションを再デプロイする。

**正解: C**

**解説:**
[cite_start]**既存の**Kubernetes Engineクラスタに対してオートスケーリングを有効にするには、`gcloud container clusters update`コマンドを使用します [cite: 933]。`--enable-autoscaling`フラグと、ノード数の最小値（`--min-nodes`）および最大値（`--max-nodes`）を指定することで、**クラスタオートスケーラー**が有効になります。これにより、Podのスケジューリング要求に応じて、クラスタ内のノード（VMインスタンス）の数が自動的に増減します。`resize` (A)は手動でのサイズ変更であり、自動ではありません。

-----

### <a name="no48"></a>**NO.48**

あなたの会社は、低リスクでクラウドを試してみたいと考えています。約100TBのログデータをクラウドにアーカイブし、そこで利用可能な分析機能をテストすると同時に、そのデータを長期的な災害復旧バックアップとしても保持したいと考えています。どの2つのステップを実行すべきですか？（2つ選択）

A. ログをGoogle BigQueryにロードする。
B. ログをGoogle Cloud SQLにロードする。
C. ログをGoogle Stackdriverにインポートする。
D. ログをGoogle Cloud Bigtableに挿入する。
E. ログファイルをGoogle Cloud Storageにアップロードする。

**正解: A, E**

**解説:**
これは「安価なストレージ」と「SQLライクな分析」という2つの要件を満たす組み合わせです。

  * **E (Google Cloud Storage)**: 100TBものデータを安価に**アーカイブ**し、**長期的なバックアップ**として保持するための最適な場所です。特にColdlineやArchiveストレージクラスは非常に低コストです。
  * **A (Google BigQuery)**: ビジネスアナリストがSQLインターフェースを使用して**分析**を行うためのサーバーレスデータウェアハウスです。Cloud Storageに保存したログファイルを外部テーブルとして参照したり、直接BigQueryにロードしたりすることで、大規模なログデータに対して強力な分析クエリを実行できます。

-----

### <a name="no49"></a>**NO.49**

あなたの会社は最近、Google Cloudにインフラを持つ会社を買収しました。各社は独自のGoogle Cloud組織を持っています。各社は、アプリケーションにネットワーク接続を提供するために共有Virtual Private Cloud（VPC）を使用しています。両社が使用するサブネットの一部が重複しています。両ビジネスが統合するために、アプリケーションはプライベートネットワーク接続を持つ必要があります。これらのアプリケーションは重複するサブネット上にはありません。最小限の再設計で接続を提供したいです。どうすべきですか？

A. VPCピアリングを設定し、各共有VPCをピアリングする。
B. 各アプリケーションでSSHポートフォワーディングを構成して、異なる共有VPCのアプリケーション間の接続を提供する。
C. 買収した会社のプロジェクトをあなたの会社のGoogle Cloud組織に移行する。あなたの会社の共有VPCでインスタンスを再起動する。
D. 各共有VPCにCloud VPNゲートウェイを設定し、Cloud VPNをピアリングする。

**正解: D**

**解説:**
**VPCネットワークピアリング (A)** は、サブネットのIP範囲が重複しているVPC間では使用できません。これはピアリングの基本的な制約です。組織の移行 (C) は「最小限の再設計」という要件に反します。SSHポートフォワーディング (B) は一時的な解決策であり、本格的なアプリケーション間通信には適していません。
一方、**Cloud VPN**は、IPsecトンネルを介してネットワークを接続します。VPNはルーティングレベルで動作するため、IPアドレスの重複があっても、慎重なルート設定やNAT構成（この場合は不要ですが）を組み合わせることで接続が可能です。このシナリオでは、重複していないサブネット間の通信が必要なため、VPNを介してルーティングを設定することが、IP重複の問題を回避しつつ接続を実現する最も現実的で最小限の変更で済む方法です。

-----

### <a name="no50"></a>**NO.50**

あなたのCompute Engineマネージドインスタンスグループで障害が発生し、すべてのインスタンスが5秒後に再起動を繰り返しています。ヘルスチェックは構成されていますが、自動スケーリングは無効になっています。Linuxの専門家である同僚が問題の調査を申し出てくれました。彼がVMにアクセスできるようにする必要があります。どうすべきですか？

A. 同僚にプロジェクト閲覧者のIAMロールを付与する。
B. インスタンスグループに対してローリングリスタートを実行する。
C. インスタンスグループのヘルスチェックを無効にする。彼のSSHキーをプロジェクト全体のSSHキーに追加する。
D. インスタンスグループの自動スケーリングを無効にする。彼のSSHキーをプロジェクト全体のSSHキーに追加する。

**正解: C**

**解説:**
[cite_start]インスタンスが再起動を繰り返している原因は、**ヘルスチェック**が失敗し、インスタンスグループの**自動修復（auto-healing）機能がインスタンスを「不健康」と判断して再作成しているためです [cite: 962]。同僚がSSHでログインして調査するためには、この再作成ループを一時的に止める必要があります。そのためには、まずヘルスチェックを無効**にします。その後、彼がインスタンスにアクセスできるよう、彼の公開SSHキーを**プロジェクト全体のSSHキーメタデータ**に追加します。これで、インスタンスが安定した状態で調査可能になります。


はい、承知いたしました。問題51から100までを、同様に検証・解説付きで出力します。

-----

### <a name="no51"></a>**NO.51**

あなたの会社は最近Cloud Identityを有効にしてユーザーを管理し始めました。Google Cloud組織も設定済みです。セキュリティチームは、組織の一部となるプロジェクトを保護する必要があります。今後、ドメイン外のIAMユーザーが権限を取得するのを禁止したいと考えています。どうすべきですか？

A. 組織ポリシーを構成して、ドメインによってIDを制限する。
B. 組織ポリシーを構成して、サービスアカウントの作成をブロックする。
C. Cloud Schedulerを使用して、Cloud Identityドメインに属さないすべてのユーザーをすべてのプロジェクトから1時間ごとに削除するCloud Functionをトリガーする。

**正解: A**

**解説:**
**組織ポリシーサービス**は、組織全体のリソースに対して制約を一元的に設定するための機能です。`constraints/iam.allowedPolicyMemberDomains` という制約を使用することで、IAMポリシーに追加できるユーザーのGoogle WorkspaceまたはCloud Identityドメインを明示的に許可リスト形式で指定できます。これにより、許可されたドメインに属さないユーザーやサービスアカウントがIAMロールを付与されることを防ぎ、セキュリティを強化できます。これはGoogleが推奨するベストプラクティスです。

-----

### <a name="no52"></a>**NO.52**

あなたは更新が必要なApp Engineアプリケーションを持っています。現在のアプリケーションバージョンを置き換える前に、本番トラフィックで更新をテストしたいです。どうすべきですか？

A. インスタンスグループアップデーターを使用して更新をデプロイし、カナリアテストを可能にする部分的なロールアウトを作成する。
B. App Engineアプリケーションで新しいバージョンとして更新をデプロイし、新しいバージョンと現在のバージョン間でトラフィックを分割する。
C. 新しいVPCに更新をデプロイし、GoogleのグローバルHTTPロードバランシングを使用して、更新アプリケーションと現在のアプリケーション間でトラフィックを分割する。
D. 新しいApp Engineアプリケーションとして更新をデプロイし、GoogleのグローバルHTTPロードバランシングを使用して、新しいアプリケーションと現在のアプリケーション間でトラフィックを分割する。

**正解: B**

**解説:**
**App Engine**には、**トラフィックスプリッティング**という機能が組み込まれています。これにより、新しいバージョンをデプロイした後、本番トラフィックの一部（例: 1%）だけを新しいバージョンにルーティングし、残りの99%は現在の安定バージョンに流し続けることができます。これにより、少量の実際のトラフィックで新しいバージョンの動作を安全にテスト（カナリアテスト）し、問題がなければ徐々にトラフィックの割合を増やしていくことができます。これはApp Engineで安全なリリースを行うための標準的な方法です。

-----

### <a name="no53"></a>**NO.53**

あなたの会社は、オンプレミスのユーザー認証PostgreSQLデータベースのバックアップレプリカをGoogle Cloud Platform上に構築することを決定しました。データベースは4TBで、大規模な更新が頻繁に行われます。レプリケーションにはプライベートアドレス空間での通信が必要です。どのネットワーキングアプローチを使用すべきですか？

A. Google Cloud Dedicated Interconnect
B. データセンターネットワークに接続されたGoogle Cloud VPN
C. オンプレミスにインストールされたNATおよびTLS変換ゲートウェイ
D. データセンターネットワークに接続された、VPNサーバーがインストールされたGoogle Compute Engineインスタンス

**正解: A**

**解説:**
要件は「4TBのデータベース」「頻繁な大規模更新」「プライベートIP通信」です。これは、高帯域幅で安定した低レイテンシの接続を必要とすることを示唆しています。**Dedicated Interconnect**は、オンプレミスのデータセンターとGoogle Cloud間を、パブリックインターネットを経由しない専用の物理回線で接続します。これにより、高スループット、低レイテンシ、そして安定したパフォーマンスが保証され、大規模なデータベースのレプリケーションに最適です。Cloud VPN (B) はインターネット経由のため、パフォーマンスが不安定になる可能性があります。

-----

### <a name="no54"></a>**NO.54**

あなたは、永続層としてCloud Datastoreを使用するApp Engineアプリケーションを作成しています。IDを持っているいくつかのルートエンティティを取得する必要があります。Cloud Datastoreによって実行される操作のオーバーヘッドを最小限に抑えたいです。どうすべきですか？

A. 各エンティティのKeyオブジェクトを作成し、バッチget操作を実行する。
B. 各エンティティのKeyオブジェクトを作成し、エンティティごとに1つのget操作を複数回実行する。
C. IDを使用してクエリフィルタを作成し、バッチクエリ操作を実行する。
D. IDを使用してクエリフィルタを作成し、エンティティごとに1つのクエリ操作を複数回実行する。

**正解: A**

**解説:**
PDFの回答Cは誤りです。Cloud Datastore/Firestoreでは、キーが既にわかっているエンティティを取得する場合、クエリ（フィルタリング）を実行するのは非効率です。**バッチget操作（lookup）**は、複数のキーを一度のリクエストで指定してエンティティを直接取得するための最も効率的な方法です。これにより、ネットワークの往復回数とオペレーションコストを最小限に抑えることができます。複数回のget操作（B）はオーバーヘッドが大きくなります。クエリ（C, D）はインデックスを使用するため、キーによる直接取得よりもコストと時間がかかります。

-----

### <a name="no55"></a>**NO.55**

あなたのアプリケーションはCloud StorageにHTTPリクエストを送信します。時々、リクエストがHTTPステータスコード5xxおよび429で失敗します。これらのタイプのエラーをどのように処理すべきですか？

A. パフォーマンス向上のため、HTTPの代わりにgRPCを使用する。
B. 切り捨て指数バックオフ戦略を使用して再試行ロジックを実装する。
C. 地理的冗長性のためにCloud Storageバケットがマルチリージョンであることを確認する。
D. [https://status.cloud.google.com/feed.atomを監視し](https://www.google.com/search?q=https://status.cloud.google.com/feed.atom%E3%82%92%E7%9B%A3%E8%A6%96%E3%81%97)、Cloud Storageがインシデントを報告していない場合にのみリクエストを送信する。

**正解: B**

**解説:**
PDFの回答Aは不適切です。gRPCはCloud Storageの標準的なインターフェースではありません。

  * **5xx (サーバーエラー)**: Google Cloud側の一次的な問題を示します。
  * **429 (Too Many Requests)**: リクエストレートが高すぎることを示します。
    これらのエラーは**一時的なものである可能性が高い**ため、再試行することで成功する可能性があります。ただし、即座に再試行を繰り返すと、サーバーにさらに負荷をかけ状況を悪化させる可能性があります。そのため、Googleが推奨するベストプラクティスは、**指数バックオフ**（リトライの間隔を指数関数的に長くしていく）を伴う再試行ロジックを実装することです。これにより、システムが回復するための時間を与えつつ、最終的にリクエストを成功させることができます。

-----

### <a name="no56"></a>**NO.56**

あなたは、マネージドインスタンスグループと、OSパッケージの依存関係をインストールする起動スクリプトの作成を自動化したいと考えています。インスタンスグループ内のVMの起動時間を最小限に抑えたいです。どうすべきですか？

A. Terraformを使用してマネージドインスタンスグループを作成し、起動スクリプトでOSパッケージの依存関係をインストールする。
B. すべてのOSパッケージの依存関係が含まれたカスタムVMイメージを作成する。Deployment Managerを使用して、そのVMイメージでマネージドインスタンスグループを作成する。
C. Puppetを使用してマネージドインスタンスグループを作成し、OSパッケージの依存関係をインストールする。
D. Deployment Managerを使用してマネージドインスタンスグループを作成し、Ansibleを使用してOSパッケージの依存関係をインストールする。

**正解: B**

**解説:**
起動スクリプト（A, C, D）は、VMが起動する**たびに**毎回実行されます。パッケージのインストールには時間がかかり、これが起動時間を長くする主な原因となります。起動時間を最小限に抑えるには、必要なパッケージやアプリケーションをすべてインストール済みの**カスタムイメージ**を事前に作成しておくのがベストプラクティスです。インスタンスは、この「準備完了」状態のイメージから起動するため、起動スクリプトで lengthy なインストールプロセスを実行する必要がなくなり、起動時間が劇的に短縮されます。

-----

### <a name="no57"></a>**NO.57**

あなたはGoogle Cloud上で複数のプロジェクトを管理しており、gcloud CLIツールを使用してBigQuery、Bigtable、Kubernetes Engineと日常的にやり取りする必要があります。あなたは頻繁に出張し、週に異なるワークステーションで作業します。gcloud CLIを手動で管理するのを避けたいです。どうすべきですか？

A. パッケージマネージャーを使用して、手動ではなくワークステーションにgcloudをインストールする。
B. Compute Engineインスタンスを作成し、そのインスタンスにgcloudをインストールする。SSH経由でこのインスタンスに接続して、常に同じgcloudインストールを使用する。
C. すべてのワークステーションにgcloudをインストールする。各ワークステーションで`gcloud components auto-update`コマンドを実行する。
D. Google Cloud ConsoleのGoogle Cloud Shellを使用してGoogle Cloudとやり取りする。

**正解: D**

**解説:**
PDFの回答Cは実用的ではありません。**Google Cloud Shell**は、ブラウザからアクセスできるコマンドライン環境で、gcloud CLIやその他多くの開発ツールがプリインストールされ、常に最新の状態に保たれています。どのワークステーションからでもブラウザさえあれば、同じ認証情報と設定で一貫した環境に即座にアクセスできるため、ローカルマシンへのインストールや管理、更新の手間が一切不要になります。これが要件に最も合致する解決策です。

-----

### <a name="no58"></a>**NO.58**

この問題については、Dress4Winのケーススタディを参照してください。

Dress4Winは、オンプレミスのMySQLデプロイメントをクラウドに移行する方法についてあなたにアドバイスを求めました。彼らは移行中のオンプレミスソリューションへのダウンタイムとパフォーマンスへの影響を最小限に抑えたいと考えています。どのアプローチを推奨すべきですか？

A. オンプレミスのMySQLマスターサーバーのダンプを作成し、それをシャットダウンし、クラウド環境にアップロードして新しいMySQLクラスタにロードする。
B. クラウド環境にMySQLレプリカサーバー/スレーブを設定し、カットオーバーまでオンプレミスのMySQLマスターサーバーからの非同期レプリケーション用に構成する。
C. クラウドに新しいMySQLクラスタを作成し、アプリケーションをオンプレミスとクラウドの両方のMySQLマスターに書き込むように構成し、カットオーバー時に元のクラスタを破棄する。
D. MySQLレプリカサーバーのダンプをクラウド環境に作成し、それをGoogle Cloud Datastoreにロードし、カットオーバー時にアプリケーションをCloud Datastoreに読み書きするように構成する。

**正解: B**

**解説:**
ダウンタイムを最小限に抑えてデータベースを移行するための標準的な手法は、**レプリケーション**を利用することです。まず、クラウド上（この場合はCloud SQLなど）にレプリカを作成し、オンプレミスの本番データベース（マスター）と同期させます。データが完全に同期された後、アプリケーションの接続先をオンプレミスからクラウドの新しいマスターに切り替えます。この切り替え（カットオーバー）にかかる時間は非常に短く、ダウンタイムを数分、あるいは数秒に抑えることができます。ダンプとリストア（A）では、その間サービスを停止する必要があるため、ダウンタイムが長くなります。

-----

### <a name="no59"></a>**NO.59**

あなたは、総メモリ使用率が80%を超えたときに自動スケーリングするCompute Engineアプリケーションを持っています。Cloud Opsエージェントをインストールし、自動スケーリングポリシーを次のように構成しました：

メトリック識別子: `agent.googleapis.com/memory/percent_used`
フィルタ: `metric.label.state = 'used' AND metric.label.state = 'buffered' AND metric.label.state = 'cached' AND metric.label.state = 'slab'`
ターゲット使用率レベル: 80
ターゲットタイプ: GAUGE

アプリケーションが高負荷下でスケールしないことに気づきました。これを解決したいです。どうすべきですか？

A. メトリック識別子を `agent.googleapis.com/memory/bytes_used` に変更する。
B. フィルタを `metric.label.state = 'used'` に変更する。
C. フィルタを `metric.label.state = 'free'` に変更し、ターゲット使用率レベルを20に変更する。
D. ターゲットタイプを `DELTA_PER_MINUTE` に変更する。

**正解: B**

**解説:**
PDFの回答Cは論理的に正しいですが、Bの方がより直接的です。元のフィルタは `state = 'used' AND state = 'buffered' ...` となっており、`state`ラベルが同時に複数の値を持つことはないため、この条件は決して真になりません。そのため、メトリクスが収集されず、オートスケーラーがトリガーされません。Linuxのメモリ使用率を正しく評価するには、バッファやキャッシュを除いた、純粋にアプリケーションによって**使用されている（used）**メモリを監視するのが一般的です。したがって、フィルタを `metric.label.state = 'used'` に単純化することが正しい修正方法です。

-----

### <a name="no60"></a>**NO.60**

あなたの会社は、Linux RHEL 6.5+の仮想マシンをリフト＆シフト移行する計画です。仮想マシンはオンプレミスのVMware環境で実行されています。Google推奨のプラクティスに従って、それらをCompute Engineに移行したいです。どうすべきですか？

A. アプリケーションのリストとその依存関係に基づいて移行計画を定義する。Migrate for Compute Engineを使用して、すべての仮想マシンを個別にCompute Engineに移行する。
B. 現在のVMware環境で実行されている仮想マシンの評価を行う。すべてのディスクのイメージを作成する。Compute Engineにディスクをインポートする。インポートしたディスクをブートディスクとする標準の仮想マシンを作成する。
C. 現在のVMware環境で実行されている仮想マシンの評価を行う。移行計画を定義し、Migrate for Compute Engineの移行RunBookを準備し、移行を実行する。
D. 現在のVMware環境で実行されている仮想マシンの評価を行う。選択されたすべての仮想マシンにサードパーティのエージェントをインストールする。

**正解: C**

**解説:**
Googleが推奨するVMwareからの大規模な移行プロセスは、**Migrate for Compute Engine**（旧Velostrata）を使用する体系的なアプローチです。このプロセスは以下のフェーズで構成されます。

1.  **評価(Assess)**: オンプレミス環境を評価し、移行対象を特定します。
2.  **計画(Plan)**: 移行の波（Wave）を計画し、具体的な手順を**RunBook**にまとめます。
3.  **展開(Deploy)**: RunBookに従い、Migrate for Compute Engineを使用して、ダウンタイムを最小限に抑えながらVMを移行します（バックグラウンドでのデータ同期など）。
    この計画的でツール主導のアプローチが、Googleの推奨プラクティスです。

-----

### <a name="no61"></a>**NO.61**

あなたの開発チームは、夜間バッチプロセスを高速化するために、Google Compute Engine（GCE）仮想マシン（VM）のバッチサーバーに新しいLinuxカーネルモジュールをインストールしました。インストールの2日後、同じ夜間バッチ実行でデプロイされたWebアプリケーションの50%が失敗しました。開発チームにフィードバックするために、障害の詳細を収集したいです。どの3つのアクションを実行すべきですか？ （3つ選択）

A. Stackdriver Loggingを使用して、モジュールのログエントリを検索する。
B. APIまたはCloud Consoleを使用して、デバッグGCEアクティビティログを読み取る。
C. gcloudまたはCloud Consoleを使用して、シリアルコンソールに接続し、ログを観察する。
D. アクティビティログを使用して、失敗したサーバーのライブマイグレーションイベントが発生したかどうかを特定する。
E. Google Stackdriverのタイムラインを障害時間に合わせ、バッチサーバーのメトリクスを観察する。
F. デバッグVMをイメージにエクスポートし、カーネルログメッセージがネイティブ画面に表示されるローカルサーバーでイメージを実行する。

**正解: A, C, E**

**解説:**
カーネルモジュールのインストール後に障害が発生したため、OSレベルとアプリケーションレベルの両方から調査する必要があります。

  * **A (Stackdriver Logging)**: アプリケーションがログを出力している場合、エラーメッセージを特定するのに役立ちます。
  * **C (シリアルコンソール)**: VMが正常に起動しない場合や、カーネルパニックのような低レベルのエラーが発生している場合、そのログを確認するのに不可欠です。ネットワークに接続できなくてもアクセスできます。
  * **E (Stackdriver Monitoring)**: 障害発生時刻に合わせてCPU、メモリ、ディスクI/Oなどのメトリクスを確認することで、リソースの枯渇や異常なスパイクなど、パフォーマンス上の問題を示す兆候を発見できます。

-----

### <a name="no62"></a>**NO.62**

この問題については、Dress4Winのケーススタディを参照してください。

Dress4Winは、エンドポイントの100%をカバーするエンドツーエンドテストを持っています。彼らは、クラウドへの移行が新たなバグを導入しないことを確実にしたいと考えています。停止を防ぐために、開発者はどの追加のテスト方法を採用すべきですか？

A. コード内のエラーを表示するために、アプリケーションコードでGoogle Stackdriver Debuggerを有効にする。
B. クラウドのステージング環境で、追加のユニットテストと本番規模の負荷テストを追加する。
C. コードが意図した通りに動作しているか判断するために、クラウドのステージング環境でエンドツーエンドテストを実行する。
D. 新しいリリースがレイテンシにどれだけの影響を与えるかを開発者が測定できるように、カナリアテストを追加する。

**正解: B**

**解説:**
(注：この問題はNo.37とほぼ同じですが、選択肢の順序が異なります。)
クラウド移行は、基盤となるインフラが大きく変わるため、既存のエンドツーエンドテストだけでは不十分です。

  * **追加のユニットテスト**: クラウド環境特有の動作（例: SDKの挙動、レイテンシの違い）を考慮したテストケースを追加する必要があります。
  * **本番規模の負荷テスト**: クラウド環境のスケーラビリティやパフォーマンスの特性はオンプレミスと全く異なります。オートスケーリングが正しく機能するか、データベースのパフォーマンスが十分かなどを確認するために、本番と同等規模の負荷をかけたテストが不可欠です。これにより、オンプレミスでは現れなかったパフォーマンス関連のバグを発見できます。

-----

### <a name="no63"></a>**NO.63**

あなたは、正確なリアルタイムの気象チャートアプリケーションのパフォーマンスを最適化したいと考えています。データは、タイムスタンプとセンサー読み取りの形式で、毎秒10回読み取りを送信する50,000個のセンサーから送られてきます。データをどこに保存すべきですか？

A. Google BigQuery
B. Google Cloud SQL
C. Google Cloud Bigtable
D. Google Cloud Storage

**正解: C**

**解説:**
このユースケースは**時系列（time-series）データ**の典型例です。要件は以下の通りです。

  * **高い書き込みスループット**: 50,000センサー * 10回/秒 = 500,000書き込み/秒
  * **リアルタイムチャート**: 低レイテンシでの読み取り
    **Cloud Bigtable**は、このような大規模な時系列データを扱うために設計された、低レイテンシで高スループットなNoSQLワイドカラムストアです。IoTセンサーデータやモニタリングデータの保存先として最適です。BigQuery(A)は分析には強いですが、このレベルの書き込みスループットとリアルタイム読み取りには対応できません。

-----

### <a name="no64"></a>**NO.64**

あなたの会社のテストスイートは、Linux仮想マシン上で毎日テストを実行するカスタムC++アプリケーションです。完全なテストスイートの完了には数時間かかり、テスト用に予約された限られた数のオンプレミスサーバーで実行されています。あなたの会社は、テストにかかる時間を短縮し、テストを可能な限り変更せずに、テストインフラをクラウドに移行したいと考えています。どのクラウドインフラを推奨すべきですか？

A. Google Compute Engineの非マネージドインスタンスグループとネットワークロードバランサ
B. 自動スケーリングを備えたGoogle Compute Engineのマネージドインスタンスグループ
C. 各テストを処理するためのApache Hadoopジョブを実行するGoogle Cloud Dataproc
D. ロギング用のGoogle Stackdriverを備えたGoogle App Engine

**正解: B**

**解説:**
テスト時間を短縮する最も簡単な方法は、テストを**並列実行**することです。オンプレミスではサーバー数が限られていましたが、クラウドでは必要に応じて多数のVMを起動できます。**自動スケーリングを備えたマネージドインスタンスグループ（MIG）**を使用すると、テストジョブのキューに応じてVMの数を自動的に増やし、テストが完了したら自動的に減らすことができます。これにより、多数のテストを同時に実行して全体の時間を短縮しつつ、VMがアイドル状態のときはコストを最小限に抑えることができます。

-----

### <a name="no65"></a>**NO.65**

あなたのチームは、Kubernetes Engine上でマイクロサービスアーキテクチャを使用して新しいアプリケーションの開発を開始します。開発ライフサイクルの一環として、GitHubリポジトリのリモートdevelopブランチにプッシュされたすべてのコード変更は、自動的にビルドおよびテストされる必要があります。ビルドとテストが成功すると、関連するマイクロサービスは開発環境に自動的にデプロイされます。開発環境にデプロイされたすべてのコードがこのプロセスに従うことを保証したいです。どうすべきですか？

A. 各開発者に、開発ブランチへのコミット時にコードをテストしコンテナをビルドするpre-commitフックをワークステーションにインストールさせる。成功したコミット後、開発者に新しくビルドされたコンテナイメージを開発クラスタにデプロイさせる。
B. リモートgitリポジトリに、コードが開発ブランチにプッシュされたときにコードをテストしコンテナをビルドするpost-commitフックをインストールする。成功したコミット後、開発者に新しくビルドされたコンテナイメージを開発クラスタにデプロイさせる。
C. developブランチに基づいて、コードをテストし、コンテナをビルドし、Container Registryに保存するCloud Buildトリガーを作成する。新しいイメージを監視し、新しいイメージを開発クラスタにデプロイするデプロイメントパイプラインを作成する。デプロイメントツールのみが新しいバージョンをデプロイするアクセス権を持つことを保証する。
D. developブランチに基づいて、新しいコンテナイメージをビルドしContainer Registryに保存するCloud Buildトリガーを作成する。コードテストが成功することを保証するために脆弱性スキャンに依存する。Cloud Buildプロセスの最終ステップとして、新しいコンテナイメージを開発クラスタにデプロイする。Cloud Buildのみが新しいバージョンをデプロイするアクセス権を持つことを保証する。

**正解: C**

**解説:**
これは、GitOpsの原則に従った、安全で自動化されたCI/CDパイプラインの構築方法です。

  * **Cloud Buildトリガー**: GitHubの`develop`ブランチへのプッシュをトリガーとして、自動的にテストとコンテナビルドを実行します。これにより、すべてのコードが中央で一貫して処理されることが保証されます。
  * **デプロイメントパイプライン**: ビルドされた新しいイメージを監視する別のツール（Spinnaker, ArgoCDなど、あるいはCloud Buildのデプロイステップ）が、開発環境へのデプロイを自動的に行います。
  * **権限の分離**: 開発者が直接クラスタにデプロイするのではなく、CI/CDシステム（デプロイメントツール）のサービスアカウントのみにデプロイ権限を与えることで、プロセスが強制され、手動での誤ったデプロイを防ぎます。

-----

### <a name="no66"></a>**NO.66**

あなたの会社は、予定された会議のために予約された会議室に誰かがいるかどうかを追跡したいと考えています。3大陸の5つのオフィスに1000の会議室があります。各部屋には、毎秒ステータスを報告するモーションセンサーが装備されています。モーションディテクターからのデータには、センサーIDといくつかの異なる離散的な情報項目のみが含まれます。アナリストは、このデータをアカウント所有者やオフィスの場所に関する情報とともに使用します。どのデータベースタイプを使用すべきですか？

A. フラットファイル
B. NoSQL
C. リレーショナル
D. Blobstore

**正解: B**

**解説:**
このユースケースはNoSQLデータベースに非常に適しています。

  * **大量の書き込み**: 1000室 × 1回/秒 = 1000書き込み/秒。これはリレーショナルデータベース(C)にとっては高い負荷です。
  * **シンプルなデータ構造**: データは「センサーID」といくつかの項目のみで、複雑なリレーションは不要です。これはスキーマレスまたは柔軟なスキーマを持つNoSQLの得意分野です。
  * **スケーラビリティ**: グローバルに分散したセンサーからのデータを扱うには、水平方向にスケールしやすいNoSQLデータベース（例: Cloud Bigtable, Firestore）が適しています。

-----

### <a name="no67"></a>**NO.67**

あなたはオンプレミスソリューションをいくつかのフェーズでGoogle Cloudに移行しています。移行が完了するまで、オンプレミスシステムとGoogle Cloud間の接続を維持するためにCloud VPNを使用します。この期間中、すべてのオンプレミスシステムが到達可能であり続けることを確認したいです。Google Cloudでのネットワーキングはどのように整理すべきですか？

A. オンプレミスで使用しているのと同じIP範囲をGoogle Cloudで使用する。
B. プライマリIP範囲にはオンプレミスと同じIP範囲を使用し、セカンダリ範囲にはオンプレミスで使用している範囲と重複しない範囲を使用する。
C. オンプレミスで使用している範囲と重複しないIP範囲をGoogle Cloudで使用する。
D. プライマリIP範囲にはオンプレミスで使用している範囲と重複しないIP範囲を使用し、セカンダリ範囲にはオンプレミスと同じIP範囲を使用する。

**正解: C**

**解説:**
オンプレミスネットワークとGCPのVPCをVPNやInterconnectで接続する場合、**IPアドレス範囲が重複していてはなりません**。もしIP範囲が重複していると、ルーターがどちらのネットワークにパケットを送信すればよいか判断できず、深刻なルーティングの問題が発生します。したがって、ハイブリッド接続を計画する際の最も基本的な原則は、両方のネットワークで互いに重複しない、一意のIPアドレス範囲を使用することです。

-----

### <a name="no68"></a>**NO.68**

あなたの会社は、Google Kubernetes Engine（GKE）クラスタ内のDeploymentとして実行されているアプリケーションを持っています。ローリングデプロイメントを介してアプリケーションの新しいバージョンをリリースする際に、チームが停止を引き起こしています。停止の根本原因は、本番環境でのみ使用されるパラメータの誤設定です。停止を防ぐためにプラットフォームに予防策を講じたいです。どうすべきですか？

A. Pod仕様にliveness probeとreadiness probeを構成する。
B. Cloud Monitoringにアップタイムアラートを構成する。
C. アプリケーションが利用可能かどうかを確認するためのスケジュールされたタスクを作成する。
D. マネージドインスタンスグループにヘルスチェックを構成する。

**正解: A**

**解説:**
PDFの回答DはGKEではなくGCEの用語であり不適切です。
ローリングアップデート中に停止が発生するのを防ぐために、Kubernetesには**liveness probe**と**readiness probe**という仕組みがあります。

  * **Readiness Probe**: コンテナがリクエストを受け付ける準備ができているかを確認します。準備ができていない（例: 初期化中、設定ミスで起動失敗）場合、Kubernetes ServiceはそのPodをロードバランサから一時的に切り離し、トラフィックが送られないようにします。これにより、準備ができていないPodにトラフィックが送られてエラーになるのを防ぎます。
  * **Liveness Probe**: コンテナが正常に動作しているか（デッドロックしていないかなど）を確認します。失敗した場合、kubeletはコンテナを再起動します。
    このシナリオでは、特にReadiness Probeが、誤設定された新しいバージョンのPodにトラフィックが流れるのを防ぎ、停止を回避するのに役立ちます。

-----

### <a name="no69"></a>**NO.69**

あなたのマイクロサービスベースのアプリケーションへのAPIリクエストのごく一部が非常に長い時間を要します。APIへの各リクエストが多くのサービスを通過することを知っています。それらのケースでどのサービスが最も時間がかかっているかを知りたいです。どうすべきですか？

A. アプリケーションにタイムアウトを設定して、リクエストをより速く失敗させる。
B. 各リクエストのカスタムメトリクスをStackdriver Monitoringに送信する。
C. Stackdriver Monitoringを使用して、APIのレイテンシが高いことを示すインサイトを探す。
D. 各マイクロサービスでのリクエストレイテンシを分解するために、Stackdriver Traceでアプリケーションを計装する。

**正解: D**

**解説:**
**Cloud Trace (旧Stackdriver Trace)** は、分散トレーシングシステムです。アプリケーションをTraceで計装すると、単一のリクエストが複数のマイクロサービスを横断する際の処理の流れと、各サービス（およびその内部の関数呼び出し）で費やされた時間を可視化できます。これにより、「どのサービスがボトルネックになっているか」を正確に特定でき、レイテンシ問題の根本原因を突き止めるのに非常に強力なツールとなります。

-----

### <a name="no70"></a>**NO.70**

あなたは、Compute Engine VMからのパブリックインターネットアクセスが許可されていない、高度にセキュアな環境で作業しています。オンプレミスのファイルサーバーにアクセスするためのVPN接続はまだありません。特定のソフトウェアをCompute Engineインスタンスにインストールする必要があります。どのようにソフトウェアをインストールすべきですか？

A. 必要なインストールファイルをCloud Storageにアップロードする。プライベートGoogleアクセスサブネットが構成されたサブネット上のVMを構成する。VMには内部IPアドレスのみを割り当てる。gsutilを使用してVMにインストールファイルをダウンロードする。
B. 必要なインストールファイルをCloud Storageにアップロードし、Cloud StorageのIPアドレス範囲を除くすべてのトラフィックをブロックするファイアウォールルールを使用する。gsutilを使用してVMにファイルをダウンロードする。
C. 必要なインストールファイルをCloud Source Repositoriesにアップロードする。プライベートGoogleアクセスサブネットが構成されたサブネット上のVMを構成する。VMには内部IPアドレスのみを割り当てる。gcloudを使用してVMにインストールファイルをダウンロードする。
D. 必要なインストールファイルをCloud Source Repositoriesにアップロードし、Cloud Source RepositoriesのIPアドレス範囲を除くすべてのトラフィックをブロックするファイアウォールルールを使用する。gsutilを使用してVMにファイルをダウンロードする。

**正解: A**

**解説:**
要件は「外部IPなしのVM」から「GCPサービス」にアクセスすることです。このための機能が**限定公開のGoogleアクセス (Private Google Access)** です。

1.  **Cloud Storage**にインストールファイルをアップロードします。
2.  VMが属するサブネットで**限定公開のGoogleアクセスを有効**にします。
3.  これにより、VMは外部IPアドレスを持たなくても、内部IPアドレスを使用してGoogleの内部ネットワーク経由でCloud StorageなどのGoogle APIにアクセスできるようになります。
4.  VMから`gsutil`コマンドでCloud Storageからファイルをダウンロードできます。

-----

### <a name="no71"></a>**NO.71**

あなたの会社はクラウドへの移行を成功させ、業務を最適化するためにデータストリームを分析したいと考えています。この分析のための既存のコードはなく、すべての選択肢を検討しています。これには、時間ごとのジョブを実行し、一部のデータをリアルタイムで処理するため、バッチ処理とストリーム処理の混合が含まれます。このためにどのテクノロジーを使用すべきですか？

A. Google Cloud Dataproc
B. Google Cloud Dataflow
C. Bigtableを備えたGoogle Container Engine
D. Google BigQueryを備えたGoogle Compute Engine

**正解: B**

**解説:**
**Cloud Dataflow**の最大の特徴は、**単一のプログラミングモデル（Apache Beam）でバッチ処理とストリーム処理の両方を記述できる**ことです。これは「統合データ処理モデル」と呼ばれます。要件には「バッチとストリーム処理の混合」と明記されており、Dataflowはこの要件に完全に一致するサービスです。Dataproc (A) は主に既存のHadoop/Sparkのバッチジョブを移行するのに使われます。

-----

### <a name="no72"></a>**NO.72**

あなたは、ビジネスクリティカルなトランザクションデータを含む単一のCloud SQL MySQL第2世代データベースを実装しています。壊滅的な障害が発生した場合に失われるデータ量を最小限に抑えたいです。どの2つの機能を実装すべきですか？（2つ選択）

A. シャーディング
B. リードレプリカ
C. バイナリロギング
D. 自動バックアップ
E. セミ同期レプリケーション

**正解: C, D**

**解説:**
データ損失を最小限に抑えるには、**ポイントインタイムリカバリ（PITR）**を可能にすることが重要です。PITRを実現するためには、以下の2つが必要です。

  * **D (自動バックアップ)**: 定期的にデータベース全体のバックアップを取得します。これにより、特定の時点の状態に復元できます。
  * **C (バイナリロギング)**: バックアップ取得後に行われたすべてのデータ変更トランザクションを記録したログです。
    この2つを組み合わせることで、「最後のバックアップ時点までデータを復元し、その後、障害発生直前までのトランザクションをバイナリログから再生する」ことが可能になり、データ損失を最小限（数秒〜数分レベル）に抑えることができます。

-----

### <a name="no73"></a>**NO.73**

あなたの会社のアプリケーションワークロードはCompute Engineで実行されています。アプリケーションは本番、驗收、開発の各環境にデプロイされています。本番環境はビジネスクリティカルで24時間365日使用されますが、驗收および開発環境は営業時間中にのみクリティカルです。CFOから、アイドル時間中のコスト削減を達成するためにこれらの環境を最適化するよう依頼されました。どうすべきですか？

A. 開発および驗收インスタンスのマシンタイプを営業時間外に小さなマシンタイプに変更するgcloudコマンドを使用するシェルスクリプトを作成する。本番インスタンスの1つでシェルスクリプトをスケジュールしてタスクを自動化する。
B. Cloud Schedulerを使用して、営業時間後に開発および驗收環境を停止し、営業時間の直前に開始するCloud Functionをトリガーする。
C. 開発および驗收アプリケーションをマネージドインスタンスグループにデプロイし、自動スケーリングを有効にする。
D. 本番環境には通常のCompute Engineインスタンスを使用し、驗收および開発環境にはプリエンプティブルVMを使用する。

**正解: B**

**解説:**
開発・驗收環境は「営業時間中にのみ」使用されるため、夜間や週末は完全に停止させることが最も効果的なコスト削減策です。この「起動と停止のスケジューリング」を自動化するのに最適な組み合わせが**Cloud Scheduler**と**Cloud Functions**です。

  * **Cloud Scheduler**: cronのように、指定したスケジュールでジョブをトリガーします（例: 平日の朝9時と夕方18時）。
  * **Cloud Functions**: Schedulerによってトリガーされ、Compute Engine APIを呼び出して対象のVMインスタンスを起動または停止する短いコードを実行します。
    この方法はサーバーレスで運用負荷が低く、要件を正確に満たします。

-----

### <a name="no74"></a>**NO.74**

あなたの組織は、異なる部門のIAMポリシーを独立して、しかし中央で管理したいと考えています。どのアプローチを取るべきですか？

A. 複数のフォルダを持つ複数の組織
B. 各部門ごとに1つの複数の組織
C. 各部門ごとにフォルダを持つ単一の組織
D. それぞれに中央の所有者がいる複数のプロジェクトを持つ単一の組織

**正解: C**

**解説:**
Google Cloudの**リソース階層**は、`組織 > フォルダ > プロジェクト` という構造になっています。

  * **組織**: 会社全体で1つ設定し、すべてを中央管理するためのルートです。
  * **フォルダ**: 組織を部門やチームなどの論理的な単位にグループ化するために使用します。フォルダレベルでIAMポリシーを設定すると、そのフォルダ内のすべてのプロジェクトやサブフォルダにポリシーが継承されます。
    これにより、「部門ごとに独立して（フォルダ単位で）」「中央から（組織管理者が）管理する」という要件を最も効果的に実現できます。

-----

### <a name="no75"></a>**NO.75**

あなたは8歳から30歳までのメンバーがいるスポーツ協会で働いています。協会は、負傷などの大量の健康データを収集しています。このデータをBigQueryに保存しています。現在の法律では、対象者の要求に応じてそのような情報を削除する必要があります。そのような要求に対応できるソリューションを設計したいです。どうすべきですか？

A. 各個人に一意の識別子を使用する。削除要求があった場合、BigQueryからこの識別子を持つすべての行を削除する。
B. BigQueryに新しいデータを取り込む際に、Data Loss Prevention（DLP）APIを介してデータを実行し、個人情報を特定する。DLPスキャンの一環として、結果をData Catalogに保存する。削除要求があった場合、Data Catalogをクエリして個人情報を含む列を見つける。
C. すべてのデータを含むテーブルの上にBigQueryビューを作成する。削除要求があった場合、このビューから対象者のデータに影響する行を除外する。すべての分析タスクでソーステーブルの代わりにこのビューを使用する。
D. 各個人に一意の識別子を使用する。削除要求があった場合、一意の識別子を持つ列をその値のソルト付きSHA256で上書きする。

**正解: A**

**解説:**
GDPRなどのプライバシー規制における「削除権（忘れられる権利）」に対応するための最も直接的で確実な方法は、対象のデータを物理的に削除することです。各個人に一意の識別子（例: `user_id`）を割り当てておけば、削除要求があった際に `DELETE FROM table WHERE user_id = '...'` のようなSQL文を実行することで、そのユーザーに関連するすべてのデータをBigQueryから完全に削除できます。ビュー(C)や上書き(D)ではデータが物理的に残ってしまうため、規制要件を完全に満たさない可能性があります。

-----

### <a name="no76"></a>**NO.76**

あなたの会社は、3つのVirtual Private Cloud（VPC）を持つGoogle Cloudプロジェクトを持っています。各VPCにCompute Engineインスタンスがあります。ネットワークサブネットは重複しておらず、分離されたままでなければなりません。ネットワーク構成は以下の通りです。

（注：図には、VPC#1, VPC#2, VPC#3がそれぞれ分離しており、各々にInstance #1, #2, #3が存在する様子が示されています。）

Instance #1は例外で、内部IPを介してInstance #2とInstance #3の両方と直接通信する必要があります。これをどのように達成すべきですか？

A. Cloud Routerを作成して、サブネット#2とサブネット#3をサブネット#1にアドバタイズする。
B. Instance #1に以下の構成で2つの追加NICを追加する：
*NIC1: VPC: VPC #2, SUBNETWORK: subnet #2
*NIC2: VPC: VPC #3, SUBNETWORK: subnet #3
インスタンス間のトラフィックを有効にするためにファイアウォールルールを更新する。
C. Cloud VPNを介して2つのVPNトンネルを作成する：VPC #1とVPC #2間、VPC #2とVPC #3間。インスタンス間のトラフィックを有効にするためにファイアウォールルールを更新する。
D. 3つのVPCすべてをピアリングする：VPC #1とVPC #2をピア、VPC #2とVPC #3をピア。インスタンス間のトラフィックを有効にするためにファイアウォールルールを更新する。

**正解: B**

**解説:**
Compute EngineのVMインスタンスは、複数のネットワークインターフェースカード（**NIC**）を持つことができます。そして、**各NICは異なるVPCネットワークに接続する必要があります**。この機能を利用して、Instance #1にデフォルトのNIC（VPC#1に接続）に加えて、VPC#2に接続するNICとVPC#3に接続するNICを追加します。これにより、Instance #1は3つのVPCすべてにまたがるIPアドレスを持つことになり、Instance #2および#3と内部IPで直接通信できるようになります。VPC自体は分離されたままであるという要件も満たします。

-----

### <a name="no77"></a>**NO.77**

あなたは、最適化されたオンプレミスの仮想マシンからサードパーティのアプリケーションをGoogle Cloudに移行しています。最適なCPUとメモリのオプションについては不明です。アプリケーションは数週間にわたって一貫した使用パターンを持っています。最低コストでリソース使用量を最適化したいです。どうすべきですか？

A. アプリケーションの現在のオンプレミス仮想マシンと同様のCPUとメモリのオプションを持つCompute Engineインスタンスを作成する。クラウドモニタリングエージェントをインストールし、サードパーティアプリケーションをデプロイする。サードパーティアプリケーションに通常のトラフィックレベルで負荷をかけ、Cloud Consoleのサイズ適正化の推奨に従う。
B. App Engineフレキシブル環境を作成し、Dockerファイルとカスタムランタイムを使用してサードパーティアプリケーションをデプロイする。app.yamlファイルでアプリケーションの現在のオンプレミス仮想マシンと同様のCPUとメモリのオプションを設定する。
C. 利用可能な最小のマシンタイプでインスタンステンプレートを作成し、現在のオンプレミス仮想マシンから取得したサードパーティアプリケーションのイメージを使用する。平均CPUを使用してグループ内のインスタンス数を自動スケーリングするマネージドインスタンスグループを作成する。実行中のインスタンス数を最適化するために、平均CPU使用率のしきい値を変更する。
D. さまざまなCPUとメモリのオプションを持つ複数のCompute Engineインスタンスを作成する。クラウドモニタリングエージェントをインストールし、それぞれにサードパーティアプリケーションをデプロイする。アプリケーションに高トラフィックレベルで負荷テストを実行し、その結果を使用して最適な設定を決定する。

**正解: A**

**解説:**
「一貫した使用パターン」を持つアプリケーションのサイジングに最適なGoogle Cloudの機能は、**サイズ適正化の推奨（Rightsizing Recommendations）**です。この機能は、Cloud Monitoringエージェントが収集した過去のCPUとメモリの使用率データを分析し、「このインスタンスは過剰プロビジョニングされているため、より小さなマシンタイプに変更することでコストを節約できます」といった具体的な推奨を自動的に提示します。オンプレミスと同様のサイズから始め、実際の負荷をかけた後のこの推奨に従うことが、データに基づいてリソースを最適化するための最も効率的でGoogle推奨の方法です。

-----

### <a name="no78"></a>**NO.78**

あなたは、echo-deploymentという名前のDeploymentを使用してKubernetes Engineにアプリケーションをデプロイしました。このデプロイメントは、echo-serviceというServiceを介して公開されています。アプリケーションに最小限のダウンタイムで更新を実行する必要があります。どうすべきですか？

A. `kubectl set image deployment/echo-deployment <new-image>` を使用する。
B. Kubernetesクラスタの背後にあるインスタンスグループのローリングアップデート機能を使用する。
C. deploymentのyamlファイルを新しいコンテナイメージで更新する。`kubectl delete deployment/echo-deployment`と`kubectl create -f <yaml-file>`を使用する。
D. serviceのyamlファイルを新しいコンテナイメージで更新する。`kubectl delete service/echoservice`と`kubectl create -f <yaml-file>`を使用する。

**正解: A**

**解説:**
KubernetesのDeploymentリソースは、**ローリングアップデート**をサポートしており、これがダウンタイムを最小限に抑えながらアプリケーションを更新する標準的な方法です。`kubectl set image`コマンドは、既存のDeploymentが使用するコンテナイメージを新しいものに更新するための最も簡単な命令です。このコマンドを実行すると、Kubernetesは古いバージョンのPodを一つずつ停止させながら、新しいバージョンのPodを起動していくため、サービスが完全に停止することはありません。DeleteしてからCreateする(C)と、明示的なダウンタイムが発生します。

-----

### <a name="no79"></a>**NO.79**

この問題については、Dress4Winのケーススタディを参照してください。

Dress4Winは、いくつかのアプリケーションを迅速に現状のまま（as-is）デプロイすることで、クラウドへのアプリケーションデプロイに慣れたいと考えています。彼らはあなたに推薦を求めました。何をアドバイスすべきですか？

A. 外部依存関係を持つ自己完結型のアプリケーションを、クラウドへの最初の移行対象として特定する。
B. 内部依存関係を持つエンタープライズアプリケーションを特定し、これらをクラウドへの最初の移行対象として推薦する。
C. 社内データベースをクラウドに移行し、オンプレミスのアプリケーションへのリクエストを引き続き処理することを提案する。
D. メッセージキューサーバーをクラウドに移行し、オンプレミスのアプリケーションへのリクエストを引き続き処理することを提案する。

**正解: A**

**解説:**
クラウド移行の初期フェーズ（「Lift and Shift」など）では、成功体験を積み重ねてチームの習熟度を上げることが重要です。そのためには、移行が比較的容易なアプリケーションから始めるのが定石です。**自己完結型で、外部（他の社内システムではなく、外部APIなど）への依存関係が少ないアプリケーション**は、他のシステムとの複雑な連携を考慮する必要がないため、移行のリスクと複雑さが最も低くなります。このようなアプリケーションを最初に移行することで、チームはクラウド環境に慣れ、自信を持って次のステップに進むことができます。

-----

### <a name="no80"></a>**NO.80**

あなたの会社のアプリケーション信頼性チームは、サーバーイベントのすべてを最終的な分析のためにGoogle Cloud Storageに送信するデバッグ機能をバックエンドサービスに追加しました。イベントレコードは最低50KB、最大15MBで、ピーク時には毎秒3,000イベントに達すると予想されます。データ損失を最小限に抑えたいです。どのプロセスを実装すべきですか？

A. ファイル本文にメタデータを追記する。個々のファイルを圧縮する。ファイル名をserverName-Timestampで命名する。バケットが1時間より古い場合は新しいバケットを作成し、個々のファイルを新しいバケットに保存する。それ以外の場合は、既存のバケットにファイルを保存する。
B. 10,000イベントごとにメタデータ用の単一マニフェストファイルでバッチ処理する。イベントファイルとマニフェストファイルを単一のアーカイブファイルに圧縮する。ファイル名をserverName-EventSequenceで命名する。バケットが1日より古い場合は新しいバケットを作成し、単一のアーカイブファイルを新しいバケットに保存する。それ以外の場合は、単一のアーカイブファイルを既存のバケットに保存する。
C. 個々のファイルを圧縮する。ファイル名をserverName-EventSequenceで命名する。ファイルを1つのバケットに保存する。保存後に各オブジェクトにカスタムメタデータヘッダーを設定する。
D. ファイル本文にメタデータを追記する。個々のファイルを圧縮する。ファイル名をランダムなプレフィックスパターンで命名する。ファイルを1つのバケットに保存する。

**正解: D**

**解説:**
Cloud Storageは、オブジェクト名が辞書順で近いものに高いレートでアクセスすると、パフォーマンスが低下する可能性があります（ホットスポッティング）。毎秒3,000イベントという高い書き込みレートを維持するためには、オブジェクト名が特定の範囲に集中しないようにすることが重要です。ファイル名（オブジェクト名）の先頭に**ランダムなプレフィックス**を付けることで、書き込みがバケット内の異なるキースペースに分散され、高いパフォーマンスを維持できます。`serverName-Timestamp` (A) のような命名規則は、特定のサーバーからの書き込みが集中し、ホットスポットを引き起こす可能性があります。

-----

### <a name="no81"></a>**NO.81**

あなたの会社は、小売顧客に推薦エンジンを提供しています。小売顧客には、ユーザーIDを送信するとそのユーザーの推薦リストを返すAPIを提供しています。あなたはAPIライフサイクルを担当しており、APIが後方互換性のない変更を行う場合に顧客の安定性を確保する必要があります。Google推奨のプラクティスに従いたいと考えています。どうすべきですか？

A. 古いAPIを新しいAPIで置き換える少なくとも1か月前に、後方互換性のない変更についてすべての顧客に通知するための配布リストを作成する。
B. APIドキュメントを自動生成するプロセスを作成し、APIへの更新をデプロイする際にCI/CDプロセスの一部として公開APIドキュメントを更新する。
C. 後方互換性のない変更ごとにバージョン番号を増やすAPIのバージョニング戦略を使用する。
D. 現在のAPIバージョン番号に「DEPRECATED」という接尾辞を追加するAPIのバージョニング戦略を使用する。新しいAPIには現在のバージョン番号を使用する。

**正解: C**

**解説:**
APIの後方互換性を破壊する変更（例: フィールドの削除、エンドポイントの変更）を行う場合、既存のクライアントが壊れないようにすることが非常に重要です。そのための標準的でGoogleも推奨するプラクティスは、**セマンティックバージョニング**に従い、メジャーバージョン番号をインクリメントすることです（例: `/v1/recommend` から `/v2/recommend` へ）。これにより、既存のユーザーは古いバージョンのAPI（v1）を使い続けることができ、新しいユーザーや準備ができたユーザーから新しいバージョンのAPI（v2）に移行することができます。

-----

### <a name="no82"></a>**NO.82**

あなたのチームは、新しく構築されたアプリケーションをホストするためにGoogle Kubernetes Engine（GKE）クラスタを作成する必要があります。このアプリケーションはインターネット上のサードパーティサービスにアクセスする必要があります。あなたの会社は、Google Cloud上のどのCompute EngineインスタンスにもパブリックIPアドレスを持つことを許可していません。これらのガイドラインに従うデプロイ戦略を作成する必要があります。どうすべきですか？

A. Compute Engineインスタンスを作成し、そのインスタンスにNATプロキシをインストールする。GKE上のすべてのワークロードがこのプロキシを経由してインターネット上のサードパーティサービスにアクセスするように構成する。
B. GKEクラスタをプライベートクラスタとして構成し、クラスタサブネット用にCloud NATゲートウェイを構成する。
C. GKEクラスタをルートベースのクラスタとして構成する。Virtual Private Cloud（VPC）でプライベートGoogleアクセスを構成する。
D. GKEクラスタをプライベートクラスタとして構成する。Virtual Private Cloud（VPC）でプライベートGoogleアクセスを構成する。

**正解: B**

**解説:**

  * **プライベートGKEクラスタ**: ノード（VM）にパブリックIPアドレスを持たせないようにするGKEの構成です。これにより、会社のセキュリティ要件を満たします。
  * **Cloud NAT**: プライベートIPアドレスしか持たないリソースが、インターネットへの**アウトバウンド**（外向き）接続を行えるようにするマネージドサービスです。
    プライベートクラスタとCloud NATを組み合わせることで、ノードは外部に公開されることなく、安全に必要なサードパーティサービスにアクセスできるようになります。これがこの要件に対する標準的な解決策です。

-----

### <a name="no83"></a>**NO.83**

あなたはGoogle Cloud上にステートフルなワークロードをデプロイする必要があります。ワークロードは水平方向にスケールできますが、各インスタンスは同じPOSIXファイルシステムに対して読み書きを行う必要があります。高負荷時、ステートフルワークロードは最大100MB/sの書き込みをサポートする必要があります。どうすべきですか？

A. 各インスタンスに永続ディスクを使用する。
B. 各インスタンスにリージョン永続ディスクを使用する。
C. Cloud Filestoreインスタンスを作成し、各インスタンスにマウントする。
D. Cloud Storageバケットを作成し、gcsfuseを使用して各インスタンスにマウントする。

**正解: C**

**解説:**
要件の核心は、「複数のインスタンス」が「同じPOSIXファイルシステム」に「同時に読み書き」することです。これは、共有ファイルシステムが必要であることを意味します。

  * **Cloud Filestore**は、フルマネージドのNFS（Network File System）サービスであり、POSIX準拠の共有ファイルシステムを提供します。複数のVMインスタンスから同時にマウントして読み書きすることができ、まさにこの要件のために設計されています。
  * 永続ディスク(A, B)は、通常一度に1つのVMにしか（書き込みモードで）アタッチできません。
  * gcsfuse(D)は便利ですが、POSIXセマンティクスを完全にエミュレートするものではなく、一貫性の保証やパフォーマンスの点で、本番のステートフルワークロードには適していません。

-----

### <a name="no84"></a>**NO.84**

あなたの会社はデータウェアハウジングにBigQueryを使用するGoogle Cloudプロジェクトを持っています。一部のテーブルには個人識別情報（PII）が含まれています。コンプライアンスチームのみがPIIにアクセスできます。テーブル内の他の情報はデータサイエンスチームが利用できる必要があります。コストと、テーブルへの適切なアクセスを割り当てる時間を最小限に抑えたいです。どうすべきですか？

A. ソースデータがあるデータセットから、PIIを除外して共有したいテーブルのビューを作成する。データサイエンスチームのメンバーに適切なプロジェクトレベルのIAMロールを割り当てる。ビューを含むデータセットにアクセス制御を割り当てる。
B. ソースデータがあるデータセットから、PIIを除外して共有したいテーブルのマテリアライズドビューを作成する。データサイエンスチームのメンバーに適切なプロジェクトレベルのIAMロールを割り当てる。ビューを含むデータセットにアクセス制御を割り当てる。
C. データサイエンスチーム用のデータセットを作成する。PIIを除外して共有したいテーブルのビューを作成する。データサイエンスチームのメンバーに適切なプロジェクトレベルのIAMロールを割り当てる。ビューを含むデータセットにアクセス制御を割り当てる。ビューにソースデータセットへのアクセスを承認する。
D. データサイエンスチーム用のデータセットを作成する。PIIを除外して共有したいテーブルのマテリアライズドビューを作成する。データサイエンスチームのメンバーに適切なプロジェクトレベルのIAMロールを割り当てる。ビューを含むデータセットにアクセス制御を割り当てる。ビューにソースデータセットへのアクセスを承認する。

**正解: C**

**解説:**
BigQueryで列レベルのアクセス制御を行うためのベストプラクティスは、**承認済みビュー（Authorized View）**を使用することです。

1.  **データサイエンスチーム用のデータセットを作成します**。これにより、権限管理が容易になります。
2.  この新しいデータセット内に、元のテーブルからPIIの列を除外するSELECT文で**ビューを作成します**。ビューは実データを持たないのでコストがかかりません。
3.  データサイエンスチームには、この**新しいデータセットへの閲覧権限のみ**を与えます。
4.  最も重要なステップとして、作成した**ビューに対して、元の（PIIを含む）データセットへのアクセスを承認します**。
    これにより、データサイエンスチームのメンバーは元のデータに直接アクセスすることなく、ビューを介して安全にPIIが除外されたデータのみをクエリできるようになります。

-----

### <a name="no85"></a>**NO.85**

あなたは、Compute Engineインスタンスグループでホストされている静的HTTP(S)ウェブサイトコンテンツを配信するためにCloud CDNを使用しています。キャッシュヒット率を改善したいです。どうすべきですか？

A. キャッシュキーをカスタマイズして、キーからプロトコルを除外する。
B. キャッシュされたオブジェクトの有効期限を短縮する。
C. HTTP(S)ヘッダー「Cache-Region」がユーザーの最も近いリージョンを指していることを確認する。
D. 静的コンテンツをCloud Storageバケットに複製する。Cloud CDNをそのバケット上のロードバランサに向ける。

**正解: A**

**解説:**
デフォルトでは、Cloud CDNは完全なリクエストURL（プロトコルを含む）をキャッシュキーとして使用します。つまり、`http://example.com/logo.png`と`https://example.com/logo.png`は、同じコンテンツであっても別々のキャッシュエントリとして扱われます。これにより、キャッシュが非効率になります。キャッシュキーをカスタマイズして**プロトコル（HTTP/HTTPS）を無視**するように設定すると、どちらのプロトコルでリクエストされても同じキャッシュエントリが使用されるようになり、**キャッシュヒット率が向上**します。

-----

### <a name="no86"></a>**NO.86**

あなたの会社は、オンプレミスのデータセンターからWindows Server 2022をGoogle Cloudに移行する計画です。現在オンプレミスの仮想マシンで使用しているライセンスをターゲットのクラウド環境に持ち込む必要があります。どうすべきですか？

A. オンプレミスの仮想マシンのイメージを作成し、Cloud Storageにアップロードする。Compute Engineで仮想ディスクとしてイメージをインポートする。
B. Compute Engineで標準インスタンスを作成する。OSとして、現在オンプレミス環境で使用しているのと同じMicrosoft Windowsバージョンを選択する。
C. オンプレミスの仮想マシンのイメージを作成する。Compute Engineで仮想ディスクとしてイメージをインポートする。Compute Engineで標準インスタンスを作成し、OSとして現在オンプレミス環境で使用しているのと同じMicrosoft Windowsバージョンを選択する。作成したイメージと一致するデータを含むデータディスクをアタッチする。
D. オンプレミスの仮想マシンのイメージを作成する。`--os windows-2022-dc-v`を使用してCompute Engineで仮想ディスクとしてイメージをインポートする。インポートしたディスクをブートディスクとして使用するCompute Engineの単一テナントインスタンスを作成する。

**正解: D**

**解説:**
Microsoftのライセンス規約では、既存のライセンスをクラウドに持ち込むこと（**BYOL: Bring Your Own License**）が許可されるのは、通常、共有ハードウェア上ではなく、**物理的に専有されたホスト**上でのみです。Google Cloudでは、この要件を満たすために**単一テナントノード（Sole-tenant nodes）を提供しています。したがって、正しい手順は、オンプレミスのVMイメージをインポートし、そのイメージをブートディスクとして使用するVMを単一テナントノード上**に作成することです。

-----

### <a name="no87"></a>**NO.87**

あなたの会社はGoogle Cloudリソースの使用を開始したいと考えていますが、ID管理のためにオンプレミスのActive Directoryドメインコントローラーを維持したいと考えています。どうすべきですか？

A. Admin Directory APIを使用して、Active Directoryドメインコントローラーに対して認証する。
B. Google Cloud Directory Syncを使用してActive Directoryのユーザー名をクラウドIDと同期し、SAML SSOを構成する。
C. Cloud Identity-Aware Proxyを構成し、オンプレミスのActive DirectoryドメインコントローラーをIDプロバイダーとして使用する。
D. Compute Engineを使用して、Google Cloud Directory Syncを使用してオンプレミスのADドメインコントローラーのレプリカであるActive Directory（AD）ドメインコントローラーを作成する。

**正解: B**

**解説:**
オンプレミスのActive Directory (AD) を信頼できるIDソース（IdP）としてGoogle Cloudと連携させるための標準的な方法は、以下の2つのツールを組み合わせることです。

1.  **Google Cloud Directory Sync (GCDS)**: ADのユーザーとグループをGoogle Cloud Identity（またはGoogle Workspace）に**同期**します。これにより、ユーザーアカウントがGoogle側にプロビジョニングされます。
2.  **SAMLによるシングルサインオン (SSO)**: ユーザーがGoogle Cloudにログインしようとすると、認証要求がオンプレミスのAD FS（Active Directory Federation Services）などのSAML IdPにリダイレクトされます。ユーザーは使い慣れたADの認証情報でログインし、成功するとGoogle Cloudにリダイレクトされます。
    この方法により、ID管理を一元化し、ユーザー体験を向上させることができます。

-----

### <a name="no88"></a>**NO.88**

あなたは、新しいGCPプロジェクトに対するチームの準備状況を評価する必要があります。評価を実施し、コスト最適化というビジネス目標を組み込んだスキルギャップ計画を作成する必要があります。あなたのチームはこれまでに2つのGCPプロジェクトを成功裏にデプロイしています。どうすべきですか？

A. チームトレーニングの予算を割り当てる。新しいGCPプロジェクトの締め切りを設定する。
B. チームトレーニングの予算を割り当てる。職務に基づいたGoogle Cloud認定資格をチームが取得するためのロードマップを作成する。
C. 熟練した外部コンサルタントを雇うための予算を割り当てる。新しいGCPプロジェクトの締め切りを設定する。
D. 熟練した外部コンサルタントを雇うための予算を割り当てる。職務に基づいたGoogle Cloud認定資格をチームが取得するためのロードマップを作成する。

**正解: B**

**解説:**
チームの準備状況を評価し、スキルギャップを埋めるための体系的なアプローチが求められています。**Google Cloud認定資格**は、特定の職務（例: Cloud Architect, Data Engineer）に必要なスキルセットを網羅するように設計されています。チームメンバーがそれぞれの役割に応じた認定資格の取得を目指す**ロードマップ**を作成することは、必要なスキルを明確にし、学習の目標を設定するための効果的な方法です。単にトレーニング予算を割り当てる(A)だけでは、目標が曖昧です。外部コンサルタント(C, D)は短期的な解決策にはなりますが、チーム自身のスキル向上には直接つながりません。

-----

### <a name="no89"></a>**NO.89**

あなたは営業時間中にのみ使用するアプリケーションを設計しています。最小実行可能製品（MVP）リリースでは、アクティビティがないときにはコストが発生しないように、「ゼロにスケールダウン」するマネージド製品を使用したいと考えています。どの主要なコンピュートリソースを選択すべきですか？

A. Cloud Functions
B. Compute Engine
C. Kubernetes Engine
D. App Engineフレキシブル環境

**正解: A**

**解説:**
「スケール・トゥ・ゼロ」は、トラフィックやリクエストが全くない場合に、インスタンスやコンテナがゼロになり、課金が完全に停止するサーバーレスコンピューティングの重要な特徴です。

  * **Cloud Functions**: イベント駆動型で、リクエストがあったときにのみコードが実行され、実行時間に対してのみ課金されます。リクエストがなければコストはゼロです。
  * Compute Engine (B) と Kubernetes Engine (C) は、トラフィックがなくてもノード（VM）が起動している限りコストが発生します。
  * App Engineフレキシブル環境 (D) も、最小インスタンス数を1以上に設定する必要があるため、通常はゼロにはなりません。（App Engine Standardはゼロにスケールできます）
    選択肢の中で、この要件に最も合致するのはCloud Functionsです。

-----

### <a name="no90"></a>**NO.90**

あなたは医療データを処理する機関で働いています。複数のワークロードをGoogle Cloudに移行しています。会社のポリシーでは、すべてのワークロードが物理的に分離されたハードウェアで実行される必要があり、異なるクライアントのワークロードも分離する必要があります。あなたは単一テナントノードグループを作成し、各クライアント用にノードを追加しました。これらの専用ホストにワークロードをデプロイする必要があります。どうすべきですか？

A. Compute Engineインスタンスを作成する際に、各ワークロードを正しいノードグループにホストするために、ネットワークタグとしてノードグループ名を追加する。
B. Compute Engineインスタンスを作成する際に、各ワークロードを正しいノードにホストするために、ネットワークタグとしてノード名を追加する。
C. Compute Engineインスタンスを作成する際に、各ワークロードを正しいノードグループにホストするために、ノードグループ名に基づいてノードアフィニティラベルを使用する。
D. Compute Engineインスタンスを作成する際に、各ワークロードを正しいノードにホストするために、ノード名に基づいてノードアフィニティラベルを使用する。

**正解: D**

**解説:**
**単一テナントノード（Sole-tenant nodes）**上の特定の物理ホストにVMを配置するには、**ノードアフィニティラベル**を使用します。各単一テナントノードには、`compute.googleapis.com/node-name` というデフォルトのラベルが自動的に付与されます。VMを作成する際に、このラベルと特定のノード名を指定することで、VMがその物理ホスト上にスケジュールされるように指示できます。これにより、「異なるクライアントのワークロードを（物理ノードレベルで）分離する」という厳しい要件を満たすことができます。

-----

### <a name="no91"></a>**NO.91**

あなたは、アップロードされた画像から有名な絵画を認識するアプリケーションをCloud ML Engineを使用して開発しました。アプリケーションをテストし、特定の人々が次の24時間画像をアップロードできるようにしたいです。すべてのユーザーがGoogleアカウントを持っているわけではありません。ユーザーはどのように画像をアップロードすべきですか？

A. ユーザーにCloud Storageに画像をアップロードさせる。24時間後に期限切れになるパスワードでバケットを保護する。
B. ユーザーに24時間後に期限切れになる署名付きURLを使用してCloud Storageに画像をアップロードさせる。
C. ユーザーが画像をアップロードできるApp Engine Webアプリケーションを作成する。24時間後にアプリケーションを無効にするようにApp Engineを構成する。Cloud Identityを介してユーザーを認証する。
D. ユーザーが次の24時間画像をアップロードできるApp Engine Webアプリケーションを作成する。Cloud Identityを介してユーザーを認証する。

**正解: B**

**解説:**
PDFの回答Aは非現実的です。Cloud Storageバケットはパスワードで保護できません。
**署名付きURL（Signed URLs）**は、Googleアカウントを持っていないユーザーに対しても、特定のCloud Storageオブジェクトに対する時間制限付きのアクセス権（この場合はアップロード権限）を付与するための安全で標準的な方法です。

1.  バックエンドで、特定のオブジェクト名に対するアップロード用の署名付きURLを生成します。このURLには、24時間という有効期限を設定できます。
2.  このURLをユーザーに渡します。
3.  ユーザーは、このURLに対して直接HTTP PUTリクエストを送信することで、認証なしでファイルをアップロードできます。
    これにより、安全かつ一時的に、非Googleアカウントユーザーからのファイルアップロードが可能になります。

-----

### <a name="no92"></a>**NO.92**

あなたはGoogle Kubernetes Engine（GKE）で実行されているアプリケーションを持っています。過去2週間、顧客からアプリケーションの特定の部分が非常に頻繁にエラーを返すと報告されています。現在、GKEクラスタでロギングやモニタリングソリューションは有効になっていません。問題を診断したいのですが、問題を再現できていません。アプリケーションへの混乱を最小限に抑えたいです。どうすべきですか？

A. 1. GKEクラスタを更新してCloud Operations for GKEを使用するようにする。 2. GKE Monitoringダッシュボードを使用して、影響を受けるPodからのログを調査する。
B. 1. Cloud Operations for GKEが有効な新しいGKEクラスタを作成する。 2. 影響を受けるPodを新しいクラスタに移行し、それらのPodへのトラフィックを新しいクラスタにリダイレクトする。 3. GKE Monitoringダッシュボードを使用して、影響を受けるPodからのログを調査する。
C. 1. GKEクラスタを更新してCloud Operations for GKEを使用し、Prometheusをデプロイする。 2. アプリケーションがエラーを返すたびにトリガーされるアラートを設定する。
D. 1. Cloud Operations for GKEが有効な新しいGKEクラスタを作成し、Prometheusをデプロイする。 2. 影響を受けるPodを新しいクラスタに移行し、それらのPodへのトラフィックを新しいクラスタにリダイレクトする。 3. アプリケーションがエラーを返すたびにトリガーされるアラートを設定する。

**正解: A**

**解説:**
**Cloud Operations for GKE**（ロギングとモニタリングを含む）は、**既存のGKEクラスタに対して後から有効にすることができます**。これにより、「アプリケーションへの混乱を最小限に抑える」という要件を満たせます。新しいクラスタを作成して移行する(B, D)のは、大きな混乱を伴います。Cloud Operations for GKEを有効にすると、Podからのログが自動的にCloud Loggingに収集され始め、GKEダッシュボードで簡単に確認できるようになります。これにより、再現できていない断続的なエラーのログを捉え、問題を診断することが可能になります。

-----

### <a name="no93"></a>**NO.93**

あなたの会社はGoogle WorkspaceアカウントとGoogle Cloud組織を持っています。会社の一部の開発者はGoogle Cloud組織外でGoogle Cloudプロジェクトを作成しました。あなたは、開発者がプロジェクトを作成できるようにしつつ、本番プロジェクトを変更するのを防ぐ組織構造を作成したいと考えています。すべてのプロジェクトのポリシーを一元管理し、本番プロジェクトにはより制限的なポリシーを設定できるようにしたいです。将来ビジネスニーズが変化したときに、ユーザーと開発者への混乱を最小限に抑えたいです。Google推奨のプラクティスに従いたいです。どのように組織構造を設計すべきですか？

A. 2つ目のGoogle Workspaceアカウントと組織を作成する。すべての開発者に新しい組織でプロジェクト作成者のIAMロールを付与する。開発者プロジェクトを新しい組織に移動する。両方の組織ですべてのプロジェクトのポリシーを設定する。さらに、元の組織に本番ポリシーを設定する。
B. 組織リソースの下に「Production」という名前のフォルダを作成する。すべての開発者に組織でプロジェクト作成者のIAMロールを付与する。開発者プロジェクトを組織に移動する。組織ですべてのプロジェクトのポリシーを設定する。「Production」フォルダにさらに本番ポリシーを設定する。
C. 組織リソースの下に「Development」と「Production」という名前のフォルダを作成する。すべての開発者に「Development」フォルダでプロジェクト作成者のIAMロールを付与する。開発者プロジェクトを「Development」フォルダに移動する。組織ですべてのプロジェクトのポリシーを設定する。「Production」フォルダにさらに本番ポリシーを設定する。
D. 組織を本番プロジェクト専用に指定する。開発者が組織でプロジェクト作成者のIAMロールを持っていないことを確認する。開発者のGoogle Workspaceアカウントを使用して組織外に開発プロジェクトを作成する。組織ですべてのプロジェクトのポリシーを設定する。個々の本番プロジェクトにさらに本番ポリシーを設定する。

**正解: C**

**解説:**
これは、Google Cloudのリソース階層を効果的に使用するためのベストプラクティスです。

1.  単一の組織の下に、「Development」と「Production」という**フォルダ**を作成して環境を分離します。
2.  開発者には「Development」**フォルダレベル**でプロジェクト作成者ロールを付与します。これにより、彼らは開発環境内でのみプロジェクトを作成できます。
3.  「Production」フォルダには開発者の権限を与えないことで、本番プロジェクトの変更を防ぎます。
4.  組織レベルで共通のポリシーを、各フォルダレベルで環境固有の（より制限的な）ポリシーを設定することで、ポリシーの一元管理と柔軟な制御を両立できます。

-----

### <a name="no94"></a>**NO.94**

あるアプリケーション開発チームが、現在のロギングツールが新しいクラウドベース製品のニーズを満たさないと考えています。彼らはエラーをキャプチャし、履歴ログデータを分析するのにより良いツールを求めています。彼らのニーズを満たす解決策を見つける手助けをしたいです。どうすべきですか？

A. Google Stackdriverロギングエージェントをダウンロードしてインストールするように指示する。
B. ロギングのベストプラクティスに関するオンラインリソースのリストを送信する。
C. 彼らが要件を定義し、実行可能なロギングツールを評価するのを手伝う。
D. 新機能を利用するために現在のツールをアップグレードするのを手伝う。

**正解: C**

**解説:**
これは技術的な問題解決だけでなく、コンサルティング的なアプローチを問う問題です。単に特定のツール(A)を押し付けたり、一般的な情報(B)を与えたりするのではなく、プロフェッショナルなアーキテクトとしては、まず**顧客（この場合は開発チーム）の具体的な要件をヒアリングし、明確に定義する**ことから始めるべきです。要件が固まった上で、Cloud Logging、サードパーティツールなど、複数の選択肢を提示し、それぞれのメリット・デメリットを比較**評価**することで、彼らが最適なツールを選択できるよう支援するのが最善のアプローチです。

-----

### <a name="no95"></a>**NO.95**

あなたのWebアプリケーションは、欧州連合の一般データ保護規則（GDPR）の要件に準拠する必要があります。あなたはWebアプリケーションの技術アーキテクチャに責任があります。どうすべきですか？

A. Googleは既に様々な認証を取得しており、ネイティブ機能を使用すると「パススルー」コンプライアンスを提供するため、WebアプリケーションがGoogle Cloud Platformのネイティブ機能とサービスのみを使用するようにする。
B. アプリケーション内で使用されている各サービスについて、GCP Console内で関連するGDPRコンプライアンス設定を有効にする。
C. コンプライアンスのギャップを検出するために、テスト計画戦略の一部としてCloud Security Scannerが含まれていることを確認する。
D. GDPR要件を満たすWebアプリケーションのデータセキュリティの設計を定義する。

**正解: D**

**解説:**
Google Cloud PlatformはGDPRに準拠したインフラとサービスを提供しますが、**コンプライアンスは共有責任モデル**です。Googleはインフラ（クラウドのセキュリティ）に責任を持ちますが、顧客（あなた）はクラウド上で構築するアプリケーション（クラウド内のセキュリティ）に責任を持ちます。したがって、GCPを使用しているからといってアプリケーションが自動的にGDPRに準拠するわけではありません。データの収集、処理、保存、削除の方法など、**アプリケーション自体のアーキテクチャをGDPRの要件を満たすように設計・定義する**ことが不可欠です。

-----

### <a name="no96"></a>**NO.96**

あなたの組織は、機密データをCloud Storageバケットに保存しています。規制上の理由から、会社はバケット内のデータを暗号化するために使用される暗号化キーをローテーションできなければなりません。データはDataprocで処理されます。セキュリティに関するGoogle推奨のプラクティスに従いたいと考えています。どうすべきですか？

A. Cloud Key Management Service（KMS）でキーを作成する。Cloud KMSの暗号化メソッドを使用してデータを暗号化する。
B. Cloud Key Management Service（KMS）でキーを作成する。バケットの暗号化キーをそのCloud KMSキーに設定する。
C. GPGキーペアを生成する。GPGキーを使用してデータを暗号化する。暗号化されたデータをバケットにアップロードする。
D. AES-256暗号化キーを生成する。顧客指定の暗号化キー機能を使用してバケット内のデータを暗号化する。

**正解: B**

**解説:**
PDFの回答A, Dは、アプリケーション側で暗号化/復号を行う必要があり、Dataprocのようなサービスとの連携が複雑になります。
Google Cloudで推奨される方法は、**顧客管理の暗号鍵（CMEK）**を使用することです。

1.  **Cloud KMS**で暗号化キーを作成し、そのライフサイクル（ローテーションなど）を管理します。
2.  Cloud Storageバケットのデフォルトの暗号化設定として、作成した**KMSキーを指定**します。
    これにより、バケットにアップロードされるすべてのオブジェクトは、Googleが管理するキーではなく、あなたがKMSで管理するキーで自動的に暗号化されます。Dataprocなどのサービスは、適切なIAM権限があれば、この暗号化を透過的に処理できるため、アプリケーションの変更は不要です。キーのローテーションもKMSで行うだけで済みます。

-----

### <a name="no97"></a>**NO.97**

あなたの会社はGoogle Network Intelligence CenterのFirewall Insights機能を使用しています。Compute Engineインスタンスにいくつかのファイアウォールルールが適用されています。適用されたファイアウォールルールセットの効率を評価する必要があります。Google Cloud ConsoleでFirewall Insightsページを開くと、表示するログ行がないことに気づきました。問題をトラブルシューティングするために何をすべきですか？

A. Virtual Private Cloud（VPC）フローログを有効にする。
B. 監視したいファイアウォールルールに対してファイアウォールルールロギングを有効にする。
C. あなたのユーザーアカウントに`compute.networkAdmin`のIdentity and Access Management（IAM）ロールが割り当てられていることを確認する。
D. Google Cloud SDKをインストールし、コマンドライン出力にファイアウォールログがないことを確認する。

**正解: B**

**解説:**
**Firewall Insights**は、**ファイアウォールルールロギング**によって生成されたログデータを分析して、シャドウィングルール（他のルールによって隠されているルール）や非アクティブなルールなどを特定する機能です。Firewall Insightsにデータが表示されない場合、その根本原因は、分析対象のデータソースであるファイアウォールルールログが生成されていないことです。したがって、最初にすべきことは、Insightsで分析したいファイアウォールルールに対して**ファイアウォールルールロギングを有効にする**ことです。

-----

### <a name="no98"></a>**NO.98**

あなたは30のマイクロサービスを持つ大規模な分散アプリケーションを設計しています。分散された各マイクロサービスは、データベースバックエンドに接続する必要があります。認証情報を安全に保存したいです。認証情報はどこに保存すべきですか？

A. ソースコード内
B. 環境変数内
C. シークレット管理システム内
D. ACLでアクセスが制限された設定ファイル内

**正解: C**

**解説:**
データベースの認証情報のような機密情報を、ソースコード(A)、環境変数(B)、設定ファイル(D)に平文で保存するのは、セキュリティ上の重大なリスクです。これらの情報はバージョン管理システムにコミットされたり、ログに出力されたり、不正なアクセスを受けたりする可能性があります。ベストプラクティスは、**Secret Manager**のような専用の**シークレット管理システム**を使用することです。これにより、認証情報は暗号化されて安全に保存され、IAMを通じてアクセスが厳密に制御され、監査も可能になります。

-----

### <a name="no99"></a>**NO.99**

あなたは、大規模なCRMデプロイメントのデータベースバックエンドとしてCloud SQLを使用しています。使用量が増加するにつれてスケールし、ストレージが不足しないようにし、CPU使用率を75%に維持し、レプリケーションラグを60秒未満に保ちたいと考えています。要件を満たすための正しい手順は何ですか？

A. 1) インスタンスの自動ストレージ増加を有効にする。 2) CPU使用率が75%を超えたときにStackdriverアラートを作成し、CPU使用率を減らすためにインスタンスタイプを変更する。 3) レプリケーションラグに対してStackdriverアラートを作成し、レプリケーション時間を短縮するためにデータベースをシャーディングする。
B. 1) インスタンスの自動ストレージ増加を有効にする。 2) CPU使用率を75%未満に保つためにインスタンスタイプを32コアマシンに変更する。 3) レプリケーションラグに対してStackdriverアラートを作成し、レプリケーション時間を短縮するためにデータベースをシャーディングする。
C. 1) ストレージが75%を超えたときにStackdriverアラートを作成し、インスタンスで利用可能なストレージを増やしてスペースを確保する。 2) CPU負荷を軽減するためにmemcachedをデプロイする。 3) レプリケーションラグを減らすためにインスタンスタイプを32コアマシンに変更する。
D. 1) ストレージが75%を超えたときにStackdriverアラートを作成し、インスタンスで利用可能なストレージを増やしてスペースを確保する。 2) CPU負荷を軽減するためにmemcachedをデプロイする。 3) レプリケーションラグに対してStackdriverアラートを作成し、レプリケーションラグを減らすためにインスタンスタイプを32コアマシンに変更する。

**正解: A**

**解説:**
これは、運用上の問題に対して監視とアラートに基づいた体系的なアプローチを取ることを示しています。

1.  **ストレージ**: Cloud SQLには**自動ストレージ増加**機能があり、ディスク容量が不足しそうになると自動的に拡張してくれます。これを有効にするのが最も簡単で確実です。
2.  **CPU**: CPU使用率が目標（75%）を超えたことを**Cloud Monitoring（旧Stackdriver）のアラート**で検知し、それに応じて手動または自動で**インスタンスタイプを変更（スケールアップ）**するのが適切な対応です。
3.  **レプリケーションラグ**: 同様に、ラグがしきい値（60秒）を超えたことを**アラート**で検知します。ラグの根本的な解決策として、データベースの負荷を分散させる**シャーディング**は有効な手段です。

-----

### <a name="no100"></a>**NO.100**

あなたの会社は、顧客のニーズに迅速に対応し、応えることに高い価値を置いています。主なビジネス目標は、リリースのスピードと俊敏性です。セキュリティエラーが誤って導入される可能性を減らしたいと考えています。どの2つのアクションを実行できますか？（2つ選択）

A. すべてのコードチェックインがセキュリティSMEによってピアレビューされるようにする。
B. CI/CDパイプラインの一部としてソースコードセキュリティアナライザを使用する。
C. コンポーネント間のすべてのインターフェースをユニットテストするためのスタブがあることを確認する。
D. CI/CDパイプラインと統合されたコード署名と信頼できるバイナリリポジトリを有効にする。
E. 継続的インテグレーション/継続的デリバリー（CI/CD）パイプラインの一部として脆弱性セキュリティスキャナを実行する。

**正解: B, E**

**解説:**
リリースのスピードを維持しつつセキュリティを確保する（**DevSecOps**）ためには、セキュリティチェックをCI/CDパイプラインに自動的に組み込むことが重要です。

  * **B (SAST - 静的解析)**: **ソースコードセキュリティアナライザ**（Static Application Security Testing）は、コードがビルドされる段階で、既知の脆弱なコーディングパターンやセキュリティ上の欠陥を自動的に検出します。
  * **E (脆弱性スキャン)**: **脆弱性スキャナ**は、ビルドされたアーティファクト（コンテナイメージなど）やその依存ライブラリに既知の脆弱性（CVE）が含まれていないかを自動的にスキャンします。
    これらをパイプラインに組み込むことで、開発の早い段階でセキュリティ問題を自動的に発見し、手動レビュー(A)のボトルネックを避けながら、リリースのスピードと俊敏性を損なうことなくセキュリティを向上させることができます。

-----

### <a name="no101"></a>**NO.101**

あなたは、バックアップCloud Storageバケットから90日より古いバックアップファイルを削除するソリューションを作成しています。継続的なCloud Storageの支出を最適化したいです。どうすべきですか？

A. XMLでライフサイクル管理ルールを書き、gsutilでバケットにプッシュする。
B. JSONでライフサイクル管理ルールを書き、gsutilでバケットにプッシュする。
C. `gsutil ls -lr gs://backups/**` を使用して90日より古いアイテムを見つけて削除するcronスクリプトをスケジュールする。
D. `gsutil ls -l gs://backups/**` を使用して90日より古いアイテムを見つけて削除するcronスクリプトをスケジュールし、cronでスケジュールする。

**正解: B**

**解説:**
Cloud Storageには、オブジェクトの経過日数やバージョン数などの条件に基づいて、自動的にアクション（削除、ストレージクラスの変更など）を実行する**オブジェクトのライフサイクル管理**機能が組み込まれています。この機能を利用するのが、最も効率的でコストのかからない方法です。ライフサイクル構成は**JSONファイル**で定義し、`gsutil lifecycle set`コマンドでバケットに適用します。cronスクリプト(C, D)を自前で運用すると、VMのコストや管理の手間が発生するため、マネージドな機能であるライフサイクル管理が最適です。

-----

### <a name="no102"></a>**NO.102**

あなたのクライアントから、アプリケーションインフラをGCPに移行するリーダーとして指名されました。現在の問題の一つは、オンプレミスの高性能SANが、以下のように特定されるさまざまなワークロードに対応するために、頻繁で高価なアップグレードを必要としていることです：

  * 法的理由で保持される20TBのログアーカイブ
  * 500GBのVMブート/データボリュームとテンプレート
  * 500GBの画像サムネイル
  * 顧客が数日間オフラインであってもセッションを再開できる200GBの顧客セッション状態データ
    コスト効率の良いストレージ割り当てに関するあなたの推奨事項を最もよく反映しているのはどれですか？

A. 顧客セッション状態データにはローカルSSD。ログアーカイブ、サムネイル、VMブート/データボリュームにはライフサイクル管理されたCloud Storage。
B. 顧客セッション状態データにはCloud DatastoreでバックアップされたMemcache。ログアーカイブ、サムネイル、VMブート/データボリュームにはライフサイクル管理されたCloud Storage。
C. 顧客セッション状態データにはCloud SQLでバックアップされたMemcache。VMブート/データボリュームには各種ローカルSSDバックアップインスタンス。ログアーカイブとサムネイルにはCloud Storage。
D. 顧客セッション状態データにはPersistent Disk SSDストレージでバックアップされたMemcache。VMブート/データボリュームには各種ローカルSSDバックアップインスタンス。ログアーカイブとサムネイルにはCloud Storage。

**正解: D**

**解説:**
PDFの回答Bは、VMブートボリュームをCloud Storageに置く点が不適切です。各ワークロードに最適なストレージを選択します。

  * **顧客セッション状態データ (200GB)**: 高速なアクセスが必要なため、インメモリの**Memorystore (Memcache/Redis)** が適しています。その永続化層として**Persistent Disk SSD**を使用するのが一般的です。
  * **ログアーカイブ (20TB)**: アクセス頻度が低い長期保存データなので、最も安価な**Cloud Storage**（ColdlineやArchiveクラス）が最適です。
  * **VMブート/データボリューム (500GB)**: VMが直接使用するため、**ローカルSSD**（最高速）または**永続ディスク**が適しています。
  * **画像サムネイル (500GB)**: 頻繁にアクセスされる静的コンテンツなので、**Cloud Storage**（Standardクラス）が最適です。
    これらの組み合わせを最も適切に表現しているのがDです。

-----

### <a name="no103"></a>**NO.103**

あなたの会社は、Google Analytics 360ですべてのWebトラフィックデータを取得し、BigQueryに保存しています。各国には独自のデータセットがあります。各データセットには複数のテーブルがあります。各国の分析者が、それぞれの国のデータのみを表示およびクエリできるようにしたいです。アクセス権はどのように構成すべきですか？

A. 国ごとにグループを作成する。分析者をそれぞれの国グループに追加する。「all_analysts」という単一のグループを作成し、すべての国グループをメンバーとして追加する。「all-analysis」グループにBigQueryジョブユーザーのIAMロールを付与する。適切なデータセットを、それぞれの分析者国グループに閲覧アクセスで共有する。
B. 国ごとにグループを作成する。分析者をそれぞれの国グループに追加する。「all_analysts」という単一のグループを作成し、すべての国グループをメンバーとして追加する。「all-analysis」グループにBigQueryジョブユーザーのIAMロールを付与する。適切なテーブルを、それぞれの分析者国グループに閲覧アクセスで共有する。
C. 国ごとにグループを作成する。分析者をそれぞれの国グループに追加する。「all_analysts」という単一のグループを作成し、すべての国グループをメンバーとして追加する。「all-analysis」グループにBigQueryデータ閲覧者のIAMロールを付与する。適切なデータセットを、それぞれの分析者国グループに閲覧アクセスで共有する。
D. 国ごとにグループを作成する。分析者をそれぞれの国グループに追加する。「all_analysts」という単一のグループを作成し、すべての国グループをメンバーとして追加する。「all-analysis」グループにBigQueryデータ閲覧者のIAMロールを付与する。適切なテーブルを、それぞれの分析者国グループに閲覧アクセスで共有する。

**正解: A**

**解説:**
BigQueryの権限管理におけるベストプラクティスは以下の通りです。

1.  **プロジェクトレベルのIAMロール**: ユーザーがクエリジョブを実行するためには、プロジェクトレベルで**BigQueryジョブユーザー (`roles/bigquery.jobUser`)** ロールが必要です。これにより、ユーザーはプロジェクトで課金されるジョブを実行できますが、データ自体へのアクセス権はまだありません。
2.  **データセットレベルのアクセス制御**: 実際のデータへのアクセス権は、**データセットレベル**で制御します。国ごとのGoogleグループを作成し、そのグループに対して対応する国のデータセットへの**閲覧アクセス（`roles/bigquery.dataViewer`に相当）** を付与します。
    これにより、ユーザーはクエリを実行する権限を持ちつつ、アクセスを許可されたデータセットしか見ることができないという、適切な権限分離が実現できます。

-----

### <a name="no104"></a>**NO.104**

あなたの会社は、高可用性と高パフォーマンスを必要とするエンタープライズアプリケーションをCompute Engine上で実行しています。アプリケーションは、同じリージョン内の2つのゾーンにアクティブ/パッシブモードで2つのインスタンスにデプロイされています。単一ゾーン障害が発生した場合、そのデータは即座に他のゾーンの他のインスタンスで利用可能になる必要があります。ダウンタイムとデータ損失を最小限に抑えながらパフォーマンスを最大化したいです。どうすべきですか？

A. 1. 最初のインスタンスに永続SSDディスクをアタッチする。 2. 1時間ごとにスナップショットを作成する。 3. ゾーン障害が発生した場合、作成されたスナップショットからデータを取得して、2番目のインスタンスに永続SSDディスクを再作成する。
B. 1. Cloud Storageバケットを作成する。 2. gcs-fuseを使用して最初のインスタンスにバケットをマウントする。 3. ゾーン障害が発生した場合、gcs-fuseを使用して2番目のインスタンスにCloud Storageバケットをマウントする。
C. 1. 最初のインスタンスにローカルSSDディスクをアタッチする。 2. 1時間ごとにrsyncコマンドを実行し、ターゲットを2番目のインスタンスにアタッチされた永続SSDディスクにする。 3. ゾーン障害が発生した場合、2番目のインスタンスを使用する。
D. 1. 最初のインスタンスにリージョンSSD永続ディスクをアタッチする。 2. ゾーン障害が発生した場合、ディスクを他のインスタンスに強制的にアタッチする。

**正解: D**

**解説:**
この要件（ゾーン障害時にデータを即座に利用可能にする）を満たすために設計されたのが**リージョン永続ディスク**です。

  * **リージョン永続ディスク**は、同じリージョン内の2つのゾーン間でデータを**同期的に**複製します。
  * アクティブ側のゾーンで障害が発生した場合、スタンバイ側のゾーンにあるインスタンスにこのディスクを**強制アタッチ（force-attach）**することで、データ損失なく（RPO=0）、即座にサービスを再開できます。
    スナップショット(A)はRPO（目標復旧時点）が最大1時間となり、「即座に」という要件を満たしません。

-----

### <a name="no105"></a>**NO.105**

ニュースフィードのWebサービスが、Google App Engineで以下のコードを実行しています。ピーク負荷時、ユーザーは既に見たいくつかのニュース記事が表示されると報告しています。この問題の最も可能性の高い原因は何ですか？

```python
import news
from flask import Flask, redirect, request
from flask.ext.api import status
from google.appengine.api import users

app = Flask(__name__)
sessions = {}

@app.route("/")
def homepage():
    user = users.get_current_user()
    if not user:
        return "Invalid login", status.HTTP_401_UNAUTHORIZED
    if user not in sessions:
        sessions[user] = {"viewed": []}
    
    news_articles = news.get_new_news(user, sessions[user]["viewed"])
    sessions[user]["viewed"] += [n["id"] for n in news_articles]
    
    return news.render(news_articles)

if __name__ == "__main__":
    app.run()
```

A. セッション変数が単一のインスタンスに対してローカルである。
B. セッション変数がCloud Datastoreで上書きされている。
C. キャッシュを防ぐためにAPIのURLを変更する必要がある。
D. キャッシュを停止するためにHTTP Expiresヘッダーを-1に設定する必要がある。

**正解: A**

**解説:**
App Engine Standardは、トラフィックに応じてインスタンス数を自動的にスケールします。コード内の`sessions = {}`は、各App Engineインスタンスのメモリ内にのみ存在する**グローバル変数**です。ユーザーからのリクエストは、ロードバランサによって異なるインスタンスに振り分けられる可能性があります。そのため、あるリクエストでインスタンスAに保存されたセッション情報（既読記事ID）は、次のリクエストがインスタンスBに送られた場合には存在しません。結果として、ユーザーは既に見たいくつかの記事を再度受け取ってしまいます。これを解決するには、MemcacheやFirestoreなど、全インスタンスからアクセス可能な共有のセッションストレージを使用する必要があります。

-----

### <a name="no106"></a>**NO.106**

監査人が12ヶ月ごとにあなたのチームを訪問し、過去12ヶ月間のすべてのGoogle Cloud Identity and Access Management（Cloud IAM）ポリシーの変更をレビューするよう求めます。分析と監査のプロセスを合理化し、迅速化したいです。どうすべきですか？

A. カスタムのGoogle Stackdriverアラートを作成し、監査人に送信する。
B. Cloud LoggingのエクスポートをGoogle BigQueryに有効にし、ACLとビューを使用して監査人と共有するデータをスコープする。
C. Cloud Functionsを使用してログエントリをGoogle Cloud SQLに転送し、ACLとビューを使用して監査人のビューを制限する。
D. Google Cloud Storage（GCS）へのログエクスポートを有効にして監査ログをGCSバケットに入れ、バケットへのアクセスを委任する。

**正解: B**

**解説:**
PDFの回答Dも可能ですが、Bの方が分析には優れています。**Cloud Audit Logs**には、IAMポリシーの変更を含むすべての管理者アクティビティが記録されます。この監査ログを**BigQueryにエクスポート**するのがベストプラクティスです。

  * **BigQuery**にエクスポートすることで、強力なSQLを使用して、特定の期間、特定のユーザー、特定の操作（例: `SetIamPolicy`）といった複雑な条件でログを高速に検索・集計・分析できます。
  * **ビュー**や**承認済みデータセット**機能を使えば、監査人が必要とする情報のみにアクセスを制限したビューを提供でき、セキュリティとコンプライアンスを両立できます。
    Cloud Storage(D)は安価な長期保存には適していますが、直接的な分析にはBigQueryがはるかに強力です。

-----

### <a name="no107"></a>**NO.107**

あなたの会社のユーザーフィードバックポータルは、2つのゾーンに複製された標準的なLAMPスタックで構成されています。us-central1リージョンにデプロイされ、データベースを除くすべてのレイヤーで自動スケーリングされたマネージドインスタンスグループを使用しています。現在、選ばれた少数の顧客グループのみがポータルにアクセスできます。ポータルはこれらの条件下で99.99%の可用性SLAを満たしています。しかし、来四半期には、認証されていないユーザーを含むすべてのユーザーにポータルを公開します。追加のユーザー負荷が導入された後もシステムがSLAを維持することを確認するために、回復力テスト戦略を開発する必要があります。どうすべきですか？

A. 既存のユーザー入力をキャプチャし、すべてのレイヤーで自動スケーリングがトリガーされるまでキャプチャしたユーザー負荷を再生する。同時に、ゾーンの1つですべてのリソースを終了させる。
B. 合成的なランダムユーザー入力を作成し、少なくとも1つのレイヤーで自動スケーリングロジックがトリガーされるまで合成負荷を再生し、両方のゾーンでランダムなリソースを終了させることでシステムに「カオス」を導入する。
C. 新しいシステムをより大きなユーザーグループに公開し、すべてのレイヤーで自動スケーリングロジックがトリガーされるまで毎日グループサイズを増やす。同時に、両方のゾーンでランダムなリソースを終了させる。
D. 既存のユーザー入力をキャプチャし、リソース使用率が80%を超えるまでキャプチャしたユーザー負荷を再生する。また、既存のユーザーのアプリ使用状況に基づいて推定ユーザー数を導き出し、予想負荷の200%を処理できる十分なリソースをデプロイする。

**正解: A**

**解説:**
回復力テスト（Resilience Testing）は、システムが障害に対してどれだけうまく対応できるかを検証するものです。このシナリオでは、2つの重要な側面をテストする必要があります。

1.  **負荷への対応**: 新規ユーザーの負荷によって**自動スケーリング**が正しく機能するか。
2.  **障害への対応**: **ゾーン障害**が発生してもサービスが継続できるか。
    選択肢Aは、この両方をテストしています。「キャプチャしたユーザー負荷を再生」してオートスケールをトリガーし、「ゾーンの1つですべてのリソースを終了させる」ことでゾーン障害をシミュレートします。これにより、高負荷時におけるゾーン障害からのフェイルオーバーがSLAを満たせるかを現実的に検証できます。

-----

### <a name="no108"></a>**NO.108**

あなたのマーケティング部門は、プロモーションメールキャンペーンを送信したいと考えています。開発チームは、直接的な運用管理を最小限に抑えたいと考えています。彼らは、1日あたり100から500,000のクリックスルーという幅広い顧客応答を予測しています。リンクは、プロモーションを説明し、ユーザー情報と好みを収集するシンプルなWebサイトにつながります。どのインフラを推奨すべきですか？（2つ選択）

A. Webサイトを提供するためにGoogle App Engineを使用し、ユーザーデータを保存するためにGoogle Cloud Datastoreを使用する。
B. Webサイトを提供するためにGoogle Container Engineクラスタを使用し、データを永続ディスクに保存する。
C. Webサイトを提供するためにマネージドインスタンスグループを使用し、ユーザーデータを保存するためにGoogle Cloud Bigtableを使用する。
D. Webサーバーをホストするために単一のCompute Engine仮想マシン（VM）を使用し、Google Cloud SQLでバックアップする。

**正解: A, C**

**解説:**
PDFの回答(AC)はやや疑問が残りますが、要件から最適なものを検討します。要件は「運用管理を最小限に」「100〜500,000の幅広いトラフィックに対応」です。これは**自動スケーリング**と**マネージドサービス**を強く示唆しています。

  * **A (App Engine + Datastore)**: **App Engine Standard**はトラフィックに応じてゼロから自動的にスケールし、運用が非常に簡単です。**Cloud Datastore (Firestore)**も同様にサーバーレスでスケーラブルなデータベースです。この組み合わせは要件に完全に一致します。
  * **C (MIG + Bigtable)**: **マネージドインスタンスグループ(MIG)** はCompute Engineの自動スケーリングを提供します。**Cloud Bigtable**は非常に高いスループットに対応できるNoSQLデータベースです。Aよりは運用管理が必要ですが、スケーラビリティ要件は満たせます。

BとDはスケーラビリティや運用管理の観点から劣るため不適切です。Aが最も要件に合致しています。

-----

### <a name="no109"></a>**NO.109**

GCP上でMicrosoft SQL Serverをセットアップする必要があります。GCPリージョン内のいずれかのゾーンでデータセンターの停止が発生した場合でもダウンタイムがないことを経営陣は要求しています。どうすべきですか？

A. 高可用性が有効なCloud SQLインスタンスを構成する。
B. リージョンインスタンス構成のCloud Spannerインスタンスを構成する。
C. Compute Engine上でSQL Serverをセットアップし、Windowsフェイルオーバークラスタリングを使用してAlways On可用性グループを使用する。ノードを異なるサブネットに配置する。
D. SQL Server Always On可用性グループをWindowsフェイルオーバークラスタリングを使用してセットアップする。ノードを異なるゾーンに配置する。

**正解: D**

**解説:**
Microsoft SQL Serverでゾーン障害に対する高可用性を実現するための標準的な構成は、**Always On可用性グループ**を**異なるゾーン**にまたがって配置することです。Windows Server Failover Clustering (WSFC) と組み合わせることで、一方のゾーンに障害が発生した場合、もう一方のゾーンにあるセカンダリノードに自動的にフェイルオーバーします。Cloud SQL for SQL Server(A)もHA構成を提供しますが、Compute Engine上で自前で構築する場合はDが正しい方法です。

-----

### <a name="no110"></a>**NO.110**

あなたの顧客は、企業のアプリケーションをオンプレミスのデータセンターからGoogle Cloud Platformに移行しています。セキュリティチームは、組織内のすべてのプロジェクトに対する詳細な可視性を求めています。あなたはGoogle Cloudリソースマネージャーをプロビジョニングし、自分自身を組織管理者として設定しました。セキュリティチームにどのGoogle Cloud Identity and Access Management（Cloud IAM）ロールを付与すべきですか？

A. 組織閲覧者、プロジェクトオーナー
B. 組織閲覧者、プロジェクト閲覧者
C. 組織管理者、プロジェクトブラウザ
D. プロジェクトオーナー、ネットワーク管理者

**正解: B**

**解説:**
セキュリティチームの要件は「**詳細な可視性**」であり、リソースの**変更権限**ではありません。**最小権限の原則**に従い、必要最小限の権限を付与すべきです。

  * **組織閲覧者 (`roles/resourcemanager.organizationViewer`)**: 組織内のすべてのプロジェクトとリソース階層を**閲覧**できますが、変更はできません。
  * **プロジェクト閲覧者 (`roles/viewer`)**: 各プロジェクト内のリソース（VM、バケットなど）の詳細を**閲覧**できますが、変更はできません。
    この2つの閲覧者ロールを組み合わせることで、セキュリティチームは組織全体の構成と各プロジェクト内のリソースを完全に把握でき、要件を安全に満たすことができます。

-----

### <a name="no111"></a>**NO.111**

あなたの会社は、ユーザーが会社のWebサイトからダウンロードできるレンダリングソフトウェアを作成しています。会社には世界中に顧客がいます。すべての顧客のレイテンシを最小限に抑えたいです。Google推奨のプラクティスに従いたいです。ファイルをどのように保存すべきですか？

A. マルチリージョンCloud Storageバケットにファイルを保存する。
B. リージョンCloud Storageバケットにファイルを保存する、リージョンのゾーンごとに1つのバケット。
C. 複数のリージョンCloud Storageバケットにファイルを保存する、リージョンごとにゾーンごとに1つのバケット。
D. 複数のマルチリージョンCloud Storageバケットにファイルを保存する、マルチリージョンごとに1つのバケット。

**正解: A**

**解説:**
世界中の顧客に対してコンテンツ（この場合はダウンロードファイル）を低レイテンシで提供するには、地理的に分散した場所にデータを配置するのが最適です。**マルチリージョンCloud Storageバケット**は、大陸レベルで地理的に離れた複数のデータセンターにデータを自動的に複製します。Cloud Storageは、Googleのグローバルネットワークとエッジキャッシュと連携し、ユーザーに最も近い場所からコンテンツを配信するため、ダウンロードのレイテンシを最小限に抑えることができます。これがこのユースケースのベストプラクティスです。

-----

### <a name="no112"></a>**NO.112**

あなたのBigQueryプロジェクトには複数のユーザーがいます。監査目的で、各ユーザーが先月実行したクエリの数を確認する必要があります。

A. Google Data StudioをBigQueryに接続する。ユーザーのディメンションとユーザーごとのクエリ数のメトリックを作成する。
B. BigQueryインターフェースで、JOBSテーブルに対してクエリを実行して必要な情報を取得する。
C. `bq show`を使用してすべてのジョブをリストする。ジョブごとに`bq ls`を使用してジョブ情報をリストし、必要な情報を取得する。
D. Cloud Audit Loggingを使用してCloud Audit Logsを表示し、クエリオペレーションにフィルタを作成して必要な情報を取得する。

**正解: B**

**解説:**
PDFの回答Cは非効率です。BigQueryでは、プロジェクトで実行されたすべてのジョブに関するメタデータが、`INFORMATION_SCHEMA.JOBS_BY_PROJECT` (またはリージョンごとの `JOBS_BY_...`) という組み込みの**ビュー（テーブル）**に自動的に記録されます。このビューには、ジョブを実行したユーザー（`user_email`）、ジョブのタイプ（`job_type`）、作成時刻（`creation_time`）などが含まれています。したがって、このビューに対して標準的なSQLクエリを実行するだけで、ユーザーごとのクエリ数を簡単に集計できます。これが最も直接的で効率的な方法です。

-----

### <a name="no113"></a>**NO.113**

あなたの組織は、外部ユーザーがファイルをアップロードして共有できるようにするためのモノリシックな3層アプリケーションを開発しました。このソリューションは簡単に拡張できず、信頼性に欠けています。開発チームは、マイクロサービスとフルマネージドサービスアプローチを採用するためにアプリケーションを再設計したいと考えていますが、その努力が価値あるものであることを経営陣に納得させる必要があります。経営陣にどの利点を強調すべきですか？

A. 新しいアプローチは大幅にコストが安くなり、基盤となるインフラストラクチャの管理が容易になり、CI/CDパイプラインを自動的に管理する。
B. モノリシックソリューションはDockerでコンテナに変換できる。生成されたコンテナはKubernetesクラスタにデプロイできる。
C. 新しいアプローチは、インフラストラクチャとアプリケーションの分離、新機能の開発とリリース、基盤となるインフラストラクチャの管理、CI/CDパイプラインの管理、A/Bテストの実施、および必要に応じたソリューションのスケーリングを容易にする。
D. プロセスはMigrate for Compute Engineで自動化できる。

**正解: C**

**解説:**
(注：この問題はNo.8と同じですが、選択肢の順序が異なります。)
マイクロサービスアーキテクチャとマネージドサービスへの移行は、単なる技術的な変更以上のビジネス上の利点をもたらします。選択肢Cは、その利点を最も包括的に説明しています。各サービスを**独立して開発・デプロイ**できるため、新機能のリリースサイクルが速くなります。 インフラをマネージドサービスに任せることで、運用負荷が軽減されます。また、サービスごとにスケーリングできるため、リソース効率が向上し、**A/Bテスト**なども実施しやすくなります。 これらはすべて、ビジネスの俊敏性と信頼性の向上に直結する重要な利点です。

-----

### <a name="no114"></a>**NO.114**

あなたの組織は、Google Cloud Platform上の同じネットワークに3層Webアプリケーションをデプロイしています。各層（Web、API、データベース）は他の層とは独立してスケールします。ネットワークトラフィックはWebからAPI層へ、そしてデータベース層へと流れるべきです。Web層とデータベース層の間でトラフィックが流れるべきではありません。ネットワークはどのように構成すべきですか？

A. 各層を異なるサブネットワークに追加する。
B. 個々のVMにソフトウェアベースのファイアウォールをセットアップする。
C. 各層にタグを追加し、目的のトラフィックフローを許可するルートを設定する。
D. 各層にタグを追加し、目的のトラフィックフローを許可するファイアウォールルールを設定する。

**正解: D**

**解説:**
VPCファイアウォールルールは、VMインスタンス間のトラフィックを制御するためのステートフルな機能です。**ネットワークタグ**を使用すると、VMインスタンスを論理的にグループ化できます（例: `web-server`, `api-server`, `db-server`）。
ファイアウォールルールを作成する際に、送信元と宛先をこれらのタグで指定できます。

  * `web-server`タグから`api-server`タグへのトラフィックを許可。
  * `api-server`タグから`db-server`タグへのトラフィックを許可。
  * `web-server`タグから`db-server`タグへのトラフィックを暗黙的または明示的に拒否。
    これにより、IPアドレスに依存せず、アプリケーションの層に基づいた柔軟で安全なネットワークセグメンテーション（マイクロセグメンテーション）を実現できます。

-----

### <a name="no115"></a>**NO.115**

あなたのチームは、Google Kubernetes Engine（GKE）にデプロイされるWebアプリケーションを開発しています。CTOは成功したローンチを期待しており、アプリケーションが数万人のユーザーの予想される負荷を処理できることを確認する必要があります。アプリケーションのレイテンシが特定のしきい値を下回ることを確認するために、現在のデプロイメントをテストしたいです。どうすべきですか？

A. 負荷テストツールを使用して、予想される同時ユーザー数とアプリケーションへの総リクエストをシミュレートし、結果を検査する。
B. GKEクラスタで自動スケーリングを有効にし、アプリケーションのデプロイメントで水平ポッド自動スケーリングを有効にする。アプリケーションにcurlリクエストを送信し、自動スケーリングが機能するかどうかを検証する。
C. すべてのGoogle Cloudリージョンで複数のGKEクラスタにアプリケーションを複製する。単一のグローバルIPアドレスを介して異なるクラスタを公開するために、グローバルHTTP(S)ロードバランサを構成する。
D. 開発環境でCloud Debuggerを使用して、異なるマイクロサービス間のレイテンシを理解する。

**正解: A**

**解説:**
アプリケーションが特定の負荷（数万人のユーザー）の下でパフォーマンス要件（レイテンシしきい値）を満たすかどうかを確認する唯一確実な方法は、実際にその負荷をかけてみることです。**負荷テストツール**（JMeter, Gatling, k6, Locustなど）を使用して、本番環境で予想されるトラフィックパターンをシミュレートし、その際のアプリケーションの応答時間（レイテンシ）やエラー率を測定します。これにより、ボトルネックを特定し、スケーラビリティを客観的に評価できます。

-----

### <a name="no116"></a>**NO.116**

あなたの会社には、ネットワーキングチームと開発チームがあります。開発チームは、機密データを含むCompute Engineインスタンス上でアプリケーションを実行します。開発チームはCompute Engineに対する管理者権限を必要とします。あなたの会社は、すべてのネットワークリソースがネットワーキングチームによって管理されることを要求しています。開発チームは、ネットワーキングチームがインスタンス上の機密データにアクセスできないようにしたいと考えています。どうすべきですか？

A. 1. スタンドアロンVPCを持つプロジェクトを作成し、ネットワーキングチームにネットワーク管理者ロールを割り当てる。 2. スタンドアロンVPCを持つ2番目のプロジェクトを作成し、開発チームにコンピュート管理者ロールを割り当てる。 3. Cloud VPNを使用して2つのVPCを接続する。
B. 1. スタンドアロンVirtual Private Cloud（VPC）を持つプロジェクトを作成し、ネットワーキングチームにネットワーク管理者ロールを、開発チームにコンピュート管理者ロールを割り当てる。
C. 1. 共有VPCを持つプロジェクトを作成し、ネットワーキングチームにネットワーク管理者ロールを割り当てる。 2. VPCなしで2番目のプロジェクトを作成し、それを共有VPCサービスプロジェクトとして構成し、開発チームにコンピュート管理者ロールを割り当てる。
D. 1. スタンドアロンVPCを持つプロジェクトを作成し、ネットワーキングチームにネットワーク管理者ロールを割り当てる。 2. スタンドアロンVPCを持つ2番目のプロジェクトを作成し、開発チームにコンピュート管理者ロールを割り当てる。 3. VPCピアリングを使用して2つのVPCを接続する。

**正解: C**

**解説:**
この「ネットワーク管理の一元化」と「開発チームの権限分離」という要件を満たすために設計された機能が**共有VPC（Shared VPC）**です。

1.  **ホストプロジェクト**: ネットワーキングチームが所有・管理するプロジェクト。ここに共有VPC（サブネット、ファイアウォールルールなど）を作成します。ネットワーキングチームにはこのプロジェクトで`compute.networkAdmin`ロールを付与します。
2.  **サービスプロジェクト**: 開発チームが所有・管理するプロジェクト。このプロジェクトをホストプロジェクトの共有VPCにアタッチします。
3.  開発チームには、サービスプロジェクトで`compute.instanceAdmin`ロールなどを付与します。
    これにより、開発チームは中央管理されたネットワーク内にVMインスタンスなどのリソースを作成・管理できますが、ネットワーク自体の設定を変更することはできません。また、ネットワーキングチームはネットワークを管理できますが、サービスプロジェクト内のVMやデータにアクセスすることはできません。

-----

### <a name="no117"></a>**NO.117**

あなたの会社は、エンタープライズデータウェアハウスとしてBigQueryを使用しています。データはいくつかのGoogle Cloudプロジェクトに分散しています。BigQuery上のすべてのクエリは、単一のプロジェクトで課金される必要があります。データを含むプロジェクトでクエリコストが発生しないようにしたいです。ユーザーはデータセットをクエリできる必要がありますが、編集はできません。ユーザーのアクセスロールはどのように構成すべきですか？

A. すべてのユーザーをグループに追加する。課金プロジェクトでグループにBigQueryユーザーロールを付与し、データを含むプロジェクトでBigQueryデータ閲覧者ロールを付与する。
B. すべてのユーザーをグループに追加する。課金プロジェクトでグループにBigQueryデータ閲覧者ロールを付与し、データを含むプロジェクトでBigQueryユーザーロールを付与する。
C. すべてのユーザーをグループに追加する。課金プロジェクトでグループにBigQueryジョブユーザーロールを付与し、データを含むプロジェクトでBigQueryデータ閲覧者ロールを付与する。
D. すべてのユーザーをグループに追加する。課金プロジェクトでグループにBigQueryデータ閲覧者ロールを付与し、データを含むプロジェクトでBigQueryジョブユーザーロールを付与する。

**正解: C**

**解説:**
BigQueryのIAMロールは、役割に応じて分離されています。

  * **BigQueryジョブユーザー (`roles/bigquery.jobUser`)**: クエリなどの**ジョブを実行する権限**です。このロールを**課金対象のプロジェクト**でユーザーに付与すると、そのユーザーが実行したクエリのコストがそのプロジェクトに計上されます。
  * **BigQueryデータ閲覧者 (`roles/bigquery.dataViewer`)**: データセットやテーブルの**データを読み取る権限**です。このロールを**データが実際に存在するプロジェクト**でユーザーに付与します。
    この2つを組み合わせることで、「ユーザーはデータプロジェクトのデータを閲覧できるが、クエリの実行と課金は中央の課金プロジェクトで行う」という要件を正確に満たすことができます。（注: `roles/bigquery.user` は `jobUser` と `dataEditor` 権限を含むため、この場合は権限が広すぎます）

-----

### <a name="no118"></a>**NO.118**

この問題については、TerramEarthのケーススタディを参照してください。あなたはTerramEarthのためにマイクロサービスベースのアプリケーションを構築しています。アプリケーションはDockerコンテナに基づいています。Google推奨のプラクティスに従って、アプリケーションを継続的に構築し、ビルドアーティファクトを保存したいです。どうすべきですか？

A. 1. 新しいソース変更のためのトリガーをCloud Buildに構成する。 2. Cloud Buildを呼び出して1つのコンテナイメージをビルドし、「latest」ラベルでイメージをタグ付けする。 3. イメージをArtifact Registryにプッシュする。
B. 1. 新しいソース変更のためのトリガーをCloud Buildに構成する。 2. Cloud Buildを呼び出して各マイクロサービスのコンテナイメージをビルドし、コードのコミットハッシュを使用してタグ付けする。 3. イメージをArtifact Registryにプッシュする。
C. 1. 1分ごとにリポジトリをチェックするSchedulerジョブを作成する。 2. 新しい変更があれば、Cloud Buildを呼び出してマイクロサービスのコンテナイメージをビルドする。 3. 現在のタイムスタンプを使用してイメージをタグ付けし、Artifact Registryにプッシュする。
D. 1. 新しいソース変更のためのトリガーをCloud Buildに構成する。 2. トリガーがビルドジョブを呼び出し、マイクロサービスのコンテナイメージをビルドする。 3. バージョン番号でイメージをタグ付けし、Cloud Storageにプッシュする。

**正解: B**

**解説:**
継続的インテグレーション（CI）のベストプラクティスは以下の通りです。

  * **自動トリガー**: **Cloud Build**をソースコードリポジトリ（GitHub, Cloud Source Repositoriesなど）に接続し、コードがプッシュされると自動的にビルドがトリガーされるようにします。
  * **不変で追跡可能なアーティファクト**: ビルドされたコンテナイメージには、再現性と追跡可能性のために一意なタグを付ける必要があります。「latest」(A)やタイムスタンプ(C)は上書きされたり一意でなかったりするため不適切です。**Gitのコミットハッシュ**をタグとして使用するのが最も確実で推奨される方法です。
  * **アーティファクトの保存場所**: **Artifact Registry**は、コンテナイメージやその他のビルドアーティファクトを安全に保存・管理するためのGoogle Cloudの推奨サービスです。Cloud Storage(D)は汎用ストレージであり、コンテナレジストリとしての機能はありません。

-----

### <a name="no119"></a>**NO.119**

この問題については、Dress4Winのケーススタディを参照してください。

新しいアプリケーション体験の一環として、Dress4Winは顧客が自分の画像をアップロードできるようにします。顧客は、これらの画像を表示できる人を排他的に制御します。顧客は最小限のレイテンシで画像をアップロードでき、ログイン時にはメインアプリケーションページで自分の画像を迅速に表示できる必要があります。Dress4Winはどの構成を使用すべきですか？

A. 画像ファイルをGoogle Cloud Storageバケットに保存する。Google Cloud Datastoreを使用して、各顧客のIDとその画像ファイルをマッピングするメタデータを維持する。
B. 画像ファイルをGoogle Cloud Storageバケットに保存する。Cloud Storageにアップロードされた画像に、顧客の一意のIDを含むカスタムメタデータを追加する。
C. 顧客の画像を保存するために分散ファイルシステムを使用する。ストレージの必要性が増すにつれて、より多くの永続ディスクやノードを追加する。各顧客に一意のIDを割り当て、それが各ファイルの所有者属性を設定し、画像のプライバシーを確保する。
D. 顧客の画像を保存するために分散ファイルシステムを使用する。ストレージの必要性が増すにつれて、より多くの永続ディスクやノードを追加する。Google Cloud SQLデータベースを使用して、各顧客のIDとその画像ファイルをマッピングするメタデータを維持する。

**正解: A**

**解説:**

  * **画像ファイルの保存**: 画像のような非構造化データ（バイナリオブジェクト）を大量に保存するには、スケーラブルでコスト効率の良い**Cloud Storage**が最適です。
  * **メタデータ管理と高速な検索**: 「ログイン時に自分の画像を迅速に表示する」ためには、特定のユーザーIDに関連付けられた画像のリストを高速に検索する必要があります。この種のメタデータ（`user_id` -> `image_file_path`）を管理し、高速にクエリするには**Cloud Datastore (Firestore)** のようなNoSQLデータベースが非常に適しています。Cloud Storageのカスタムメタデータ(B)で検索するのは効率的ではありません。

-----

### <a name="no120"></a>**NO.120**

あなたはMySQLを使用するアプリケーションをオンプレミスからGoogle Cloudに移行しています。アプリケーションはCompute Engineで実行され、Cloud SQLを使用します。顧客へのダウンタイムとデータ損失を最小限に抑えながら、アプリケーションのCompute Engineデプロイメントにカットオーバーしたいです。アプリケーションを最小限の変更で移行したいです。また、カットオーバー戦略を決定する必要があります。どうすべきですか？

A. (VPN設定後) オンプレミスアプリ停止 -> mysqldump作成 -> GCSへアップロード -> Cloud SQLへインポート -> (両書き込みコード変更後) GCEアプリ起動 -> オンプレミスアプリ停止
B. (プロキシ設定後) mysqldump作成 -> GCSへアップロード -> Cloud SQLへインポート -> オンプレミスアプリ停止 -> GCEアプリ起動
C. (VPN設定後) オンプレミスアプリ停止 -> GCEアプリ(オンプレDB接続)起動 -> Cloud SQLでレプリケーション構成 -> (同期完了後) GCEアプリ停止 -> Cloud SQLをスタンドアロンに昇格 -> GCEアプリ(Cloud SQL接続)再起動
D. オンプレミスアプリ停止 -> mysqldump作成 -> GCSへアップロード -> Cloud SQLへインポート -> GCEアプリ起動

**正解: C**

**解説:**
PDFの回答Cは手順が多いですが、これは**Database Migration Service (DMS)** を使わずに手動で**外部レプリカ**を作成し、**ダウンタイムを最小化**する手順を正確に記述しています。（ただし、実際にはアプリの停止・起動が複数回あり、より良い方法があります。）
より理想的な手順は、オンプレミスアプリを稼働させたまま、Cloud SQLをオンプレミスMySQLの**外部レプリカ**として設定し、データを継続的に同期させます。データが完全に同期（レプリケーションラグがゼロに近づく）したら、

1.  オンプレミスアプリケーションをメンテナンスモードにする（書き込み停止）。
2.  Cloud SQLレプリカをスタンドアロンインスタンスに**昇格**させる。
3.  アプリケーションの接続先をCloud SQLの新しいマスターに向ける。
4.  アプリケーションのメンテナンスモードを解除する。
    この手順により、カットオーバー時のダウンタイムは数分に抑えられます。選択肢の中では、レプリケーションを利用しているCが、ダンプ＆リストア(A,B,D)よりもダウンタイムが短い戦略です。

-----

### <a name="no121"></a>**NO.121**

あなたの顧客は、既存の企業アプリケーションをオンプレミスのデータセンターからGoogle Cloud Platformに移行しています。ビジネスオーナーは最小限のユーザー混乱を要求しています。パスワードの保存には厳格なセキュリティチームの要件があります。どの認証戦略を使用すべきですか？

A. G Suite Password Syncを使用してパスワードをGoogleに複製する。
B. 既存のIDプロバイダーへのSAML 2.0を介したフェデレーション認証。
C. Google Cloud Directory Syncツールを使用してGoogleにユーザーをプロビジョニングする。
D. ユーザーにGoogleパスワードを企業パスワードと一致させるように依頼する。

**正解: B**

**解説:**
「既存のIDプロバイダーを維持」し、「パスワードをクラウドに保存しない」という厳格なセキュリティ要件を満たし、「ユーザーの混乱を最小限に」抑えるための標準的な方法が**フェデレーション**です。**SAML 2.0**を使用してGoogle Cloudをサービスプロバイダー（SP）として、既存のオンプレミスIDプロバイダー（例: ADFS, Okta, Ping）をIDプロバイダー（IdP）として構成します。これにより、ユーザーは使い慣れた企業パスワードを使用してオンプレミスで認証され、その認証結果がGoogle Cloudに安全に渡されます。Googleはパスワードを一切保持しません。

-----

### <a name="no122"></a>**NO.122**

あなたの会社は、複数のCompute Engineインスタンスで実行されているアプリケーションを持っています。アプリケーションが、高スループットを必要とするオンプレミスサービスと内部IPを介して通信できることを確認する必要があります。また、レイテンシを最小限に抑えたいです。どうすべきですか？

A. OpenVPNを使用して、オンプレミス環境とGoogle Cloud間にVPNトンネルを構成する。
B. オンプレミス環境とGoogle Cloud間にダイレクトピアリング接続を構成する。
C. Cloud VPNを使用して、オンプレミス環境とGoogle Cloud間にVPNトンネルを構成する。
D. オンプレミス環境とGoogle Cloud間にCloud Dedicated Interconnect接続を構成する。

**正解: D**

**解説:**
要件は「高スループット」「内部IP通信」「低レイテンシ」です。

  * **Cloud Dedicated Interconnect**は、オンプレミスとGoogle Cloud間を、パブリックインターネットを経由しない**専用の物理回線**で接続します。
  * これにより、VPN (A, C) のようなインターネット経由の接続に比べて、はるかに高いスループット、一貫した低レイテンシ、高い信頼性が得られます。
  * ダイレクトピアリング(B)はGoogleのパブリックIPへの接続が主目的であり、VPC内の内部IPへのプライベート接続にはInterconnectが適しています。

-----

### <a name="no123"></a>**NO.123**

あなたはモバイルチャットアプリケーションを設計しています。特定のユーザーによってメッセージが送信されたことを証明し、人々がチャットメッセージをなりすましできないようにしたいです。どうすべきですか？

A. クライアント側で、送信元ユーザー識別子と宛先ユーザーでメッセージにタグを付ける。
B. クライアント側で、共有キーを使用したブロックベースの暗号化でメッセージを暗号化する。
C. 公開鍵基盤（PKI）を使用して、クライアント側で送信元ユーザーの秘密鍵でメッセージを暗号化（署名）する。
D. 信頼できる認証局を使用して、クライアントアプリケーションとサーバー間のSSL接続を有効にする。

**正解: C**

**解説:**
メッセージが「特定のユーザーによって送信されたこと」を証明する（**否認防止**）ための暗号技術は**デジタル署名**です。公開鍵基盤（PKI）の枠組みでは、送信者は自分の**秘密鍵**でメッセージのハッシュ値を暗号化（これが署名）し、メッセージに添付します。受信者は、送信者の**公開鍵**（広く公開されている）を使って署名を復号し、メッセージのハッシュ値と一致することを確認します。これにより、その秘密鍵の所有者だけがその署名を作成できたことが証明され、なりすましを防ぐことができます。

-----

### <a name="no124"></a>**NO.124**

あなたの顧客は、Google Cloud Platformで実行されているゲームサーバーから、数GBの集計されたリアルタイムの主要業績評価指標（KPI）をキャプチャし、低レイテンシでKPIを監視したいと考えています。KPIはどのようにキャプチャすべきですか？

A. ゲームサーバーからの時系列データをGoogle Bigtableに保存し、Google Data Studioを使用して表示する。
B. ゲームサーバーからStackdriverにカスタムメトリクスを出力し、Stackdriver Monitoring Consoleでダッシュボードを作成して表示する。
C. 10分ごとにCloud Storageにアップロードされた分析ファイルをBigQueryロードジョブで取り込むようにスケジュールし、結果をGoogle Data Studioで視覚化する。
D. KPIをCloud Datastoreエンティティに挿入し、Cloud Datalabでアドホックな分析と視覚化を実行する。

**正解: A**

**解説:**
PDFの回答Bも有力ですが、Aの方がより大規模なシナリオに適しています。要件は「数GBの**集計**データ」「リアルタイム」「低レイテンシ監視」です。

  * **Cloud Bigtable**は、大規模な時系列データ（KPIなど）を低レイテンシで読み書きするために設計されたNoSQLデータベースです。ゲームサーバーからの大量のKPIデータをリアルタイムで保存するのに最適です。
  * **Data Studio**はBigtableに接続でき、保存されたデータを可視化して低レイテンシのダッシュボードを作成できます。
    Cloud Monitoringのカスタムメトリクス(B)も良い選択肢ですが、非常に大規模なデータ量や複雑な集計・分析要件がある場合、Bigtableの方が柔軟性とスケーラビリティが高いです。

-----

### <a name="no125"></a>**NO.125**

あなたは、バックエンドとしてSQLを使用するPHP App Engine Standardサービスをデプロイしています。データベースへのクエリ数を最小限に抑えたいです。どうすべきですか？

A. memcacheサービスレベルを専用に設定する。クエリのハッシュからキーを作成し、Cloud SQLにクエリを発行する前にmemcacheからデータベース値を返す。
B. memcacheサービスレベルを専用に設定する。クエリ結果を含むキーでキャッシュを埋めるために毎分実行されるcronタスクを作成する。
C. memcacheサービスレベルを共有に設定する。予想されるすべてのクエリを「cached-queries」というキーに保存するために毎分実行されるcronタスクを作成する。
D. memcacheサービスレベルを共有に設定する。「cached-queries」というキーを作成し、Cloud SQLにクエリを使用する前にキーからデータベース値を返す。

**正解: A**

**解説:**
データベースへの負荷を軽減するための最も一般的な方法は**キャッシング**です。**App Engine**には、高速なインメモリキャッシュサービスである**Memcache**がバンドルされています。アプリケーションのロジックとして、

1.  データを取得する前に、まずMemcacheにデータが存在するかを確認します（キャッシュヒット）。
2.  あれば、データベースにアクセスせずにMemcacheからデータを返します。
3.  なければ（キャッシュミス）、データベースにクエリを発行し、結果を取得します。
4.  取得した結果を、後続のリクエストのためにMemcacheに保存します。
    この「キャッシュ アサイド」パターンを実装することで、データベースへのクエリ数を大幅に削減できます。

-----

### <a name="no126"></a>**NO.126**

あなたのアプリケーションは、分析のためにログをBigQueryに書き込みます。各アプリケーションは独自のテーブルを持つべきです。45日より古いログは削除されるべきです。ストレージを最適化し、Googleの推奨プラクティスに従いたいです。どうすべきですか？

A. テーブルの有効期限を45日に設定する。
B. テーブルを時間パーティション分割し、パーティションの有効期限を45日に設定する。
C. 45日より古いアプリケーションログを削除するBigQueryのデフォルトの動作に依存する。
D. BigQueryコマンドラインツール（bq）を使用して45日より古いレコードを削除するスクリプトを作成する。

**正解: B**

**解説:**
ログのような時系列データをBigQueryで扱う際のベストプラクティスは、**時間パーティション分割テーブル**を使用することです。

  * テーブルを取り込み日やイベント日でパーティショニング（例: 日次パーティション）します。
  * **パーティションの有効期限**を45日に設定します。
    これにより、BigQueryは45日より古いパーティション（データブロック全体）を自動的に、かつ**無料で**削除します。これは、DELETE文で古い行をスキャンして削除する(D)よりもはるかに効率的でコストがかかりません。テーブル全体の有効期限(A)を設定すると、テーブル全体が一度に削除されてしまいます。

-----

### <a name="no127"></a>**NO.127**

この問題については、Dress4Winのケーススタディを参照してください。

Dress4Winは、いくつかのレガシーサービスに対してGoogle Stackdriverで新しい稼働時間チェックを構成しました。Stackdriverダッシュボードはサービスが正常であると報告していません。どうすべきですか？

A. すべてのレガシーWebサーバーにStackdriverエージェントをインストールする。
B. Cloud Platform Consoleで稼働時間サーバーのIPアドレスのリストをダウンロードし、受信ファイアウォールルールを作成する。
C. 値が `GoogleStackdriverMonitoring-UptimeChecks` と一致する場合にUser-Agent HTTPヘッダーをパススルーするようにロードバランサを構成する。
D. 値が `GoogleStackdriverMonitoring-UptimeChecks` と一致する場合にUser-Agent HTTPヘッダーを含むリクエストを許可するようにレガシーWebサーバーを構成する。

**正解: B**

**解説:**
Cloud Monitoringの**稼働時間チェック（Uptime Checks）は、世界中のGoogleのプローブサーバーからターゲットのURLやIPアドレスにリクエストを送信して、外部からの到達可能性を監視します。これらのプローブサーバーからのトラフィックが、オンプレミスやVPCのファイアウォール**によってブロックされていると、チェックは失敗します。この問題を解決するには、Googleが公開している稼働時間チェックの**IPアドレス範囲リスト**を取得し、そのIP範囲からのトラフィックを許可する受信ファイアウォールルールを作成する必要があります。

-----

### <a name="no128"></a>**NO.128**

あなたはカスタムJavaアプリケーションをGoogle App Engineにデプロイします。デプロイに失敗し、以下のスタックトレースが表示されます。

`java.lang.SecurityException: SHA1 digest error for com/Altostrat/CloakedServlet.class`
...(スタックトレースの続き)

どうすべきですか？

A. 不足しているJARファイルをアップロードし、アプリケーションを再デプロイする。
B. すべてのJARファイルにデジタル署名し、アプリケーションを再デプロイする。
C. CloakedServletクラスをMD5ハッシュの代わりにSHA1を使用して再コンパイルする。

**正解: B**

**解説:**
(注：この問題はNo.38とほぼ同じですが、選択肢が異なります。)
`SecurityException: SHA1 digest error` というエラーメッセージは、JARファイルのマニフェスト（`MANIFEST.MF`）に記録されているファイルのハッシュ値と、実際に展開されたファイルのハッシュ値が一致しないことを示しています。これは、JARファイルが破損しているか、署名後に変更された場合に発生します。App Engine環境のセキュリティ要件を満たすために、すべてのJARファイルに**デジタル署名**を正しく行い、ファイルが改ざんされていないことを保証してから再デプロイすることで、この問題を解決できます。

-----

### <a name="no129"></a>**NO.129**

あなたのVPC内のすべてのCompute Engineインスタンスは、特定のポートでActive Directoryサーバーに接続できる必要があります。インスタンスから発信される他のすべてのトラフィックは許可されません。VPCファイアウォールルールを使用してこれを強制したいです。ファイアウォールルールはどのように構成すべきですか？

A. 優先度1000のegressルールを作成して、すべてのインスタンスのすべてのトラフィックを拒否する。優先度100の別のegressルールを作成して、すべてのインスタンスのActive Directoryトラフィックを許可する。
B. 優先度100のegressルールを作成して、すべてのインスタンスのすべてのトラフィックを拒否する。優先度1000の別のegressルールを作成して、すべてのインスタンスのActive Directoryトラフィックを許可する。
C. 優先度1000のegressルールを作成して、Active Directoryトラフィックを許可する。優先度100の暗黙の拒否egressルールに依存して、すべてのインスタンスのすべてのトラフィックをブロックする。
D. 優先度100のegressルールを作成して、Active Directoryトラフィックを許可する。優先度1000の暗黙の拒否egressルールに依存して、すべてのインスタンスのすべてのトラフィックをブロックする。

**正解: A**

**解説:**
VPCファイアウォールルールは、**優先度（priority）が低い数値**のルールから評価されます（0が最高優先度）。
この要件（特定トラフィックのみ許可し、他はすべて拒否）を実現するための正しいアプローチは次のとおりです。

1.  **より高い優先度（低い数値）**で、許可したいトラフィック（Active Directoryへの通信）の**許可（allow）**ルールを作成します。例：`priority 100`。
2.  **より低い優先度（高い数値）**で、その他すべてのトラフィックを**拒否（deny）**するルールを作成します。例：`priority 1000`。
    これにより、まずADへのトラフィックが許可ルールにマッチし、許可されます。AD宛でない他のすべてのトラフィックはこのルールにマッチせず、次に評価される拒否ルールにマッチしてブロックされます。

-----

### <a name="no130"></a>**NO.130**

最近の監査で、あなたのGCPプロジェクトに新しいネットワークが作成されたことがわかりました。このネットワークでは、GCEインスタンスのSSHポートが世界に公開されています。このネットワークの起源を発見したいです。どうすべきですか？

A. Stackdriverアラートコンソールで「Create VM」エントリを検索する。
B. ホームセクションのアクティビティページに移動する。カテゴリを「データアクセス」に設定し、「Create VM」エントリを検索する。
C. コンソールのロギングセクションで、ロギングセクションとして「GCE Network」を指定する。「Create Insert」エントリを検索する。
D. プロジェクトSSHキーを使用してGCEインスタンスに接続する。システムログで以前のログインを特定し、これらをプロジェクトオーナーリストと照合する。

**正解: C**

**解説:**
Google Cloudで行われたすべての管理者アクティビティ（リソースの作成、変更、削除など）は、**Cloud Audit Logs**に記録されます。これらのログは**Cloud Logging**で確認できます。ネットワークやファイアウォールルール、VMインスタンスの作成などのイベントは、**管理アクティビティ監査ログ**として記録されます。Cloud Loggingのコンソールで、対象のリソースタイプ（例: `GCE Network`）と操作（例: `v1.compute.networks.insert`）でフィルタリングすることで、誰が、いつ、どのネットワークを作成したかを正確に特定できます。

-----

### <a name="no131"></a>**NO.131**

あなたの会社は、レポートのソースとして機能する複数のオンプレミスシステムを持っています。データは十分に維持されておらず、時間とともに劣化しています。Google推奨のプラクティスを使用して、会社のデータの異常を検出したいです。どうすべきですか？

A. ファイルをCloud Storageにアップロードする。Cloud Datalabを使用してデータを探索し、クリーンにする。
B. ファイルをCloud Storageにアップロードする。Cloud Dataprepを使用してデータを探索し、クリーンにする。
C. Cloud Datalabをオンプレミスシステムに接続する。Cloud Datalabを使用してデータを探索し、クリーンにする。
D. Cloud Dataprepをオンプレミスシステムに接続する。Cloud Dataprepを使用してデータを探索し、クリーンにする。

**正解: B**

**解説:**
**Cloud Dataprep**は、データアナリストやデータサイエンティストが、コードを書かずに、視覚的なインターフェースでデータを探索、クレンジング、準備するためのインテリジェントなデータサービスです。外れ値、欠損値、フォーマットの不整合といった**データの異常**を自動的に検出し、クレンジングのためのレシピを提案してくれます。「データが劣化している」状態をクリーンにするのに最適なツールです。まずデータをCloud Storageにアップロードし、そこからDataprepで処理するのが一般的なワークフローです。

-----

### <a name="no132"></a>**NO.132**

あなたは、住宅ローン承認文書をCloud Storageに保存する金融機関で働いています。これらの承認文書への変更は、別の承認ファイルとしてアップロードする必要があるため、これらの文書が次の5年間、削除または上書きされないようにする必要があります。どうすべきですか？

A. バケットに5年間の保持ポリシーを作成する。保持ポリシーにロックをかける。
B. 均一なバケットレベルのアクセスでバケットを作成し、サービスアカウントにオブジェクトライターのロールを付与する。サービスアカウントを使用して新しいファイルをアップロードする。
C. バケットの暗号化に顧客管理のキーを使用する。5年後にキーをローテーションする。
D. 詳細なアクセス制御でバケットを作成し、サービスアカウントにオブジェクトライターのロールを付与する。サービスアカウントを使用して新しいファイルをアップロードする。

**正解: A**

**解説:**
この「削除・上書きの絶対禁止」というコンプライアンス要件を満たすために設計された機能が、Cloud Storageの**バケットロック**です。

1.  まず、バケットに**保持ポリシー**を設定します（例: 5年間）。これにより、指定された期間、オブジェクトの削除や上書きができなくなります。
2.  次に、この保持ポリシー自体が変更されたり削除されたりしないように、ポリシーを**ロック**します。一度ロックすると、Google Cloudのどのユーザー（組織管理者を含む）も、指定された期間が経過するまでポリシーを解除したり期間を短縮したりすることはできません。

-----

### <a name="no133"></a>**NO.133**

あなたは、実行中のGoogle Container Engineクラスタが、アプリケーションの需要の変化に応じてスケーリングできるようにしたいです。どうすべきですか？

A. `gcloud container clusters resize`コマンドを使用して、Container Engineクラスタにノードを追加する。
B. `gcloud compute instances add-tags`コマンドを使用して、クラスタ内のインスタンスにタグを追加する。
C. `gcloud alpha container clusters update mycluster --enable-autoscaling --min-nodes=1 --max-nodes=10`コマンドを使用して、既存のContainer Engineクラスタを更新する。
D. `gcloud alpha container clusters create mycluster --enable-autocaling --min-nodes=1 --max-nodes=10`コマンドを使用して新しいContainer Engineクラスタを作成し、アプリケーションを再デプロイする。

**正解: C**

**解説:**
(注：この問題はNo.47と同じです。)
**既存の**Kubernetes Engineクラスタに対してオートスケーリングを有効にするには、`gcloud container clusters update`コマンドを使用します。`--enable-autoscaling`フラグと、ノード数の最小値（`--min-nodes`）および最大値（`--max-nodes`）を指定することで、**クラスタオートスケーラー**が有効になります。これにより、Podのスケジューリング要求に応じて、クラスタ内のノード（VMインスタンス）の数が自動的に増減します。`resize`(A)は手動でのサイズ変更であり、自動ではありません。

-----

### <a name="no134"></a>**NO.134**

あなたのソリューションは、ステージングおよびテスト環境では見られなかったパフォーマンスのバグを本番環境で生み出しています。将来この問題を回避するために、テストおよびデプロイの手順を調整したいです。どうすべきですか？

A. 本番環境への変更のデプロイを少なくする。
B. 本番環境へのより小さな変更をデプロイする。
C. テストおよびステージング環境の負荷を増やす。
D. 本番環境へのロールアウトの前に、ユーザーの小さなサブセットに変更をデプロイする。

**正解: C**

**解説:**
「本番環境でしか発生しないパフォーマンスバグ」の最も一般的な原因は、テスト/ステージング環境の負荷が本番環境の実際の負荷と乖離していることです。データベースのコネクションプール枯渇、スケーリングの遅延、競合状態など、多くのパフォーマンス問題は高負荷下でのみ顕在化します。したがって、テスト/ステージング環境で**本番環境と同等の負荷**をかける**負荷テスト**や**ストレステスト**を実施することが、この種の問題を事前に発見し、回避するための最も効果的な方法です。

-----

### <a name="no135"></a>**NO.135**

あなたは、Compute Engine上のインスタンスとオンプレミスのデータセンター間にプライベートな接続を確立する必要があります。少なくとも20Gbpsの接続が必要です。Google推奨のプラクティスに従いたいです。接続はどのように設定すべきですか？

A. VPCを作成し、Dedicated Interconnectを使用してオンプレミスのデータセンターに接続する。
B. VPCを作成し、単一のCloud VPNを使用してオンプレミスのデータセンターに接続する。
C. Cloud Content Delivery Network（Cloud CDN）を作成し、Dedicated Interconnectを使用してオンプレミスのデータセンターに接続する。
D. Cloud Content Delivery Network（Cloud CDN）を作成し、単一のCloud VPNを使用してオンプレミスのデータセンターに接続する。

**正解: A**

**解説:**
20Gbpsという高帯域要件を満たすことができるのは、**Dedicated Interconnect**です。これは、10Gbpsまたは100Gbpsのポート速度を持つ専用の物理接続を提供します（複数束ねて帯域を増やすことも可能）。Cloud VPN(B)はトンネルあたりのスループットが低く、この要件には適していません。Cloud CDN(C,D)はコンテンツ配信ネットワークであり、オンプレミスとのプライベート接続には使用しません。

-----

### <a name="no136"></a>**NO.136**

Google Cloud Platformのリソースは、組織、フォルダ、プロジェクトを使用して階層的に管理されます。これらの異なるレベルにCloud Identity and Access Management（IAM）ポリシーが存在する場合、階層の特定のノードでの有効なポリシーは何ですか？

A. 有効なポリシーは、ノードで設定されたポリシーによってのみ決定される。
B. 有効なポリシーは、ノードで設定されたポリシーであり、その祖先のポリシーによって制限される。
C. 有効なポリシーは、ノードで設定されたポリシーとその祖先から継承されたポリシーの和集合である。
D. 有効なポリシーは、ノードで設定されたポリシーとその祖先から継承されたポリシーの積集合である。

**正解: C**

**解説:**
Google CloudのIAMポリシーは**階層の上から下へ継承**されます。あるリソース（例: プロジェクト）に対するユーザーの有効な権限は、そのリソース自体に設定されたIAMポリシーと、その親であるフォルダや組織から**継承されたポリシーの和集合（Union）になります。例えば、組織レベルで閲覧者ロールを持ち、プロジェクトレベルで編集者ロールを持つユーザーは、そのプロジェクトに対して閲覧者かつ**編集者の権限を持ちます。権限は上位の階層で拒否することはできず、常に加算されていきます。

-----

### <a name="no137"></a>**NO.137**

あなたは、スタートアップのGCP試用をサポートするためにビジネスプロセスを分析・定義しており、製品の消費者需要がどうなるかまだわかっていません。マネージャーは、GCPのサービスコストを最小限に抑え、Googleのベストプラクティスを遵守するよう要求しています。どうすべきですか？

A. 無料利用枠と継続利用割引を活用する。サービスコスト管理のためのスタッフポジションを確保する。
B. 無料利用枠と継続利用割引を活用する。サービスコスト管理についてチームにトレーニングを提供する。
C. 無料利用枠と確約利用割引を活用する。サービスコスト管理のためのスタッフポジションを確保する。
D. 無料利用枠と確約利用割引を活用する。サービスコスト管理についてチームにトレーニングを提供する。

**正解: D**

**解説:**
PDFの回答Bの「継続利用割引」は古い制度です。現在のGCPでは**確約利用割引（CUDs）**が主流です。
需要が不確定なスタートアップにとって、コストを最小化するためのアプローチは以下の通りです。

  * **無料利用枠**: 試用期間中にコストをかけずにサービスを評価するために活用します。
  * **確約利用割引 (CUDs)**: 1年または3年の継続利用を約束（コミット）することで、Compute Engineなどのリソースを大幅な割引価格で利用できます。ベースラインとなる最小限の利用量に対して確約を結ぶのが良い戦略です。
  * **チームへのトレーニング**: コスト意識は全員が持つべきです。どのような操作がコストに影響するかをチーム全体が理解することで、日々の開発活動の中で自然とコスト最適化が図られます。

-----

### <a name="no138"></a>**NO.138**

あなたはCloud Run for Anthosにデプロイされたアプリケーションを管理しており、アプリケーションの新しいバージョンをデプロイするための戦略を定義する必要があります。ロールアウトを進めるべきか決定するために、本番トラフィックのサブセットで新しいコードを評価したいです。どうすべきですか？

A. 新しいバージョンでCloud Runに新しいリビジョンをデプロイする。リビジョン間のトラフィックパーセンテージを構成する。
B. 新しいバージョンでCloud Runに新しいサービスをデプロイする。両方のサービスの前にCloud Load Balancingインスタンスを追加する。
C. Google Cloud ConsoleのCloud Runページで、開発ブランチ用のCloud Buildを使用した継続的デプロイメントを設定する。Cloud Buildトリガーの一部として、新しいバージョンに誘導したいトラフィックのパーセンテージで置換変数TRAFFIC_PERCENTAGEを構成する。
D. Google Cloud Consoleで、Cloud Run上のアプリケーションの新しいバージョンを指す新しいサービスでTraffic Directorを構成する。Traffic Directorを構成して、トラフィックの小さなパーセンテージをアプリケーションの新しいバージョンに送信する。

**正解: A**

**解説:**
**Cloud Run**は、**リビジョン**ベースのデプロイメントモデルを採用しています。新しいコードをデプロイすると、新しいリビジョンが作成されます。Cloud Runには、複数のリビジョンにトラフィックを分割する機能が組み込まれています。例えば、「新しいリビジョンに10%、最新の安定リビジョンに90%」のようにトラフィックの割合を設定できます。これにより、新しいバージョンを安全にテスト（カナリアリリース）し、問題がなければ徐々に新しいリビジョンへのトラフィックを100%に移行できます。

-----

### <a name="no139"></a>**NO.139**

あなたの会社は、コンピュートニーズのためにGoogle Compute Engineへの迅速なリフト＆シフトを完了しました。よりクラウドネイティブなソリューションを設計・デプロイするためにあと9ヶ月あります。具体的には、**no-ops**で**自動スケーリング**するシステムを望んでいます。どの2つのコンピュート製品を選ぶべきですか？（2つ選択）

A. コンテナを備えたCompute Engine
B. コンテナを備えたGoogle Kubernetes Engine
C. Google App Engine Standard Environment
D. カスタムインスタンスタイプを備えたCompute Engine
E. マネージドインスタンスグループを備えたCompute Engine

**正解: B, C**

**解説:**

  * **No-Ops**: インフラのプロビジョニングや管理をプラットフォームに任せ、開発者はアプリケーションコードに集中できる状態を指します。
  * **C (App Engine Standard)**: まさにNo-Opsの代表例です。コードをデプロイするだけで、Googleがインフラの管理、パッチ適用、スケーリングをすべて自動で行います。
  * **B (Google Kubernetes Engine)**: GKEは、コントロールプレーンがGoogleによって完全に管理されるマネージドなKubernetesサービスです。ノードの自動アップグレードや自動修復、クラスタオートスケーラーなどの機能により、運用負荷が大幅に軽減され、No-Opsに近い体験を提供します。
    Compute Engine (A, D, E) はIaaSであり、OSのパッチ適用など、より多くの運用管理責任をユーザーが負う必要があります。

-----

### <a name="no140"></a>**NO.140**

オペレーションマネージャーが、J2EEアプリケーションをクラウドに移行する際に考慮すべき推奨プラクティスのリストを求めています。どの3つのプラクティスを推奨すべきですか？（3つ選択）

A. アプリケーションコードをGoogle App Engineで実行するように移植する。
B. リアルタイムメトリクスをキャプチャするために、Cloud Dataflowをアプリケーションに統合する。
C. Stackdriver Debuggerのような監視ツールでアプリケーションを計装する。
D. クラウドインフラを確実にプロビジョニングするために自動化フレームワークを選択する。
E. ステージング環境での自動テストを備えた継続的インテグレーションツールをデプロイする。
F. MySQLからGoogle Cloud DatastoreやBigtableのようなマネージドNoSQLデータベースに移行する。

**正解: C, D, E**

**解説:**
PDFの回答A,E,Fは、具体的な実装に寄りすぎており、一般的な「プラクティス」とは言えません。クラウド移行を成功させるための一般的なベストプラクティスは以下の通りです。

  * **C (監視・計装)**: クラウドではインフラがブラックボックスになる部分があるため、アプリケーションのパフォーマンスやエラーを詳細に把握するための**監視（Monitoring）**や**デバッグ（Debugger）**ツールを導入することが不可欠です。
  * **D (Infrastructure as Code)**: **自動化フレームワーク**（Terraform, Deployment Managerなど）を使用してインフラをコードとして管理（IaC）することで、環境の再現性を確保し、ヒューマンエラーを減らし、迅速なプロビジョニングを可能にします。
  * **E (CI/CD)**: **継続的インテグレーション（CI）**ツールと自動テストを導入することで、品質を維持しながら開発とリリースの速度を向上させることができます。これはクラウドの俊敏性を最大限に活用するために重要です。

-----

### <a name="no141"></a>**NO.141**

あなたはCompute Engineで実行されるアプリケーションを持っています。リージョン障害が発生した場合に、アプリケーションが別のリージョンにフェイルオーバーする必要があるディザスタリカバリ計画を考慮したアーキテクチャを設計する必要があります。どうすべきですか？

A. 同じプロジェクトだが異なるリージョンに2つのCompute Engineインスタンスでアプリケーションをデプロイする。最初のインスタンスを使用してトラフィックを処理し、災害時にスタンバイインスタンスにフェイルオーバーするためにHTTPロードバランシングサービスを使用する。
B. Compute Engineインスタンスにアプリケーションをデプロイする。インスタンスを使用してトラフィックを処理し、災害時にオンプレミスのインスタンスにフェイルオーバーするためにHTTPロードバランシングサービスを使用する。
C. 同じプロジェクトだが異なるリージョンにそれぞれ2つのCompute Engineインスタンスグループでアプリケーションをデプロイする。最初のインスタンスグループを使用してトラフィックを処理し、災害時にスタンバイインスタンスグループにフェイルオーバーするためにHTTPロードバランシングサービスを使用する。
D. 別のプロジェクトで異なるリージョンにそれぞれ2つのCompute Engineインスタンスグループでアプリケーションをデプロイする。最初のインスタンスグループを使用してトラフィックを処理し、災害時にスタンバイインスタンスにフェイルオーバーするためにHTTPロードバランシングサービスを使用する。

**正解: C**

**解説:**
リージョン間のディザスタリカバリ（DR）を構成するための標準的な方法は以下の通りです。

1.  **複数リージョンへのデプロイ**: アプリケーションを複数のリージョンに**インスタンスグループ**としてデプロイします。インスタンスグループを使用することで、高可用性とスケーラビリティが確保されます。
2.  **グローバルロードバランサ**: **外部HTTP(S)ロードバランサ**はグローバルリソースであり、複数のリージョンにまたがるバックエンド（インスタンスグループ）にトラフィックを分散できます。
3.  **フェイルオーバー**: ロードバランサはバックエンドのヘルスチェックを常に行っており、プライマリリージョンのバックエンドが利用不可になった場合、自動的にトラフィックをセカンダリ（DR）リージョンのバックエンドにフェイルオーバーさせます。

-----

### <a name="no142"></a>**NO.142**

あなたは、2つのリージョンにまたがる単一のVPCでCompute Engineアプリケーションを確立したいと考えています。アプリケーションは、VPNを介してオンプレミスネットワークと通信する必要があります。VPNはどのようにデプロイすべきですか？

A. VPCとオンプレミスネットワーク間でVPCネットワークピアリングを使用する。
B. IAMとVPC共有を使用してVPCをオンプレミスネットワークに公開する。
C. 各リージョンからオンプレミスのピアゲートウェイへのVPNトンネルを持つグローバルCloud VPNゲートウェイを作成する。
D. 各リージョンにCloud VPNゲートウェイをデプロイする。各リージョンがオンプレミスのピアゲートウェイに少なくとも1つのVPNトンネルを持つことを確認する。

**正解: D**

**解説:**
PDFの回答Cの「グローバルCloud VPNゲートウェイ」は存在しません。VPCはグローバルリソースですが、Cloud VPNゲートウェイとCloud Routerは**リージョンリソース**です。高可用性を確保し、リージョン障害に備えるためには、アプリケーションがデプロイされている**各リージョンに**Cloud VPNゲートウェイとCloud Routerをデプロイし、それぞれがオンプレミスネットワークへのVPNトンネルを確立するのがベストプラクティスです。BGPを使用するダイナミックルーティングを構成すれば、あるリージョンへの経路に障害が発生した場合、トラフィックは自動的に別のリージョンを経由してルーティングされます。

-----

### <a name="no143"></a>**NO.143**

あなたの会社は、Compute Engineインスタンス上で実行されるステートレスなアプリケーションを持っています。アプリケーションは通常の営業時間中に重く使用され、営業時間外は軽く使用されます。ユーザーは、ピーク時にアプリケーションが遅いと報告しています。アプリケーションのパフォーマンスを最適化する必要があります。どうすべきですか？

A. 既存のディスクのスナップショットを作成する。スナップショットからインスタンステンプレートを作成する。インスタンステンプレートから自動スケーリングされたマネージドインスタンスグループを作成する。
B. 既存のディスクのスナップショットを作成する。スナップショットからカスタムイメージを作成する。カスタムイメージから自動スケーリングされたマネージドインスタンスグループを作成する。
C. 既存のディスクからカスタムイメージを作成する。カスタムイメージからインスタンステンプレートを作成する。インスタンステンプレートから自動スケーリングされたマネージドインスタンスグループを作成する。
D. 既存のディスクからインスタンステンプレートを作成する。インスタンステンプレートからカスタムイメージを作成する。カスタムイメージから自動スケーリングされたマネージドインスタンスグループを作成する。

**正解: C**

**解説:**
負荷の変動に応じてパフォーマンスを最適化するには、**自動スケーリング**が最適な解決策です。そのための正しい手順は以下の通りです。

1.  **カスタムイメージの作成**: まず、現在のインスタンスのディスクから、アプリケーションと設定が含まれた再利用可能な**カスタムイメージ**を作成します。
2.  **インスタンステンプレートの作成**: 次に、そのカスタムイメージを使用し、マシンタイプなどの設定を定義した**インスタンステンプレート**を作成します。
3.  **マネージドインスタンスグループの作成**: 最後に、そのインスタンステンプレートに基づいて**マネージドインスタンスグループ（MIG）を作成し、CPU使用率などのメトリクスに基づく自動スケーリングポリシー**を設定します。
    これにより、負荷が高いときにはインスタンスが自動的に増え、負荷が低いときには減るため、パフォーマンスとコストが最適化されます。

-----

### <a name="no144"></a>**NO.144**

あなたは、Debian Linux環境で実行する必要があるアプリケーションをGoogle Cloudにデプロイする必要があります。アプリケーションは正しく動作するために広範な構成が必要です。利用可能になったときに、最小限の手動介入でDebianディストリビューションの更新をインストールできるようにしたいです。どうすべきですか？

A. 最新のDebianイメージを使用してCompute Engineインスタンステンプレートを作成する。このテンプレートからインスタンスを作成し、起動スクリプトの一部としてアプリケーションをインストールおよび構成する。新しいGoogle管理のDebianイメージが利用可能になるたびにこのプロセスを繰り返す。
B. DebianベースのCompute Engineインスタンスを作成し、アプリケーションをインストールおよび構成し、OSパッチ管理を使用して利用可能な更新をインストールする。
C. 最新の利用可能なDebianイメージでインスタンスを作成する。SSH経由でインスタンスに接続し、インスタンス上でアプリケーションをインストールおよび構成する。新しいGoogle管理のDebianイメージが利用可能になるたびにこのプロセスを繰り返す。
D. DebianをベースイメージとするDockerコンテナを作成する。Dockerイメージ作成プロセスの一部としてアプリケーションをインストールおよび構成する。Google Kubernetes Engineでコンテナをホストし、新しい更新が利用可能になるたびにコンテナを再起動する。

**正解: B**

**解説:**
「最小限の手動介入でOSの更新をインストール」という要件に最も合致するのは、Google Cloudの**OS Patch Management**サービスです。このサービスを使用すると、Compute Engineインスタンス群に対してOSのパッチ（セキュリティ更新など）をスケジュールに従って自動的に適用したり、オンデマンドで適用したりできます。これにより、各インスタンスにログインして手動で`apt-get upgrade`などを実行する手間が省け、一貫したパッチ適用を大規模に管理できます。

-----

### <a name="no145"></a>**NO.145**

あなたはCompute Engine上にいくつかのインスタンスをデプロイしました。セキュリティ要件として、インスタンスはパブリックIPアドレスを持つことができません。Google Cloudとあなたのオフィス間にVPN接続はなく、セキュリティ要件に違反することなく、特定の マシンにSSHで接続する必要があります。どうすべきですか？

A. インスタンスがホストされているサブネットにCloud NATを構成する。インスタンスに到達するためにCloud NATのIPアドレスにSSH接続を作成する。
B. すべてのインスタンスを非マネージドインスタンスグループに追加する。バックエンドとしてインスタンスグループを持つTCPプロキシ負荷分散を構成する。TCPプロキシIPを使用してインスタンスに接続する。
C. インスタンスに対してIdentity-Aware Proxy（IAP）を構成し、IAPセキュアトンネルユーザーのロールを持っていることを確認する。gcloudコマンドラインツールを使用してインスタンスにSSHで接続する。
D. ネットワークに踏み台ホストを作成し、オフィスの場所から踏み台ホストにSSHで接続する。踏み台ホストから、目的のインスタンスにSSHで接続する。

**正解: C**

**解説:**
**Identity-Aware Proxy (IAP) のTCP転送**機能は、まさにこの目的（パブリックIPを持たないVMへの安全なSSH/RDP接続）のために設計されています。IAPを有効にすると、`gcloud compute ssh`コマンドは自動的にIAPを介した暗号化されたトンネルを作成し、ユーザーのIAM権限（IAPセキュアトンネルユーザーロール）を検証した上で、VMに接続します。これにより、VPNや踏み台ホスト（D）を構築・管理する必要がなく、ゼロトラストの原則に基づいて安全にアクセスできます。

-----

### <a name="no146"></a>**NO.146**

あなたは、US-Centralリージョンにある本番Linux仮想マシンのコピーを作成したいです。本番仮想マシンに変更があった場合に、コピーを簡単に管理および交換できるようにしたいです。コピーをUS-Eastリージョンの別のプロジェクトに新しいインスタンスとしてデプロイします。どの手順を踏むべきですか？

A. Linuxのddとnetcatコマンドを使用して、ルートディスクの内容をUS-Eastリージョンの新しい仮想マシンインスタンスにコピーしてストリーミングする。
B. ルートディスクのスナップショットを作成し、US-Eastリージョンに新しい仮想マシンインスタンスを作成する際に、そのスナップショットをルートディスクとして選択する。
C. Linuxのddコマンドでルートディスクからイメージファイルを作成し、そのイメージファイルから新しいディスクを作成し、それを使用してUS-Eastリージョンに新しい仮想マシンインスタンスを作成する。
D. ルートディスクのスナップショットを作成し、そのスナップショットからGoogle Cloud Storageにイメージファイルを作成し、そのイメージファイルをルートディスクとして使用してUS-Eastリージョンに新しい仮想マシンインスタンスを作成する。

**正解: D**

**解説:**
Compute Engineのディスクやスナップショットはリージョンリソースであり、直接他のリージョンから参照することはできません。また、プロジェクトをまたぐ必要もあります。これを実現するための標準的な手順は以下の通りです。

1.  元のディスクの**スナップショット**を作成します。
2.  そのスナップショットから**カスタムイメージ**を作成します。イメージはグローバルリソースとして扱え、複数のプロジェクトからアクセス可能です。
3.  新しいプロジェクトで、作成したカスタムイメージを指定して、US-Eastリージョンに新しいVMインスタンスを作成します。
    これにより、リージョンとプロジェクトをまたいでVMを複製できます。

-----

### <a name="no147"></a>**NO.147**

あなたは、重要なビジネス情報をCloud Storageバケットに保存したいと考えています。情報は定期的に変更されますが、以前のバージョンを定期的に参照する必要があります。これらのバケット内の情報へのすべての変更の記録があることを確認したいです。誤った編集や削除を簡単にロールバックできるようにしたいです。どの機能を有効にすべきですか？

A. バケットロック
B. オブジェクトバージョニング
C. オブジェクト変更通知
D. オブジェクトライフサイクル管理

**正解: B**

**解説:**
**オブジェクトバージョニング**を有効にすると、Cloud Storageバケット内のオブジェクトを上書きまたは削除しても、古いバージョンが失われることはありません。代わりに、非現行バージョンとして保持されます。これにより、

  * 誤って削除または上書きしたファイルを**簡単に復元（ロールバック）**できます。
  * オブジェクトの**変更履歴**がすべて保持されるため、監査や参照の要件を満たせます。
    これがこのユースケースに最適な機能です。

-----

### <a name="no148"></a>**NO.148**

あなたはKubernetes Engineにアプリケーションをデプロイし、Cloud SQLプロキシコンテナを使用して、Kubernetesで実行されているサービスがCloud SQLデータベースを利用できるようにしています。アプリケーションがデータベース接続の問題を報告しているとの通知を受けました。あなたの会社のポリシーでは、事後分析が必要です。どうすべきですか？

A. `gcloud sql instances restart`を使用する。
B. Cloud SQLプロキシコンテナが使用するサービスアカウントがまだCloud Build編集者ロールを持っていることを検証する。
C. GCPコンソールでStackdriver Loggingに移動する。Kubernetes EngineとCloud SQLのログを参照する。
D. GCPコンソールでCloud SQLに移動する。最新のバックアップを復元する。`kubectl`を使用してすべてのポッドを再起動する。

**正解: C**

**解説:**
データベース接続の問題が発生した場合、まず原因を特定するために**ログを確認する**のが鉄則です。

  * **Kubernetes Engineのログ**: Cloud SQLプロキシコンテナ自体のログを確認し、認証エラー、ネットワークの問題、またはプロキシのクラッシュなどが発生していないか調べます。これらのログはCloud Loggingに集約されています。
  * **Cloud SQLのログ**: データベースサーバー側のログを確認し、接続過多、エラークエリ、またはインスタンス自体の問題などがないか調べます。
    両方のログを突き合わせることで、問題の根本原因を特定できます。再起動(A)やリストア(D)は、原因を特定する前の場当たり的な対応であり、推奨されません。

-----

### <a name="no149"></a>**NO.149**

あるリードソフトウェアエンジニアが、彼の新しいアプリケーション設計がWebSocketと、Webサーバー間で分散されていないHTTPセッションを使用しているとあなたに告げました。あなたは、彼のアプリケーションがGoogle Cloud Platformで適切に実行されることを確認する手助けをしたいです。どうすべきですか？

A. エンジニアがWebSocketコードをHTTPストリーミングを使用するように変換するのを手伝う。
B. セキュリティチームとWebSocket接続の暗号化要件を確認する。
C. クラウド運用チームとエンジニアと会い、ロードバランサのオプションについて話し合う。
D. エンジニアがWebSocketとHTTPセッションに依存しない分散ユーザーセッションサービスを使用するようにアプリケーションを再設計するのを手伝う。

**正解: C**

**解説:**
「分散されていないHTTPセッション」は、**セッションアフィニティ（スティッキーセッション）**が必要であることを示唆しています。つまり、同じユーザーからのリクエストは常に同じバックエンドサーバーに送信される必要があります。また、WebSocketも特定のサーバーとの長期間の接続を維持します。Google Cloudのロードバランサには、これらの要件に対応できるものとできないものがあります。例えば、外部HTTP(S)ロードバランサはWebSocketをサポートし、バックエンドサービスでセッションアフィニティを構成できます。どのロードバランサがアプリケーションの要件に最適かをエンジニアと運用チームで議論し、適切なものを選択することが、問題を解決するための最初のステップです。

-----

### <a name="no150"></a>**NO.150**

あなたの顧客は、最近更新されたGoogle App Engineアプリケーションの読み込みに、一部のユーザーで約30秒かかると報告しています。この動作は更新前には報告されていませんでした。どのような戦略を取るべきですか？

A. ISPと協力して問題を診断する。
B. サポートチケットを開き、問題を診断するためのネットワークキャプチャとフローデータを要求し、その後アプリケーションをロールバックする。
C. まず以前の既知の正常なリリースにロールバックし、その後、開発/テスト/ステージング環境でStackdriver Traceとロギングを使用して問題を診断する。
D. 以前の既知の正常なリリースにロールバックし、その後、静かな時間帯に再度リリースをプッシュして調査する。その後、Stackdriver Traceとロギングを使用して問題を診断する。

**正解: C**

**解説:**
本番環境で重大なパフォーマンス低下（30秒の読み込み）が発生した場合、最優先事項は**ユーザーへの影響を最小限に抑えること**です。そのため、まずApp Engineのバージョン管理機能を使って、即座に**以前の正常なバージョンにロールバック**します。サービスを復旧させた後、安全な**非本番環境（開発/テスト/ステージング）**で、問題のバージョンをデプロイし直し、**Cloud Trace**を使ってリクエストのどこで時間がかかっているのかを詳細に分析し、**Cloud Logging**で関連するエラーや警告がないかを確認します。これにより、本番環境に影響を与えることなく、根本原因を安全に調査できます。

-----

### <a name="no151"></a>**NO.151**

あなたの会社は、世界中に分散した数千の物理デバイスからデータを収集するアプリケーションをGoogle Cloudで実行しています。データはPub/Subに公開され、Dataflowパイプラインを介して単一のCloud Bigtableクラスタにリアルタイムでストリーミングされます。運用チームから、Cloud Bigtableクラスタにホットスポットが発生し、クエリが予想以上に時間がかかっていると通知がありました。問題を解決し、将来の発生を防ぐ必要があります。どうすべきですか？

A. クライアントにNodeJS APIの代わりにHBase APIを使用するようアドバイスする。
B. RowKey戦略を見直し、キーがアルファベット全体に均等に分散されるようにする。
C. 30日より古いレコードを削除する。
D. 現在のノード数を2倍にする。

**正解: B**

**解説:**
Cloud Bigtableでは、データは**行キー（RowKey）の辞書順でソートされて保存されます。行キーがタイムスタンプやデバイスIDのように連続した値で始まっていると、すべての書き込みがクラスタ内の単一のノードに集中してしまいます。これがホットスポット**の原因です。この問題を解決し、書き込み負荷をクラスタ全体に分散させるためには、行キーの設計を見直し、キーの先頭がランダムになるようにする必要があります。例えば、キーの先頭にデバイスIDのハッシュ値を追加したり、キーの要素の順序を逆にしたり（リバースドメインネームのように）するなどの戦略が有効です。

-----

### <a name="no152"></a>**NO.152**

あなたの会社は、機密データをCloud Storageバケットに保存しています。データアナリストは、バケットを読み取るためのIdentity Access Management（IAM）権限を持っています。データアナリストがオフィスネットワークの外部からバケット内のデータを取得するのを防ぎたいです。どうすべきですか？

A. 1. バケットを含むプロジェクトを含むVPC Service Controlsペリメータを作成する。 2. オフィスネットワークのCIDRでアクセスレベルを作成する。
B. 1. Virtual Private Cloud（VPC）ネットワーク内のすべてのインスタンスに対して、ソース範囲のファイアウォールルールを作成する。 2. オフィスネットワークのClassless Inter-domain Routing（CIDR）を使用する。
C. 1. バケットからIAM権限を削除するCloud Functionと、バケットにIAM権限を追加する別のCloud Functionを作成する。 2. 業務開始時に権限を追加し、業務終了時に権限を削除するようにCloud SchedulerでCloud Functionsをスケジュールする。
D. 1. オフィスネットワークへのCloud VPNを作成する。 2. オンプレミスホストに対してプライベートGoogleアクセスを構成する。

**正解: A**

**解説:**
**VPC Service Controls**は、Google Cloudサービス（Cloud Storageなど）へのアクセスを、定義されたネットワーク境界（ペリメータ）内に制限することで、データ漏洩を防ぐための機能です。

1.  保護したいCloud Storageバケットを含むプロジェクトを**サービスペリメータ**に追加します。
2.  **アクセスレベル**を定義し、許可するIPアドレス範囲としてオフィスネットワークのCIDRを指定します。
3.  このアクセスレベルをサービスペリメータに適用します。
    これにより、たとえIAM権限を持つユーザーであっても、オフィスネットワーク（許可されたIP範囲）の外部からCloud Storageバケットにアクセスしようとすると、VPC Service Controlsによってブロックされます。

-----

### <a name="no153"></a>**NO.153**

あなたはKubernetes Engine上のクラスタでWebアプリケーションを運用しています。ユーザーから、アプリケーションの特定の部分が応答しなくなったと報告されています。デプロイメントのすべてのPodが2秒後に再起動を繰り返していることに気づきました。アプリケーションはログを標準出力に書き込みます。問題の原因を見つけるためにログを調査したいです。どのアプローチを取ることができますか？

A. クラスタ内のノードとして機能している各Compute EngineインスタンスのStackdriverログを確認する。
B. 応答しないアプリケーション部分を提供している特定のKubernetes EngineコンテナのStackdriverログを確認する。
C. gcloud credentialsを使用してクラスタに接続し、いずれかのPod内のコンテナに接続してログを読み取る。
D. クラスタ内のノードとして機能している各Compute Engineインスタンスのシリアルポートログを確認する。

**正解: B**

**解説:**
(注：この問題はNo.6と同じです。)
Podが短時間で再起動を繰り返す「CrashLoopBackOff」状態は、通常、コンテナ内のアプリケーションが起動直後にエラーで終了していることが原因です。Kubernetes Engineでは、コンテナが標準出力（stdout）や標準エラー出力（stderr）に書き込んだログは、**Cloud Logging (旧Stackdriver)** に自動的に集約されます。したがって、Cloud Loggingで該当コンテナのログを確認することが、エラーの原因を特定する最も直接的で標準的な方法です。

-----

### <a name="no154"></a>**NO.154**

あなたは、多くの依存関係を持つPython Webアプリケーションを運用しており、本番環境で動作させるには0.1 CPUコアと128MBのメモリが必要です。マシンの使用率を監視し、最大化したいです。また、新しいバージョンのアプリケーションを確実にデプロイしたいです。どの手順を踏むべきですか？

A. 1) f1-microタイプのマシンでマネージドインスタンスグループを作成する。 2) 起動スクリプトを使用してリポジトリをクローンし、本番ブランチをチェックアウトし、依存関係をインストールし、Pythonアプリを起動する。 3) インスタンスを再起動して、新しい本番リリースを自動的にデプロイする。
B. 1) n1-standard-1タイプのマシンでマネージドインスタンスグループを作成する。 2) すべての依存関係を含み、Pythonアプリを自動的に起動する本番ブランチからCompute Engineイメージをビルドする。 3) Compute Engineイメージを再ビルドし、インスタンステンプレートを更新して新しい本番リリースをデプロイする。
C. 1) n1-standard-1タイプのマシンでKubernetes Engineクラスタを作成する。 2) すべての依存関係を持つ本番ブランチからDockerイメージをビルドし、バージョン番号でタグ付けする。 3) imagePullPolicyを"IfNotPresent"に設定したKubernetes Deploymentをステージングネームスペースに作成し、テスト後に本番ネームスペースにプロモートする。
D. 1) n1-standard-4タイプのマシンでKubernetes Engine（GKE）クラスタを作成する。 2) すべての依存関係を持つマスターブランチからDockerイメージをビルドし、「latest」でタグ付けする。 3) imagePullPolicyを「Always」に設定したKubernetes Deploymentをデフォルトネームスペースに作成する。ポッドを再起動して、新しい本番リリースを自動的にデプロイする。

**正解: C**

**解説:**
PDFの回答Dは、「latest」タグの使用や手動でのPod再起動など、ベストプラクティスに反します。
要件（リソース使用率の最大化、確実なデプロイ）を最も満たすのはコンテナベースのアプローチです。

  * **リソース使用率の最大化**: GKEは、1つのVMノード上に多数の小さなコンテナ（0.1 CPU, 128MBメモリ）を高密度に配置（**Bin Packing**）できるため、VMリソースを非常に効率的に使用できます。
  * **確実なデプロイ**:
    1.  **Dockerイメージ**にアプリケーションと依存関係をパッケージ化することで、不変でポータブルなアーティファクトを作成します。
    2.  **バージョン番号でタグ付け**することで、どのバージョンのコードがデプロイされているかを明確に追跡できます。
    3.  **Kubernetes Deployment**を使用すると、ローリングアップデートやロールバックを宣言的に、かつ確実に行うことができます。
    4.  **ステージング -> 本番**というプロモーションプロセスは、リリースの安全性を高めるための標準的なプラクティスです。

-----

### <a name="no155"></a>**NO.155**

あなたの会社は、マルチペタバイトのデータセットをクラウドに移行する計画です。データセットは24時間利用可能である必要があります。あなたのビジネスアナリストはSQLインターフェースの使用経験しかありません。分析の容易さを最適化するために、データをどのように保存すべきですか？

A. データをGoogle BigQueryにロードする。
B. データをGoogle Cloud SQLに挿入する。
C. フラットファイルをGoogle Cloud Storageに置く。
D. データをGoogle Cloud Datastoreにストリーミングする。

**正解: A**

**解説:**
要件は「マルチペタバイト」のデータ、「SQLインターフェース」での分析、そして「24時間利用可能」です。

  * **Google BigQuery**は、まさにこの目的のために設計された、サーバーレスで高度にスケーラブルなエンタープライズデータウェアハウスです。
  * ペタバイト級のデータを扱う能力を持ち、標準的な**SQL**を使用してデータを分析できます。
  * フルマネージドで高可用性なサービスであり、インフラ管理なしで24時間利用可能です。
    Cloud SQL (B) や Cloud Datastore (D) は、このデータ規模には対応できません。Cloud Storage (C) はストレージとしては適していますが、直接SQLで分析する機能はありません（BigQueryから外部テーブルとして参照することは可能）。

-----

### <a name="no156"></a>**NO.156**

あなたの会社は、複数のCompute Engineインスタンスで実行されているアプリケーションを持っています。1日に1TBのログが生成されます。コンプライアンス上の理由から、ログは少なくとも2年間保持する必要があります。ログは30日間アクティブなクエリで利用可能である必要があります。その後は、監査目的で保持するだけで十分です。コンプライアンスに準拠し、コストを最小限に抑え、Google推奨のプラクティスに従うストレージソリューションを実装したいです。どうすべきですか？

A. 1. すべてのインスタンスにCloud Opsエージェントをインストールする。 2. パーティション分割されたBigQueryテーブルにログをエクスポートするためのシンクを作成する。 3. `time_partitioning_expiration`を30日に設定する。
B. 1. すべてのインスタンスにCloud Opsエージェントをインストールする。 2. リージョンCloud Storageバケットにログをエクスポートするためのシンクを作成する。 3. 1か月後にファイルをColdline Cloud Storageバケットに移動するためのオブジェクトライフサイクルルールを作成する。 4. バケットレベルで保持ポリシーを構成してロックを作成する。
C. 1. すべてのインスタンスで実行される日次のcronジョブを作成し、ログをパーティション分割されたBigQueryテーブルにアップロードする。 2. `time_partitioning_expiration`を30日に設定する。
D. 1. すべてのインスタンスで実行される日次のcronジョブを作成し、ログをCloud Storageバケットにアップロードする。 2. リージョンCloud Storageバケットにログをエクスポートするためのシンクを作成する。 3. 1か月後にファイルをColdline Cloud Storageバケットに移動するためのオブジェクトライフサイクルルールを作成する。

**正解: B**

**解説:**
この問題は、異なる期間で異なるアクセス要件を持つデータの階層化ストレージ戦略を問うています。

1.  **ログ収集**: Cloud Opsエージェントをインストールし、**Cloud Logging**にログを集約するのが標準です。
2.  **ログのエクスポート**: Cloud Loggingから**Cloud Storage**にログをエクスポートするシンクを作成します。
3.  **階層化**:
      * **最初の30日間 (アクティブクエリ)**: Cloud Storageの**Standard**クラスに保存します。このデータは、必要に応じてBigQueryから外部テーブルとしてクエリすることも可能です。
      * **30日後〜2年間 (監査用アーカイブ)**: Cloud Storageの**オブジェクトライフサイクル管理**ルールを使用して、30日経過したオブジェクトを自動的に低コストの**Coldline**ストレージクラスに移動させます。
4.  **保持要件**: 2年間の保持を確実にするため、バケットに**保持ポリシー**と**ロック**を設定します。
    この方法が、要件を満たしつつ最もコスト効率が良いです。

-----

### <a name="no157"></a>**NO.157**

あなたの会社は、世界中に分散したユーザーが写真をアップロードし、他の選択したユーザーと共有できる新しいアプリケーションを開発しています。アプリケーションは数百万の同時ユーザーをサポートします。開発者が基盤となるインフラストラクチャを作成・維持することなく、コードの構築だけに集中できるようにしたいです。どのサービスを使用してアプリケーションをデプロイすべきですか？

A. App Engine
B. Cloud Endpoints
C. Compute Engine
D. Google Kubernetes Engine

**正解: A**

**解説:**
PDFの回答Dも可能ですが、Aの方がより要件に合致しています。
要件の核心は「**開発者がコードの構築だけに集中できるようにしたい（No-Ops）**」と「**数百万の同時ユーザーをサポート（自動スケーリング）**」です。

  * **App Engine**（特にStandard Environment）は、まさにこのためのサーバーレスプラットフォームです。開発者はコードをデプロイするだけで、Googleがトラフィックに応じたスケーリング、パッチ適用、インフラ管理のすべてを自動的に行います。
  * GKE(D)も強力なスケーラビリティを持ちますが、Dockerfileの作成、クラスタ構成、Deploymentの管理など、App Engineよりも多くのインフラ関連の知識と運用が必要です。
    この問題の要件に対しては、App Engineが最もシンプルな解決策です。

-----

### <a name="no158"></a>**NO.158**

あなたはGoogle Kubernetes Engine上でマイクロサービスアプリケーションを開発しています。テスト中に、特定のマイクロサービスが突然クラッシュした場合のアプリケーションの動作を検証したいです。どうすべきですか？

A. Kubernetesクラスタのノードの1つにテイントを追加する。特定のマイクロサービスに対して、テイントされたノードの名前を値として持つポッドアンチアフィニティラベルを構成する。
B. 障害のある動作をシミュレートしたい特定のマイクロサービスに対して、Istioのフォールトインジェクションを使用する。
C. 動作を観察するためにKubernetesクラスタのノードの1つを破棄する。
D. クラッシュするマイクロサービスからトラフィックを迂回させるために、Istioのトラフィック管理機能を構成する。

**正解: B**

**解説:**
マイクロサービスアーキテクチャの回復力をテストする手法として**カオスエンジニアリング**があります。**Istio**サービスメッシュの**フォールトインジェクション**機能は、このテストを簡単に行うためのツールです。Istioを使用すると、特定のサービスに対して、意図的に遅延（HTTP delay）を注入したり、指定した割合のリクエストを失敗させたり（HTTP abort）することができます。これにより、特定のマイクロサービスがクラッシュしたり、遅延したりした場合に、システム全体がどのように反応するか（例: リトライは正しく機能するか、サーキットブレーカーは開くか）を、実際にコードをクラッシュさせたりノードを破壊したりすることなく、安全にテストできます。

-----

### <a name="no159"></a>**NO.159**

あるリードエンジニアが、レガシーデータセンターに仮想マシンをデプロイするカスタムツールを作成しました。彼はカスタムツールを新しいクラウド環境に移行したいと考えています。あなたはGoogle Cloud Deployment Managerの採用を提唱したいです。Cloud Deployment Managerに移行する際の2つのビジネスリスクは何ですか？（2つ選択）

A. Cloud Deployment ManagerはPythonを使用する。
B. Cloud Deployment ManagerのAPIは将来廃止される可能性がある。
C. Cloud Deployment Managerは会社のエンジニアにとって馴染みがない。
D. Cloud Deployment Managerは実行するためにGoogle APIのサービスアカウントが必要である。
E. Cloud Deployment Managerはクラウドリソースを永久に削除するために使用できる。
F. Cloud Deployment ManagerはGoogle Cloudリソースの自動化のみをサポートする。

**正解: C, F**

**解説:**

  * **C (学習コスト)**: 会社のエンジニアが現在使用しているカスタムツールや他のIaCツール（Terraformなど）に慣れている場合、新しいツールである**Cloud Deployment Manager**の構文（YAML, Jinja, Python）や概念を学ぶための**学習コスト**と時間が必要になります。これはビジネス上のリスク（プロジェクトの遅延など）につながる可能性があります。
  * **F (ベンダーロックイン)**: Cloud Deployment Managerは**Google Cloudリソース専用**のプロビジョニングツールです。将来、マルチクラウド戦略を採用し、AWSやAzureなどの他のクラウドプロバイダーのリソースもコードで管理する必要が出てきた場合、Deployment Managerで作成した構成は再利用できず、Terraformのようなマルチクラウド対応ツールで書き直す必要が生じます。これは将来の柔軟性を損なうリスクです。

-----

### <a name="no160"></a>**NO.160**

Google Compute Engine上の本番データベース仮想マシンには、データファイル用にext4フォーマットの永続ディスクがあります。データベースのストレージスペースがなくなりそうです。最小限のダウンタイムで問題を修正するにはどうすればよいですか？

A. Cloud Platform Consoleで永続ディスクのサイズを増やし、Linuxでresize2fsコマンドを使用する。
B. 仮想マシンをシャットダウンし、Cloud Platform Consoleを使用して永続ディスクのサイズを増やし、仮想マシンを再起動する。
C. Cloud Platform Consoleで永続ディスクのサイズを増やし、Linuxのfdiskコマンドで新しいスペースが使用できることを確認する。
D. Cloud Platform Consoleで仮想マシンに新しい永続ディスクを作成してアタッチし、フォーマットしてマウントし、データベースサービスを構成してファイルを新しいディスクに移動する。
E. Cloud Platform Consoleで永続ディスクのスナップショットを作成し、スナップショットを新しいより大きなディスクに復元し、古いディスクをアンマウントし、新しいディスクをマウントし、データベースサービスを再起動する。

**正解: A**

**解説:**
Compute Engineの永続ディスクは、VMを**実行したまま**サイズを拡張（オンラインリサイズ）できます。

1.  まず、Google Cloud Consoleまたはgcloudコマンドで、対象の永続ディスクのサイズを増やします。
2.  次に、VMにSSHでログインし、OSに新しいディスクサイズを認識させ、ファイルシステムを拡張する必要があります。ext4ファイルシステムの場合、`resize2fs`コマンドを実行するだけで、マウントされたファイルシステムをオンラインで拡張できます。
    この方法は、VMやデータベースを停止する必要がないため、**ダウンタイムがゼロ**（または非常に短い）で済みます。

-----

### <a name="no161"></a>**NO.161**

あなたの会社は、科学計算を行うステートレスなWeb APIを持っています。Web APIは単一のGoogle Kubernetes Engine（GKE）クラスタで実行されています。クラスタは現在us-central1にデプロイされています。会社はアジアの顧客にAPIを提供するように拡大しました。アジアのユーザーのレイテンシを削減したいです。どうすべきですか？

A. Cloud CDNが有効なグローバルHTTP(s)ロードバランサを使用する。
B. asia-southeast1に2番目のGKEクラスタを作成し、両方のAPIをLoadBalancerタイプのServiceを使用して公開する。公開IPをCloud DNSゾーンに追加する。
C. クラスタ内のアプリケーションに割り当てられたメモリとCPUを増やす。
D. asia-southeast1に2番目のGKEクラスタを作成し、kubemciを使用してグローバルHTTP(s)ロードバランサを作成する。

**正解: D**

**解説:**
世界中のユーザーに対して低レイテンシを提供するには、ユーザーに近い複数のリージョンにアプリケーションをデプロイし、**グローバルロードバランサ**でトラフィックを最適なリージョンに誘導するのがベストプラクティスです。

  * **複数リージョンへのデプロイ**: `us-central1`に加えて、アジアのユーザー向けに`asia-southeast1`に2つ目のGKEクラスタを作成します。
  * **Multi Cluster Ingress**: 複数のGKEクラスタにまたがる単一のアプリケーションとして公開するための機能が**Multi Cluster Ingress**です。`kubemci`コマンドラインツールやConfig Connectorを使用してこれを構成すると、Google CloudのグローバルHTTP(S)ロードバランサが自動的に作成され、ユーザーに最も近いGKEクラスタにトラフィックがルーティングされるようになります。

-----

### <a name="no162"></a>**NO.162**

あなたの組織は、インスタンスへの外部IPアドレスの使用を承認されたインスタンスのみに制限することを決定しました。この要件をすべてのVirtual Private Cloud（VPC）で強制したいです。どうすべきですか？

A. すべてのVPCでデフォルトルートを削除する。承認されたすべてのインスタンスを、インターネットゲートウェイへのデフォルトルートを持つ新しいサブネットに移動する。
B. カスタムモードで新しいVPCを作成する。承認されたインスタンス用に新しいサブネットを作成し、この新しいサブネットにインターネットゲートウェイへのデフォルトルートを設定する。
C. 外部IPアドレスの必要性を完全になくすために、Cloud NATソリューションを実装する。
D. `constraints/compute.vmExternalIpAccess`に制約を持つ組織ポリシーを設定する。`allowedValues`リストに承認されたインスタンスをリストアップする。

**正解: D**

**解説:**
組織全体で一貫したポリシーを強制するための最も強力で推奨される方法は、**組織ポリシーサービス**を使用することです。`constraints/compute.vmExternalIpAccess`制約を使用すると、組織、フォルダ、またはプロジェクトレベルで、どのVMインスタンスが外部IPアドレスを持つことを許可されるかを制御できます。ポリシーを`Deny all`に設定し、例外として特定のインスタンスを`allowedValues`リストに追加することで、要件を確実に、かつ一元的に強制することができます。

-----

### <a name="no163"></a>**NO.163**

あなたは会社のGoogle Cloud環境を担当しています。複数の部門が独自のプロジェクトへのアクセスを必要とし、各部門内のメンバーは同じプロジェクトの責任を持ちます。各部門のプロジェクトが開始および終了するにつれて、IAM権限のメンテナンスを最小限に抑え、概要を最大化するためにGoogle Cloud環境を構造化したいです。Google推奨のプラクティスに従いたいです。どうすべきですか？

A. 部門ごとにGoogleグループを作成し、すべての部門メンバーをそれぞれのグループに追加する。部門ごとにフォルダを作成し、それぞれのグループに必要なIAM権限をフォルダレベルで付与する。プロジェクトをそれぞれのフォルダの下に追加する。
B. すべての部門メンバーに、それぞれのプロジェクトに必要なIAM権限を付与する。
C. 部門ごとにGoogleグループを作成し、すべての部門メンバーをそれぞれのグループに追加する。各グループにそれぞれのプロジェクトに必要なIAM権限を付与する。
D. 部門ごとにフォルダを作成し、部門のそれぞれのメンバーに必要なIAM権限をフォルダレベルで付与する。各部門のすべてのプロジェクトをそれぞれのフォルダの下に構造化する。

**正解: A**

**解説:**
これは、Google CloudにおけるIAMとリソース階層のベストプラクティスを組み合わせたものです。

1.  **Googleグループの使用**: ユーザーを直接リソースに割り当てるのではなく、**Googleグループ**（例: `gcp-finance-team@example.com`）を作成し、ユーザーをそのグループのメンバーにします。これにより、人の異動があった場合でも、グループのメンバーシップを変更するだけで済み、IAMポリシーの変更は不要になります。
2.  **フォルダの使用**: 部門ごとに**フォルダ**を作成し、プロジェクトをその中に配置します。
3.  **継承の活用**: IAMロールを個々のプロジェクトではなく、**フォルダレベルでGoogleグループに付与**します。権限はフォルダからプロジェクトに自動的に継承されます。
    このアプローチにより、管理が大幅に簡素化され、権限の可視性が向上し、メンテナンスが最小限になります。

-----

### <a name="no164"></a>**NO.164**

あなたは、ミッションクリティカルなアプリケーションの災害計画をテストする手順を開発する必要があります。Google推奨のプラクティスとGCP内のネイティブ機能を使用したいです。どうすべきですか？

A. Deployment Managerを使用してサービスのプロビジョニングを自動化する。アクティビティログを使用してテストを監視およびデバッグする。
B. Deployment Managerを使用してプロビジョニングを自動化する。Stackdriverを使用してテストを監視およびデバッグする。
C. gcloudスクリプトを使用してサービスのプロビジョニングを自動化する。アクティビティログを使用してテストを監視およびデバッグする。
D. 自動化されたスクリプトを使用してサービスのプロビジョニングを自動化する。アクティビティログを使用してテストを監視およびデバッグする。

**正解: B**

**解説:**
DR計画のテストには、**自動化**と**監視**が不可欠です。

  * **プロビジョニングの自動化**: **Cloud Deployment Manager**やTerraformのようなInfrastructure as Code (IaC) ツールを使用して、DRサイトのインフラ（VM, ロードバランサ, データベースなど）を迅速かつ確実にプロビジョニングするプロセスを自動化します。これにより、テストの再現性が保証されます。
  * **監視とデバッグ**: フェイルオーバーのテスト中、アプリケーションのパフォーマンスやエラーを監視するには、**Cloud Monitoring と Cloud Logging (旧Stackdriver)** がネイティブで最も強力なツールです。メトリクス、ログ、アラートを centralized に確認し、テストが成功したか、どこに問題があったかをデバッグします。

-----

### <a name="no165"></a>**NO.165**

あなたは、会社のWebホスティングプラットフォームでの誤った本番デプロイメントによる計画外のロールバックの数を減らす必要があります。QA/テストプロセスの改善により、80%の削減が達成されました。ロールバックをさらに減らすために、どの追加の2つのアプローチを取ることができますか？（2つ選択）

A. ブルー/グリーンデプロイメントモデルを導入する。
B. QA環境をカナリアリリースに置き換える。
C. モノリシックプラットフォームをマイクロサービスに分割する。
D. プラットフォームのリレーショナルデータベースシステムへの依存を減らす。
E. プラットフォームのリレーショナルデータベースシステムをNoSQLデータベースに置き換える。

**正解: A, C**

**解説:**

  * **A (Blue/Greenデプロイメント)**: 新しいバージョン（Green）を本番環境に、既存のバージョン（Blue）と並行してデプロイします。テストが完了したら、ロードバランサでトラフィックを一瞬でGreenに切り替えます。もし問題が発見されても、トラフィックを即座にBlueに戻すだけでロールバックが完了するため、リスクが大幅に低減します。
  * **C (マイクロサービス化)**: モノリシックなアプリケーションを小さな独立した**マイクロサービス**に分割すると、変更の影響範囲が限定されます。あるサービスへの変更が他のサービスに予期せぬ影響を与えるリスクが減り、問題が発生した場合でも、影響を受けるのはその小さなサービスのみとなります。これにより、システム全体に影響を及ぼすような大規模なロールバックの必要性が減少します。

-----

### <a name="no166"></a>**NO.166**

データベース管理チームが、Google Compute Engine上で実行されている新しいデータベースサーバーのパフォーマンスを向上させる手助けを求めてきました。このデータベースは、パフォーマンス統計のインポートと正規化のためのもので、Debian Linux上でMySQLを実行しています。彼らは、80GBのSSD永続ディスクを持つn1-standard-8仮想マシンを使用しています。このシステムからより良いパフォーマンスを得るために、何をすべきですか？

A. 仮想マシンのメモリを64GBに増やす。
B. PostgreSQLを実行する新しい仮想マシンを作成する。
C. SSD永続ディスクを動的に500GBにリサイズする。
D. パフォーマンスメトリクスウェアハウスをBigQueryに移行する。
E. すべてのバッチジョブを変更して、データベースへのバルクインサートを使用するようにする。

**正解: C**

**解説:**
Compute Engineの**SSD永続ディスク**のパフォーマンス（IOPSとスループット）は、**ディスクのサイズに比例して向上**します。80GBのSSD永続ディスクのパフォーマンスは比較的低く、大規模なデータのインポートや正規化のようなI/O集中型のワークロードではボトルネックになりがちです。ディスクサイズを500GBに増やすことで、ディスクのIOPSとスループットが大幅に向上し、データベースのパフォーマンスが直接的に改善されます。これは、最も簡単で効果的な最初のステップです。

-----

### <a name="no167"></a>**NO.167**

あなたはCloud MonitoringワークスペースでGoogle Kubernetes Engine（GKE）クラスタを監視しています。サイトリライアビリティエンジニア（SRE）として、インシデントを迅速にトリアージする必要があります。どうすべきですか？

A. Cloud Monitoringワークスペースの事前定義されたダッシュボードをナビゲートし、その後メトリクスを追加し、アラートポリシーを作成する。
B. Cloud Monitoringワークスペースの事前定義されたダッシュボードをナビゲートし、カスタムメトリクスを作成し、Compute Engineインスタンスにアラートソフトウェアをインストールする。
C. GKEノードからメトリクスを収集するシェルスクリプトを書き、これらのメトリクスをPub/Subトピックに公開し、データをBigQueryにエクスポートし、Data Studioダッシュボードを作成する。
D. インシデントごとにCloud Monitoringワークスペースにカスタムダッシュボードを作成し、その後メトリクスを追加し、アラートポリシーを作成する。

**正解: A**

**解説:**
インシデント対応では、迅速な状況把握が重要です。**Cloud Monitoring**には、GKE向けに最適化された**事前定義済みのダッシュボード**が用意されています。これには、クラスタ、ノード、Pod、コンテナレベルの主要なメトリクス（CPU, メモリ, ディスク使用率など）が予め表示されており、インシデントのトリアージ（問題の切り分け）を始めるのに最適です。まずこのダッシュボードで異常の概要を掴み、必要に応じて特定のメトリクスを深掘りしたり、将来のインシデントに備えて**アラートポリシー**を作成したりするのが、効率的で標準的なSREのワークフローです。

-----

### <a name="no168"></a>**NO.168**

あなたの会社は、複数のマイクロサービスを実行するAnthosクラスタ（旧Anthos GKE）にアプリケーションをデプロイしています。クラスタにはAnthos Service MeshとAnthos Config Managementの両方が構成されています。エンドユーザーから、アプリケーションの応答が非常に遅いと報告がありました。遅延の原因となっているマイクロサービスを特定したいです。どうすべきですか？

A. Cloud ConsoleのService Meshの視覚化を使用して、マイクロサービス間のテレメトリを検査する。
B. Anthos Config Managementを使用して、関連するクラスタを選択するClusterSelectorを作成する。Google Kubernetes EngineのGoogle Cloud Consoleページで、ワークロードを表示し、クラスタでフィルタリングする。フィルタリングされたワークロードの構成を検査する。
C. Anthos Config Managementを使用して、関連するクラスタネームスペースを選択するnamespaceSelectorを作成する。Google Kubernetes EngineのGoogle Cloud Consoleページで、ワークロードにアクセスし、ネームスペースでフィルタリングする。フィルタリングされたワークロードの構成を検査する。
D. リクエストレイテンシを収集するために、デフォルトのistioプロファイルを使用してistioを再インストールする。Cloud Consoleでマイクロサービス間のテレメトリを評価する。

**正解: A**

**解説:**
**Anthos Service Mesh** (Istioベース) は、マイクロサービス間の通信を監視し、制御するための機能を提供します。その主な利点の一つが**オブザーバビリティ（可観測性）です。Google Cloud ConsoleのService Mesh UI**では、サービス間のトラフィック、レイテンシ、エラー率などの**テレメトリデータ**を視覚的に表示できます。サービスグラフなどを見ることで、どのサービス呼び出しでレイテンシが急増しているかを一目で特定でき、パフォーマンスのボトルネックとなっているマイクロサービスを迅速に見つけ出すことができます。

-----

### <a name="no169"></a>**NO.169**

災害復旧計画の実施の一環として、あなたの会社は、プライベートデータセンターからGCPプロジェクトへの本番MySQLデータベースのレプリケーションを試みています。Google Cloud VPN接続を使用していますが、レプリケーションを妨げるレイテンシの問題と少量のパケットロスが発生しています。どうすべきですか？

A. UDPを使用するようにレプリケーションを構成する。
B. Google Cloud Dedicated Interconnectを構成する。
C. Google Cloud SQLを使用してデータベースを毎日復元する。
D. 追加のVPN接続を追加し、それらをロードバランスする。
E. 複製されたトランザクションをGoogle Cloud Pub/Subに送信する。

**正解: B**

**解説:**
データベースのレプリケーションは、安定した低レイテンシとパケットロスのない接続を必要とします。Cloud VPNはパブリックインターネットを経由するため、パフォーマンスが不安定で、レイテンシやパケットロスが発生しがちです。**Cloud Dedicated Interconnect**は、オンプレミスとGCP間を専用の物理回線で接続するため、インターネットを経由せず、高帯域幅、低レイテンシ、そして非常に安定した接続を提供します。これにより、データベースレプリケーションのようなミッションクリティカルなワークロードの要件を満たすことができます。

-----

### <a name="no170"></a>**NO.170**

あなたの会社は、いくつかの重要なファイルをCloud Storageにアップロードする計画です。アップロード完了後、アップロードされたコンテンツがオンプレミスにあるものと同一であることを確認したいです。このチェックを実行するためのコストと労力を最小限に抑えたいです。どうすべきですか？

A. 1) `gsutil -m`を使用してすべてのファイルをCloud Storageにアップロードする。 2) `gsutil cp`を使用してアップロードされたファイルをダウンロードする。 3) Linux `diff`を使用してファイルの内容を比較する。
B. 1) `gsutil -m`を使用してすべてのファイルをCloud Storageにアップロードする。 2) CRC32Cハッシュを計算するカスタムJavaアプリケーションを開発する。 3) `gsutil ls -L gs://[YOUR_BUCKET_NAME]`を使用してアップロードされたファイルのCRC32Cハッシュを収集する。 4) ハッシュを比較する。
C. 1) Linux `shasum`を使用してアップロードしたいファイルのダイジェストを計算する。 2) `gsutil -m`を使用してすべてのファイルをCloud Storageにアップロードする。 3) `gsutil cp`を使用してアップロードされたファイルをダウンロードする。 4) Linux `shasum`を使用してダウンロードされたファイルのダイジェストを計算する。 5) ハッシュを比較する。
D. 1) `gsutil -m`を使用してすべてのファイルをCloud Storageにアップロードする。 2) `gsutil hash -c FILE_NAME`を使用してすべてのオンプレミスファイルのCRC32Cハッシュを生成する。 3) `gsutil ls -L gs://[YOUR_BUCKET_NAME]`を使用してアップロードされたファイルのCRC32Cハッシュを収集する。 4) ハッシュを比較する。

**正解: D**

**解説:**
データの完全性を検証する最も効率的な方法は、ファイル全体を再ダウンロードする(A, C)のではなく、ハッシュ値を比較することです。

  * **gsutil**には、ファイルのCRC32Cハッシュ値を計算する`gsutil hash`コマンドが組み込まれています。
  * Cloud Storageも、アップロードされた各オブジェクトのCRC32Cハッシュ値を自動的に計算し、メタデータとして保持しています。この値は`gsutil ls -L`で取得できます。
  * したがって、オンプレミスで`gsutil hash`を実行し、Cloud Storage側で`gsutil ls -L`を実行して、両方のハッシュ値を比較するのが、追加の開発(B)や非効率なダウンロード(A, C)を伴わない、最も簡単でコストのかからない方法です。

-----

### <a name="no171"></a>**NO.171**

一日の高トラフィックの時間帯に、あなたのリレーショナルデータベースの1つがクラッシュしましたが、レプリカがマスターに昇格されることはありませんでした。将来これを避けたいです。どうすべきですか？

A. 別のデータベースを使用する。
B. データベースにより大きなインスタンスを選択する。
C. データベースのスナップショットをより定期的に作成する。
D. データベースの定期的にスケジュールされたフェイルオーバーを実装する。

**正解: D**

**解説:**
高可用性（HA）システムやディザスタリカバリ（DR）システムが実際に障害時に機能することを保証する唯一の方法は、**定期的にテスト**することです。**定期的に計画的なフェイルオーバー訓練**を実施することで、フェイルオーバーのプロセスが正しく構成されているか、自動化スクリプトは正常に動作するか、アプリケーションは新しいマスターに正しく再接続できるか、といった点を検証できます。これにより、設定の不備や潜在的な問題を実際の障害が発生する前に発見し、修正することができます。

-----

### <a name="no172"></a>**NO.172**

あなたの会社は、Pub/Subからメッセージを取得し、Firestoreに保存するKubernetesアプリケーションを持っています。アプリケーションは単純なので、単一のポッドとしてデプロイされました。インフラチームはPub/Subのメトリクスを分析し、アプリケーションがリアルタイムでメッセージを処理できていないことを発見しました。ほとんどのメッセージは処理されるまでに数分待機しています。I/O集約的な処理プロセスをスケールさせる必要があります。どうすべきですか？

A. `subscription/push_request_count`メトリックに基づいてKubernetesの自動スケーリングを構成する。
B. Kubernetesクラスタを作成する際に`--enable-autoscaling`フラグを使用する。
C. `subscription/num_undelivered_messages`メトリックに基づいてKubernetesの自動スケーリングを構成する。
D. `kubectl autoscale deployment APP_NAME --max 6 --min 2 --cpu-percent 50`を使用してKubernetesの自動スケーリングデプロイメントを構成する。

**正解: C**

**解説:**
PDFの回答AはPushサブスクリプションのメトリクスであり、このシナリオ（Pull）には不適切です。
アプリケーションがメッセージを処理しきれていない場合、Pub/Subサブスクリプションに**未配信のメッセージ（バックログ）が溜まっていきます。このバックログの数を示すメトリクスが`pubsub.googleapis.com/subscription/num_undelivered_messages`です。
KubernetesのHorizontal Pod Autoscaler (HPA)** を、この**外部メトリクス**（External Metric）に基づいて構成するのが最適な方法です。未配信メッセージ数が増加したら、HPAが自動的にPodのレプリカ数を増やして処理能力を向上させ、メッセージ数が減少したらPodを減らしてコストを最適化します。CPUベースのスケーリング(D)は、I/Oバウンドなこのワークロードには適していません。

-----

### <a name="no173"></a>**NO.173**

あなたは、レガシーなストリーミングバックエンドデータAPIのためのグローバルにスケールするフロントエンドを開発しています。このAPIは、適切な処理のために、厳密な時系列順で、繰り返しデータなしでイベントを受け取ることを期待しています。データの保証された一度だけのFIFO（先入れ先出し）配信を確実にするために、どの製品をデプロイすべきですか？

A. Cloud Pub/Sub単体
B. Cloud Pub/SubからCloud DataFlowへ
C. Cloud Pub/SubからStackdriverへ
D. Cloud Pub/SubからCloud SQLへ

**正解: B**

**解説:**

  * **Cloud Pub/Sub**: Pub/Subには**順序指定配信（Ordering Keys）**機能があり、同じキーを持つメッセージが順序通りに配信されることを保証します。しかし、Pub/Sub自体は「少なくとも一度」の配信を保証するため、ネットワークの問題などでメッセージが重複して配信される可能性があります。
  * **Cloud Dataflow**: Dataflowは、Pub/Subからのストリームを処理する際に、重複排除を自動的に行い、**厳密に一度だけ（exactly-once）の処理セマンティクスを提供します。
    したがって、順序性（Pub/Sub）と厳密に一度だけの処理**（Dataflow）を組み合わせることで、「保証された一度だけのFIFO配信」という要件を満たすことができます。

-----

### <a name="no174"></a>**NO.174**

あなたの会社は全国的に事業を展開しており、時間に制約のないものを含む複数のバッチワークロードにGCPを使用する予定です。また、HIPAA認定のGCPサービスを使用し、サービスコストを管理する必要があります。Googleのベストプラクティスを満たすように設計するにはどうすればよいですか？

A. コストを削減するためにプリエンプティブルVMをプロビジョニングする。HIPAAに準拠していないすべてのGCPサービスとAPIの使用を中止する。
B. コストを削減するためにプリエンプティブルVMをプロビジョニングする。HIPAAに準拠していないすべてのGCPサービスとAPIを無効にしてから使用を中止する。
C. コストを削減するために同じリージョンに標準VMをプロビジョニングする。HIPAAに準拠していないすべてのGCPサービスとAPIの使用を中止する。
D. コストを削減するために同じリージョンに標準VMをプロビジョニングする。HIPAAに準拠していないすべてのGCPサービスとAPIを無効にしてから使用を中止する。

**正解: B**

**解説:**

  * **コスト管理**: 「時間に制約のないバッチワークロード」は、いつ中断されても問題ないため、標準VMより最大80%安価な**プリエンプティブルVM (Spot VMs)** の最適なユースケースです。これにより、大幅なコスト削減が可能です。
  * **HIPAAコンプライアンス**: HIPAAのような規制要件がある場合、対象となるデータを扱う環境では、GoogleがHIPAAの対象として指定しているサービスのみを使用する必要があります。HIPAA対象外のサービスを誤って使用するリスクを避けるため、それらのサービスを**無効化**し、使用しないようにポリシーを徹底することがベストプラクティスです。

-----

### <a name="no175"></a>**NO.175**

あなたの会社は、ユーザーがお気に入りの音楽を再生できるアプリケーションをCompute Engineで実行しています。インスタンス数は固定です。ファイルはCloud Storageに保存され、データはユーザーに直接ストリーミングされます。ユーザーから、人気の曲を再生しようとすると、成功するまでに複数回試行する必要があることがあると報告されています。アプリケーションのパフォーマンスを向上させる必要があります。どうすべきですか？

A. 1. 人気の曲をBLOBとしてCloud SQLにコピーする。 2. Cloud Storageが過負荷のときにCloud SQLからデータを取得するようにアプリケーションコードを更新する。
B. 1. Compute Engineインスタンスでマネージドインスタンスグループを作成する。 2. グローバルロードバランサを作成し、2つのバックエンドで構成する：マネージドインスタンスグループとCloud Storageバケット。 3. バケットバックエンドでCloud CDNを有効にする。
C. 1. gcsfuseを使用して、すべてのバックエンドCompute EngineインスタンスにCloud Storageバケットをマウントする。 2. バックエンドCompute Engineインスタンスから直接音楽ファイルを配信する。
D. 1. Cloud Filestore NFSボリュームを作成し、バックエンドCompute Engineインスタンスにアタッチする。 2. Cloud Filestoreに人気の曲をダウンロードする。 3. バックエンドCompute Engineインスタンスから直接音楽ファイルを配信する。

**正解: B**

**解説:**
人気の曲へのアクセスが集中し、Cloud Storageがスロットリング（レート制限）を起こしている可能性があります。この問題を解決し、パフォーマンスを向上させるための最適な方法は**キャッシング**です。

  * **Cloud CDN (Content Delivery Network)** は、Cloud Storageバケットをオリジンとして設定できます。
  * 外部HTTP(S)ロードバランサと組み合わせることで、一度アクセスされた人気の曲は世界中のGoogleのエッジキャッシュに保存されます。
  * 2回目以降のアクセスは、オリジンのCloud Storageまで到達せず、ユーザーに最も近いエッジキャッシュから直接配信されるため、レイテンシが大幅に短縮され、Cloud Storageへの負荷も軽減されます。

-----

### <a name="no176"></a>**NO.176**

あなたはGoogle Cloudでデータウェアハウスを設計しており、機密データをBigQueryに保存したいと考えています。あなたの会社は、Google Cloudの外部で暗号化キーを生成することを要求しています。ソリューションを実装する必要があります。どうすべきですか？

A. Cloud Key Management Service（Cloud KMS）で新しいキーを生成する。顧客管理キーオプションを使用してCloud Storageにすべてのデータを保存し、作成したキーを選択する。データを復号化してBigQueryデータセットに保存するためのDataflowパイプラインを設定する。
B. Cloud Key Management Service（Cloud KMS）で新しいキーを生成する。顧客管理キーオプションを使用してBigQueryにデータセットを作成し、作成したキーを選択する。
C. Cloud KMSにキーをインポートする。顧客管理キーオプションを使用してCloud Storageにすべてのデータを保存し、作成したキーを選択する。データを復号化して新しいBigQueryデータセットに保存するためのDataflowパイプラインを設定する。
D. Cloud KMSにキーをインポートする。顧客提供キーオプションを使用してBigQueryにデータセットを作成し、作成したキーを選択する。

**正解: D**

**解説:**
要件は「Google Cloudの外部で生成したキー」を使用することです。

1.  **Cloud KMS**には、外部で生成したキーをインポートしてKMS内で管理する**キーのインポート**機能があります。まず、この機能を使ってキーをGCPに持ち込みます。
2.  BigQueryは、**顧客管理の暗号鍵 (CMEK - Customer-Managed Encryption Keys)** をサポートしています。BigQueryのテーブル（またはデータセットレベルのデフォルト）を作成する際に、暗号化に使用するキーとして、Cloud KMSにインポートしたキーを指定できます。
    これにより、BigQueryはテーブルのデータをそのキーで暗号化します。これは、キーの生成元を外部で管理したいというコンプライアンス要件を満たすための標準的な方法です。
    （注: 選択肢Dの「顧客提供キーオプション」は「顧客管理キーオプション」の誤記と思われます。文脈からCMEKを指していることは明らかです。）

-----

### <a name="no177"></a>**NO.177**

あなたの顧客は、彼らの認証レイヤーの回復力テストを行いたいと考えています。これは、Cloud SQLインスタンスに読み書きする公開REST APIを提供するリージョンマネージドインスタンスグループで構成されています。どうすべきですか？

A. セキュリティ会社と契約して、悪意のあるWebサイトでユーザーの認証データを検索するWebスクレイプを実行させ、見つかった場合は通知してもらう。
B. 仮想マシンに侵入検知ソフトウェアをデプロイして、不正アクセスを検出および記録する。
C. 災害シミュレーション演習をスケジュールし、その間にゾーン内のすべてのVMをシャットダウンして、アプリケーションがどのように動作するかを確認する。
D. マスターとは異なるゾーンにCloud SQLインスタンスのリードレプリカを構成し、REST APIのKPIを監視しながら手動でフェイルオーバーをトリガーする。

**正解: C**

**解説:**
PDFの回答Dはデータベース層のみのテストであり、認証レイヤー全体（MIGを含む）のテストではありません。
**回復力テスト**は、システムが障害（この場合はゾーン障害）からどれだけうまく回復できるかを検証するものです。リージョンマネージドインスタンスグループは複数のゾーンにまたがってインスタンスを配置するため、ゾーン障害に対する耐性を持っています。この回復力を実際にテストするには、**意図的に1つのゾーンのVMをすべてシャットダウン**し、

  * マネージドインスタンスグループが残りのゾーンでインスタンスを自動的に再作成（またはスケールアップ）するか
  * ロードバランサがトラフィックを正常なゾーンに正しくルーティングするか
  * アプリケーション全体としてサービスが継続されるか
    を確認するのが最も直接的で効果的な方法です。

-----

### <a name="no178"></a>**NO.178**

あなたは、Cloud Datastoreのインデックスが不足していることが原因でApp Engineアプリケーションでエラーを発見しました。必要なインデックスを含むYAMLファイルを作成し、これらの新しいインデックスをCloud Datastoreにデプロイしたいです。どうすべきですか？

A. `gcloud datastore create-indexes`を構成ファイルに向けてポイントする。
B. 構成ファイルをApp EngineのデフォルトCloud Storageバケットにアップロードし、App Engineに新しいインデックスを検出させる。
C. GCPコンソールで、Datastore Adminを使用して現在のインデックスを削除し、新しい構成ファイルをアップロードする。
D. 組み込みのpythonモジュールへのHTTPリクエストを作成して、インデックス構成ファイルをアプリケーションに送信する。

**正解: A**

**解説:**
Cloud Datastore (およびFirestore in Datastore mode) の複合インデックスは、`index.yaml` というファイルで定義します。このインデックス定義をデプロイするための標準的なコマンドラインツールが `gcloud` です。`gcloud datastore create-indexes <index.yamlのパス>` コマンドを実行することで、`index.yaml` ファイルに定義されているが、まだDatastoreに存在しないインデックスが作成されます。

-----

### <a name="no179"></a>**NO.179**

あなたの会社は、GKEクラスタのデプロイメントで実行されるアプリケーションを持っています。開発、ステージング、本番用に別々のクラスタがあります。チームが、開発とステージングでのテストを経ずにDockerイメージを本番クラスタにデプロイできていることがわかりました。チームの自律性を許容しつつ、これを防ぎたいです。最小限の労力で迅速に実装できるGoogle Cloudソリューションが必要です。どうすべきですか？

A. 特定の環境での使用が承認されていない場合にコンテナの起動を防ぐKubernetesアドミッションコントローラーを作成する。
B. 特定の環境での使用が承認されていない場合にコンテナの起動を防ぐKubernetesライフサイクルフックを構成する。
C. Dockerイメージが以前の環境でテストされていない限り、チームが環境にDockerイメージをデプロイするのを防ぐ企業ポリシーを実装する。
D. 開発、ステージング、本番クラスタのバイナリアンストールポリシーを構成する。継続的インテグレーションパイプラインの一部として証明書を作成する。

**正解: D**

**解説:**
この要件（承認されていないコンテナのデプロイを強制的に防ぐ）を満たすために設計されたのが**Binary Authorization**です。

1.  CI/CDパイプラインの各ステージ（例: テスト完了、QA承認）で、コンテナイメージに対して**証明書（Attestation）**を電子署名付きで作成します。
2.  各環境のGKEクラスタ（ステージング、本番）で、**Binary Authorizationポリシー**を構成します。例えば、本番クラスタのポリシーでは「QAチームによる承認の証明書があるイメージのみデプロイを許可する」と設定します。
    これにより、CI/CDパイプラインを正しく経て承認されたイメージしか本番環境にデプロイできなくなり、プロセスを技術的に強制することができます。

-----

### <a name="no180"></a>**NO.180**

あなたは、会社の多数のWebサイトのクリックデータを保存するストレージシステムを選択するように依頼されました。このデータは、カスタムWebサイト分析パッケージから通常毎分6,000クリック、最大で毎秒8,500クリックのバーストでストリーミングされます。データサイエンスおよびユーザーエクスペリエンスチームによる将来の分析のために保存する必要があります。どのストレージインフラストラクチャを選択すべきですか？

A. Google Cloud SQL
B. Google Cloud Bigtable
C. Google Cloud Storage
D. Google Cloud Datastore

**正解: B**

**解説:**
PDFの回答Cは、主にバッチ分析に適しており、リアルタイムのストリーミング書き込みには最適ではありません。
このユースケースの要件は、非常に**高い書き込みスループット**（毎秒最大8,500回）を持つストリーミングデータです。

  * **Cloud Bigtable**は、このような高スループットの時系列データや分析データの取り込みと、低レイテンシでの読み取りのために設計されたNoSQLワイドカラムストアです。IoTやクリックストリームデータの保存先として非常に適しています。
  * Cloud Storage(C)も大量データの保存は可能ですが、このように高頻度で小さなイベントをリアルタイムに書き込むシナリオでは、Bigtableの方がパフォーマンスと効率の面で優れています。

-----

### <a name="no181"></a>**NO.181**

あなたの会社は、10TBのオンプレミスデータベースのエクスポートをCloud Storageに移行したいと考えています。このアクティビティを完了するまでの時間、全体的なコスト、およびデータベースの負荷を最小限に抑えたいです。オンプレミス環境とGoogle Cloud間の帯域幅は1Gbpsです。Google推奨のプラクティスに従いたいです。どうすべきですか？

A. Data Transfer Applianceを使用してオフライン移行を実行する。
B. 商用のパートナーETLソリューションを使用してオンプレミスデータベースからデータを抽出し、Cloud Storageにアップロードする。
C. データベースから直接データを読み取り、Cloud Storageに書き込むDataflowジョブを開発する。
D. データを圧縮し、`gsutil -m`を使用してマルチスレッドコピーを有効にしてアップロードする。

**正解: D**

**解説:**
PDFの回答AのTransfer Applianceは、ネットワーク帯域が非常に遅いか、データ量が数百TB以上の場合に適しており、このシナリオでは過剰です。
1Gbpsの帯域幅があれば、10TBのデータは理論上約24時間で転送できます。

  * **圧縮**: データをアップロードする前に**圧縮**することで、転送するデータサイズを大幅に削減し、転送時間を短縮できます。
  * **`gsutil -m`**: `gsutil`コマンドの`-m`フラグは、複数のファイルを**並列（マルチスレッド）**でアップロードするため、単一の接続を飽和させ、転送スループットを最大化するのに非常に効果的です。
    この組み合わせが、既存のネットワーク接続を最大限に活用し、時間とコストを最小限に抑えるための最も現実的で推奨される方法です。

-----

### <a name="no182"></a>**NO.182**

あなたの会社はGoogle Cloudを使用しています。組織の下にFinanceとShoppingの2つのフォルダがあります。開発チームのメンバーはGoogleグループに所属しています。開発チームグループには、組織でプロジェクトオーナーのロールが割り当てられています。開発チームがFinanceフォルダ内のプロジェクトでリソースを作成するのを防ぎたいです。どうすべきですか？

A. 開発チームグループにFinanceフォルダでプロジェクト閲覧者ロールを割り当て、開発チームグループにShoppingフォルダでプロジェクトオーナーロールを割り当てる。
B. 開発チームグループにFinanceフォルダでのみプロジェクト閲覧者ロールを割り当てる。
C. 開発チームグループにShoppingフォルダでプロジェクトオーナーロールを割り当て、組織から開発チームグループのプロジェクトオーナーロールを削除する。
D. 開発チームグループにShoppingフォルダでのみプロジェクトオーナーロールを割り当てる。

**正解: C**

**解説:**
IAMポリシーは上位から下位へ**継承**され、下位レベルで権限を**取り消すことはできません**。現在、開発チームは**組織レベル**でプロジェクトオーナーであるため、その下のすべてのフォルダ（FinanceとShoppingの両方）とプロジェクトに対してオーナー権限を持っています。
Financeフォルダでのリソース作成を防ぐには、まず**組織レベルでの過剰な権限を削除**する必要があります。その後、開発チームが必要とする権限（Shoppingフォルダでのオーナー権限など）を、より低い階層である**フォルダレベルで改めて付与**します。これにより、最小権限の原則に従った適切な権限設定が実現できます。

-----

### <a name="no183"></a>**NO.183**

あなたはデータウェアハウジングチームで働いており、データ分析を行っています。チームは外部パートナーからのデータを処理する必要がありますが、データには個人識別情報（PII）が含まれています。PIIデータを一切保存せずにデータを処理・保存する必要があります。どうすべきですか？

A. 外部ソースからデータを取得するためのDataflowパイプラインを作成する。パイプラインの一部としてCloud Data Loss Prevention（Cloud DLP）APIを使用してPIIデータを削除する。結果をBigQueryに保存する。
B. 外部ソースからデータを取得するためのDataflowパイプラインを作成する。パイプラインの一部として、すべての非PIIデータをBigQueryに保存し、すべてのPIIデータを保持ポリシーが設定されたCloud Storageバケットに保存する。
C. 外部パートナーにCloud Storageにすべてのデータをアップロードするように依頼する。バケットにバケットロックを構成する。バケットからデータを読み取るためのDataflowパイプラインを作成する。パイプラインの一部として、Cloud Data Loss Prevention（Cloud DLP）APIを使用してPIIデータを削除する。結果をBigQueryに保存する。
D. 外部パートナーにBigQueryデータセットにすべてのデータをインポートするように依頼する。データを新しいテーブルにコピーするためのDataflowパイプラインを作成する。Dataflowバケットの一部として、PIIデータを持つ列のすべてのデータをスキップする。

**正解: A**

**解説:**
PDFの回答Bは、PIIを別の場所に保存するため、要件「PIIデータを一切保存せずに」に違反します。
この要件を満たすための最適な組み合わせは、**Dataflow**と**Cloud DLP API**です。

  * **Dataflow**: 外部ソースからのデータを取り込み、ストリーミングまたはバッチで処理するためのスケーラブルなパイプラインを構築します。
  * **Cloud DLP API**: Dataflowパイプラインの途中のステップでDLP APIを呼び出します。DLPは、データストリーム内のPII（名前、電話番号、クレジットカード番号など）を自動的に検出し、**マスキング、トークン化、または完全に削除（リダクション）**することができます。
    この処理を経た、PIIが除去されたクリーンなデータのみを最終的な宛先であるBigQueryに保存します。

-----

### <a name="no184"></a>**NO.184**

あなたの会社は、データサイエンスチームのためにHadoopジョブを、基盤となるインフラを変更せずに移行したいと考えています。コストとインフラ管理の手間を最小限に抑えたいです。どの製品を使用すべきですか？

A. 標準のワーカインスタンスを使用してDataprocクラスタを作成する。
B. スポットワーカインスタンスを使用してDataprocクラスタを作成する。
C. 標準インスタンスを使用してCompute Engine上に手動でHadoopクラスタをデプロイする。
D. プリエンプティブルインスタンスを使用してCompute Engine上に手動でHadoopクラスタをデプロイする。

**正解: B**

**解説:**

  * **インフラ変更なしでHadoopジョブを移行**: **Cloud Dataproc**は、既存のApache SparkおよびHadoopジョブを最小限の変更で実行できる、フルマネージドなサービスです。手動でHadoopをデプロイする(C, D)よりもはるかに管理の手間が少なくなります。
  * **コスト最小化**: Hadoop/Sparkのワーカノードは、計算処理が中断されてもマスターノードがジョブを再スケジュールできるため、耐障害性があります。このようなワークロードには、標準インスタンスより大幅に安価な**スポットVM（旧プリエンプティブルVM）**を使用するのが最適です。Dataprocは、ワーカノードとしてスポットVMを使用するように構成でき、これにより計算コストを劇的に削減できます。

-----

### <a name="no185"></a>**NO.185**

この問題については、Dress4Winのケーススタディを参照してください。

Dress4Winは、クラウドへの移行計画の一環として、トラフィック負荷の急増に対応できるように、マネージドなロギングおよび監視システムをセットアップしたいと考えています。彼らは以下のことを確実にしたいです：

  * インフラが、一日の使用量の増減に対応してスケールアップ・ダウンする必要があるときに通知を受け取れること。
  * アプリケーションがエラーを報告したときに、管理者が自動的に通知されること。
  * アプリケーションの一部を多くのホストにまたがってデバッグするために、集約されたログをフィルタリングできること。
    どのGoogle StackDriverの機能を使用すべきですか？

A. Logging, Alerts, Insights, Debug
B. Monitoring, Trace, Debug, Logging
C. Monitoring, Logging, Alerts, Error Reporting
D. Monitoring, Logging, Debug, Error Report

**正解: C**

**解説:**
PDFの回答Dも近いですが、Cの方がより正確です。各要件に対応するCloud Operations (旧Stackdriver) の機能は以下の通りです。

  * **スケールアップ・ダウンの通知**: **Cloud Monitoring**を使用してCPU使用率などのメトリクスを監視し、しきい値を超えたら**Alerts**を送信します。
  * **エラーの自動通知**: **Error Reporting**は、アプリケーションの例外（エラー）を自動的に集約し、新しいエラーが発生した際に**Alerts**を送信します。
  * **ログのフィルタリング**: **Cloud Logging**は、複数のホストからのログを一元的に集約し、強力なクエリ言語でフィルタリングしてデバッグに役立てる機能を提供します。

-----

### <a name="no186"></a>**NO.186**

あなたの運用チームは現在、10TBのデータをサードパーティのオブジェクトストレージサービスに保存しています。Google推奨のプラクティスに従い、できるだけ早くこのデータをCloud Storageバケットに移動したいと考えています。このデータ移行のコストを最小限に抑えたいです。どのアプローチを使用すべきですか？

A. `gsutil mv`コマンドを使用してデータを移動する。
B. Storage Transfer Serviceを使用してデータを移動する。
C. データをTransfer Applianceにダウンロードし、Googleに送付する。
D. データをオンプレミスのデータセンターにダウンロードし、Cloud Storageバケットにアップロードする。

**正解: B**

**解説:**
**AWS S3、Azure Blob Storageなどの他のクラウドプロバイダーのオブジェクトストレージ**からGoogle Cloud Storageに大規模なデータを移行するために設計されたマネージドサービスが**Storage Transfer Service**です。このサービスは、転送をGoogleのバックボーンネットワーク上で実行するため、高速で信頼性が高く、管理が容易です。一度設定すれば、転送はバックグラウンドで自動的に行われます。`gsutil`(A)はVMなどから実行する必要があり、Transfer Appliance(C)はオフライン転送用、オンプレミス経由(D)は非効率です。

-----

### <a name="no187"></a>**NO.187**

あなたの会社は、Webベースのアプリケーションを開発しています。本番デプロイメントがソースコードのコミットにリンクされ、完全に監査可能であることを確認する必要があります。どうすべきですか？

A. 開発者がコードコミットにコミットの日時でタグ付けしていることを確認する。
B. 開発者がコミットにデプロイメントへのリンクを含むコメントを追加していることを確認する。
C. コンテナタグをソースコードのコミットハッシュと一致させる。
D. 開発者がコミットに`:latest`とタグ付けしていることを確認する。

**正解: C**

**解説:**
デプロイされたアーティファクトと、それを生成したソースコードを確実に関連付けるための最も信頼性の高い方法は、**Gitのコミットハッシュ**をアーティファクトのバージョンとして使用することです。

  * Gitのコミットハッシュ（例: `a1b2c3d`）は、コードの特定の状態に対して一意で不変です。
  * CI/CDパイプラインでコンテナイメージをビルドする際に、このコミットハッシュを**コンテナのタグ**として付けます（例: `gcr.io/my-project/my-app:a1b2c3d`）。
    これにより、本番環境で実行中のコンテナを見れば、そのタグからどのコードコミットからビルドされたものかが即座に、かつ一意に特定でき、完全な追跡可能性と監査可能性が保証されます。`:latest`(D)は上書きされるため、この目的には絶対に使用してはいけません。

-----

### <a name="no188"></a>**NO.188**

あなたは、QAチームが、Google Compute EngineとCloud Bigtableで実行される主要なクラウドサービスの拡張性をテストするための新しい負荷テストツールを導入するのを手伝っています。彼らはどの3つの要件を含めるべきですか？（3つ選択）

A. 負荷テストがCloud Bigtableのパフォーマンスを検証することを確認する。
B. 負荷テスト環境として使用するために別のGoogle Cloudプロジェクトを作成する。
C. 負荷テストツールを定期的に本番環境に対して実行するようにスケジュールする。
D. あなたのサービスが使用するすべてのサードパーティシステムが高い負荷を処理できることを確認する。
E. 負荷テストツールによる再生のために、すべてのトランザクションを記録するように本番サービスを計装する。
F. 負荷テストツールとターゲットサービスを、詳細なロギングとメトリクス収集で計装する。

**正解: B, D, F**

**解説:**
PDFの回答A,B,Fも妥当ですが、より一般的なプラクティスとしてはB,D,Fが挙げられます。

  * **B (別プロジェクト)**: 負荷テストは大量のリソースを消費し、予期せぬ動作を引き起こす可能性があるため、本番環境とは**別の専用プロジェクト**で実行するのが安全です。これにより、本番環境への影響（クォータの消費、設定の誤変更など）を完全に防ぐことができます。
  * **D (依存関係の考慮)**: システムは自社のサービスだけで完結しているわけではありません。負荷テストを行う際には、依存している外部のサードパーティAPIなどが、テストによる高負荷に耐えられるか、またはレート制限に達しないかを事前に確認・調整する必要があります。
  * **F (計装)**: 負荷テストの目的は、単に負荷をかけることではなく、その結果を**観測**してボトルネックを特定することです。そのためには、テスト対象のサービスと負荷テストツール自体の両方に、詳細なメトリクス（レイテンシ、エラー率など）とログを出力する**計装**を施すことが不可欠です。

-----

### <a name="no189"></a>**NO.189**

あるアプリケーション開発チームがあなたにアドバイスを求めてきました。彼らはGo 1.12を使用してHTTP(S) APIを書き、デプロイする予定です。APIは非常に予測不可能なワークロードを持ち、トラフィックのピーク時にも信頼性を維持する必要があります。彼らはこのアプリケーションの運用オーバーヘッドを最小限に抑えたいと考えています。どのアプローチを推奨すべきですか？

A. Compute Engineにデプロイする際にマネージドインスタンスグループを使用する。
B. コンテナでアプリケーションを開発し、Google Kubernetes Engine（GKE）にデプロイする。
C. App Engine標準環境用にアプリケーションを開発する。
D. カスタムランタイムを使用してApp Engineフレキシブル環境用にアプリケーションを開発する。

**正解: C**

**解説:**
要件は「予測不可能なワークロード」「ピーク時の信頼性」「運用オーバーヘッドの最小化」です。

  * **App Engine Standard Environment**は、これらの要件に最も合致するサーバーレスコンピューティングプラットフォームです。
  * トラフィックがゼロのときはインスタンス数がゼロになり（コストがかからない）、トラフィックが急増すると数秒で多数のインスタンスを起動してスケールアウトできます。
  * インフラの管理、パッチ適用、スケーリングはすべてGoogleによって自動的に行われるため、運用オーバーヘッドはほぼゼロです。
    GKE(B)やApp Engine Flexible(D)もスケーラブルですが、App Engine Standardほどの迅速なスケールアウトと運用オーバーヘッドの低さは実現できません。

-----

### <a name="no190"></a>**NO.190**

あなたの会社は、基盤となるインフラを変更せずにHadoopジョブを移行する必要があります。コストとインフラ管理の手間を最小限に抑えたいです。どうすべきですか？

A. 標準のワーカインスタンスを使用してDataprocクラスタを作成する。
B. スポットワーカインスタンスを使用してDataprocクラスタを作成する。
C. 標準インスタンスを使用してCompute Engine上に手動でHadoopクラスタをデプロイする。
D. プリエンプティブルインスタンスを使用してCompute Engine上に手動でHadoopクラスタをデプロイする。

**正解: B**

**解説:**
(注：この問題はNo.184と同じです。)

  * **インフラ変更なしでHadoopジョブを移行**: **Cloud Dataproc**は、既存のApache SparkおよびHadoopジョブを最小限の変更で実行できる、フルマネージドなサービスです。手動でHadoopをデプロイする(C, D)よりもはるかに管理の手間が少なくなります。
  * **コスト最小化**: Hadoop/Sparkのワーカノードは、計算処理が中断されてもマスターノードがジョブを再スケジュールできるため、耐障害性があります。このようなワークロードには、標準インスタンスより大幅に安価な**スポットVM（旧プリエンプティブルVM）**を使用するのが最適です。Dataprocは、ワーカノードとしてスポットVMを使用するように構成でき、これにより計算コストを劇的に削減できます。

-----

### <a name="no191"></a>**NO.191**

あなたの運用チームが、Compute Engineで実行されている本番アプリケーションのパフォーマンス問題を診断する手助けを求めてきました。アプリケーションは高負荷時に到達したリクエストをドロップしています。影響を受けるインスタンスのプロセスリストは、利用可能なすべてのCPUを消費している単一のアプリケーションプロセスを示しており、自動スケーリングはインスタンスの上限に達しています。データベースを含む他の関連システムには異常な負荷はありません。できるだけ早く本番トラフィックが再び処理されるようにしたいです。どのアクションを推奨すべきですか？

A. 自動スケーリングメトリックを `agent.googleapis.com/memory/percent_used` に変更する。
B. 影響を受けるインスタンスをずらして再起動する。
C. 各インスタンスにSSHで接続し、アプリケーションプロセスを再起動する。
D. 自動スケーリンググループのインスタンスの最大数を増やす。

**正解: D**

**解説:**
問題の状況は「高負荷」「CPUをすべて消費」「オートスケーラーが上限に達している」です。これは、アプリケーションがCPUバウンドであり、現在のインスタンス数では負荷を処理しきれていないことを明確に示しています。アプリケーションがリクエストをドロップしているのは、処理能力が需要に追いついていないためです。最も迅速で直接的な解決策は、オートスケーラーが作成できるインスタンスの**最大数を増やす**ことです。これにより、オートスケーラーはさらにインスタンスを追加して負荷を分散させ、CPU使用率を下げ、リクエストのドロップを解消できます。

-----

### <a name="no192"></a>**NO.192**

あなたの会社は、アプリケーションサーバーVMからCloud Pub/Subに機密性の高いトランザクションデータのバッチをプッシュして、処理と保存を行っています。アプリケーションが要求されたGoogle Cloudサービスに認証するためのGoogle推奨の方法は何ですか？

A. VMサービスアカウントに適切なCloud Pub/Sub IAMロールが付与されていることを確認する。
B. VMサービスアカウントがCloud Pub/Subにアクセスできないようにし、VMアクセススコープを使用して適切なCloud Pub/Sub IAMロールを付与する。
C. Cloud Pub/SubにアクセスするためのOAuth2アクセストークンを生成し、それを暗号化して、各VMからアクセスできるようにCloud Storageに保存する。
D. Cloud Functionを使用してCloud Pub/Subへのゲートウェイを作成し、Cloud Functionサービスアカウントに適切なCloud Pub/Sub IAMロールを付与する。

**正解: A**

**解説:**
Compute Engine (VM) 上で実行されるアプリケーションが他のGoogle Cloudサービス（Pub/Subなど）にアクセスするための、最も安全で推奨される方法は、**サービスアカウント**を使用することです。

1.  VMに専用のサービスアカウントをアタッチします。
2.  そのサービスアカウントに対して、必要な最小限の権限を持つIAMロール（この場合は `roles/pubsub.publisher` など）を付与します。
    アプリケーションは、VMのメタデータサーバーから自動的に認証情報を取得するため、コード内に認証キーを埋め込んだり、ファイルとして管理したりする必要が一切ありません。アクセススコープ(B)は古い仕組みであり、現在はIAMロールによる制御が推奨されています。

-----

### <a name="no193"></a>**NO.193**

あなたの会社は、すべてのGoogle CloudログをCloud Loggingに送信しています。セキュリティチームはログを監視したいと考えています。不要なファイアウォールの変更やサーバーの侵害などの異常が検出された場合に、セキュリティチームが迅速に対応できるようにしたいです。Google推奨のプラクティスに従いたいです。どうすべきですか？

A. Cloud Schedulerでcronジョブをスケジュールする。スケジュールされたジョブは、関連するイベントについて毎分ログをクエリする。
B. ログをBigQueryにエクスポートし、BigQueryでクエリをトリガーして関連するイベントのログデータを処理する。
C. ログをPub/Subトピックにエクスポートし、関連するログイベントでCloud Functionをトリガーする。
D. ログをCloud Storageバケットにエクスポートし、関連するログイベントでCloud Runをトリガーする。

**正解: C**

**解説:**
これは、特定のログイベントに基づいてリアルタイムでアクションを自動化するための、イベント駆動アーキテクチャの典型的なパターンです。

1.  **Cloud Loggingシンク**を作成し、フィルタ（例: `protoPayload.methodName="compute.firewalls.patch"`）に一致するログエントリを**Pub/Subトピック**にエクスポートするように設定します。
2.  そのPub/Subトピックをトリガーとする**Cloud Function**を作成します。
3.  Cloud Function内で、通知の送信（Slack, PagerDutyなど）、問題の自動修復、またはセキュリティ調査の開始などのアクションを実行します。
    この方法は、イベント発生からアクション実行までの遅延が非常に短く、「迅速に対応する」という要件に最適です。

-----

### <a name="no194"></a>**NO.194**

あなたのWebアプリケーションは、いくつかのワークロードを管理するためにGoogle Kubernetes Engineを使用しています。あるワークロードは、ポッドのスケーリングや再起動後も一貫したホスト名のセットを必要とします。これを達成するために、Kubernetesのどの機能を使用すべきですか？

A. StatefulSets
B. ロールベースのアクセス制御
C. コンテナ環境変数
D. 永続ボリューム

**正解: A**

**解説:**
Kubernetesの**Deployment**で管理されるPodは、使い捨て（ephemeral）として扱われ、再起動すると新しい名前とIPアドレスが割り当てられます。しかし、データベースのようなステートフルなアプリケーションでは、各レプリカが安定した一意のネットワーク識別子（ホスト名）を持つことが重要です。**StatefulSet**は、まさにこの目的のために設計されたワークロードAPIです。StatefulSetで管理されるPodは、`pod-name-0`, `pod-name-1`のような、順序付けられた予測可能な一意のホスト名を常に保持します。

-----

### <a name="no195"></a>**NO.195**

あなたは、オンライン分析処理（OLAP）マーケティング分析およびレポートツールを構築するように依頼されました。これには、数百テラバイトのデータを操作できるリレーショナルデータベースが必要です。このようなアプリケーションのためのGoogle推奨ツールは何ですか？

A. グローバルに分散されているため、Cloud Spanner
B. フルマネージドのリレーショナルデータベースであるため、Cloud SQL
C. デバイス間でリアルタイム同期を提供するため、Cloud Firestore
D. 表形式データの大規模処理用に設計されているため、BigQuery

**正解: D**

**解説:**
PDFの回答Aは、OLAPよりもOLTP（トランザクション処理）に適しています。
要件は「OLAP（分析）」「数百テラバイト」「リレーショナル（SQLライク）」です。

  * **BigQuery**は、大規模なデータセットに対する複雑な分析クエリ（OLAP）を高速に実行するために設計された、サーバーレスのデータウェアハウスです。
  * 数百テラバイト、さらにはペタバイト級のデータを扱うことができ、標準的なSQLインターフェースを提供します。
    これがこのユースケースに完全に一致するGoogle推奨のツールです。Cloud Spanner(A)はグローバルなトランザクション処理、Cloud SQL(B)は中規模のOLTP向けです。

-----

### <a name="no196"></a>**NO.196**

あなたの会社は、データ中心のビジネスフォーカスをサポートするための新しいアーキテクチャを構築しています。あなたはネットワークの設定を担当しています。会社のモバイルおよびWeb向けアプリケーションはオンプレミスにデプロイされ、すべてのデータ分析はGCPで行われます。計画では、合計900TBの7年分のアーカイブ済み.csvファイルを処理・ロードし、その後毎日10TBのデータをロードし続ける予定です。現在、既存の100MBのインターネット接続があります。会社のニーズを満たすためにどのアクションを実行すべきですか？

A. `gsutil -m`オプションを使用して、アーカイブ済みファイルと毎日アップロードされるファイルの両方を圧縮してアップロードする。
B. Transfer Applianceをリースし、アーカイブ済みファイルをそれにアップロードし、Googleに送ってアーカイブデータをCloud Storageに転送する。Dedicated InterconnectまたはDirect Peering接続を使用してGoogleとの接続を確立し、それを使用して毎日ファイルをアップロードする。
C. Transfer Applianceをリースし、アーカイブ済みファイルをそれにアップロードし、Googleに送ってアーカイブデータをCloud Storageに転送する。パブリックインターネット経由でVPCネットワークへのCloud VPNトンネルを1つ確立し、`gsutil -m`オプションを使用して毎日ファイルを圧縮してアップロードする。
D. Transfer Applianceをリースし、アーカイブ済みファイルをそれにアップロードし、Googleに送ってアーカイブデータをCloud Storageに転送する。パブリックインターネット経由でVPCネットワークへのCloud VPNトンネルを確立し、毎日ファイルを圧縮してアップロードする。

**正解: B**

**解説:**
この問題は、初期の**一括データ移行**と、その後の**継続的なデータ転送**という2つの異なる要件を扱っています。

  * **一括データ移行 (900TB)**: 100Mbpsの接続では、900TBの転送には数年かかってしまい非現実的です。このような大規模なオフラインデータ移行には、物理的な**Transfer Appliance**を使用するのが最適です。
  * **継続的なデータ転送 (10TB/日)**: 毎日10TBという大量のデータを安定して転送するには、100Mbpsのインターネット接続では不十分です。高帯域幅で信頼性の高いプライベート接続である**Dedicated Interconnect**または**Direct Peering**を確立する必要があります。

-----

### <a name="no197"></a>**NO.197**

あなたの主なビジネス目標の1つは、アプリケーションに保存されたデータを信頼できることです。アプリケーションデータへのすべての変更をログに記録したいです。ログの信頼性を検証するために、ロギングシステムをどのように設計できますか？

A. クラウドとオンプレミスで同時にログを書き込む。
B. SQLデータベースを使用し、ログテーブルを変更できる人を制限する。
C. 各タイムスタンプとログエントリにデジタル署名し、署名を保存する。
D. 各ログエントリのJSONダンプを作成し、Google Cloud Storageに保存する。

**正解: C**

**解説:**
ログの「信頼性」、つまり**完全性（改ざんされていないこと）と否認防止（誰が記録したかが証明できること）を検証するための暗号学的な技術がデジタル署名**です。各ログエントリを生成する際に、生成元の秘密鍵でデジタル署名を作成し、ログと一緒に保存します。後で監査する際に、対応する公開鍵で署名を検証することで、そのログが記録されてから変更されていないこと、そしてその特定の秘密鍵の所有者によって生成されたことを数学的に証明できます。

-----

### <a name="no198"></a>**NO.198**

あなたの会社のアプリケーションはCompute Engine上で設計されています。ゾーン障害が発生した場合、アプリケーションは最新のアプリケーションデータでできるだけ早く別のゾーンで復元される必要があります。この要件を満たすためにソリューションを設計する必要があります。どうすべきですか？

A. アプリケーションデータを含むディスクのスナップショットスケジュールを作成する。ゾーン障害が発生するたびに、最新のスナップショットを使用して同じゾーンにディスクを復元する。
B. アプリケーション用にインスタンステンプレートを使用してCompute Engineインスタンスを構成し、アプリケーションデータにはリージョン永続ディスクを使用する。ゾーン障害が発生するたびに、インスタンステンプレートを使用して同じリージョン内の別のゾーンにアプリケーションを立ち上げる。アプリケーションデータにはリージョン永続ディスクを使用する。
C. アプリケーションデータを含むディスクのスナップショットスケジュールを作成する。ゾーン障害が発生するたびに、最新のスナップショットを使用して同じリージョン内の別のゾーンにディスクを復元する。
D. アプリケーション用にインスタンステンプレートを使用してCompute Engineインスタンスを構成し、アプリケーションデータにはリージョン永続ディスクを使用する。ゾーン障害が発生するたびに、インスタンステンプレートを使用して別のリージョンにアプリケーションを立ち上げる。アプリケーションデータにはリージョン永続ディスクを使用する。

**正解: B**

**解説:**
これは、高可用性（HA）アーキテクチャを構築するための典型的なパターンです。

  * **リージョン永続ディスク**: データを同じリージョン内の2つのゾーンに**同期的に**複製します。一方のゾーンで障害が発生しても、データはもう一方のゾーンで即座に利用可能です。これにより、RPO（目標復旧時点）がゼロになります。
  * **インスタンステンプレート**: VMの構成（マシンタイプ、イメージ、起動スクリプトなど）を定義したテンプレートです。
    ゾーン障害が発生した場合、DR（災害復旧）スクリプトや手動操作で、正常なゾーンに**インスタンステンプレート**から新しいVMを起動し、そこに**リージョン永続ディスク**をアタッチすることで、迅速にサービスを復旧できます。

-----

### <a name="no199"></a>**NO.199**

あなたの会社は、App Engine Standardを使用するサポートチケットソリューションを持っています。App Engineアプリケーションを含むプロジェクトには、Cloud VPNトンネルを介して会社のオンプレミス環境に完全に接続されたVirtual Private Cloud（VPC）ネットワークが既にあります。App Engineアプリケーションが、会社のオンプレミス環境で実行されているデータベースと通信できるようにしたいです。どうすべきですか？

A. プライベートサービスアクセスを構成する。
B. オンプレミスホスト専用のプライベートGoogleアクセスを構成する。
C. サーバーレスVPCアクセスを構成する。
D. プライベートGoogleアクセスを構成する。

**正解: C**

**解説:**
**App Engine Standard**環境は、Googleが管理するネットワーク上で実行されるため、デフォルトではあなたのVPCネットワークに直接アクセスできません。App Engine StandardからVPCネットワーク（およびそれにVPNで接続されたオンプレミスネットワーク）内のリソースにプライベートIPでアクセスできるようにするためのブリッジとなるのが、**サーバーレスVPCアクセスコネクタ**です。このコネクタをVPC内に作成し、App Engineアプリケーションをそれに接続するように構成することで、App EngineからのトラフィックをVPC内にルーティングできるようになります。

-----

### <a name="no200"></a>**NO.200**

あなたの会社は、データウェアハウジングにBigQueryを使用するGoogle Cloudプロジェクトを持っています。オンプレミス環境とGoogle Cloud間のVPNトンネルはCloud VPNで構成されています。セキュリティチームは、悪意のある内部関係者、侵害されたコード、および偶発的な過剰共有によるデータ漏洩を避けたいと考えています。どうすべきですか？

A. VPC Service Controlsを構成し、オンプレミスホストに対してプライベートGoogleアクセスを構成する。
B. サービスアカウントを作成し、そのサービスアカウントにBigQueryジョブユーザーロールとストレージオブジェクト閲覧者ロールを付与し、プロジェクトから他のすべてのIdentity and Access Management（IAM）アクセスを削除する。
C. プライベートGoogleアクセスを構成する。
D. Private Service Connectを構成する。

**正解: A**

**解説:**
「データ漏洩を避ける」ための最も強力な機能が**VPC Service Controls**です。

1.  まず、BigQueryやCloud Storageなど、保護したいサービスを含むプロジェクトを**サービスペリメータ**で囲みます。これにより、ペリメータの内外でのデータコピーが原則として禁止されます。
2.  次に、オンプレミスからの正当なアクセスを許可する必要があります。VPN経由で接続しているオンプレミスネットワークからGoogle API（BigQueryなど）にプライベートIPでアクセスできるように、**オンプレミスホスト向けの限定公開のGoogleアクセス**を構成します。
3.  そして、オンプレミスネットワークを**アクセスレベル**として定義し、ペリメータへのアクセスを許可します。
    この組み合わせにより、データは承認されたオンプレミスネットワークとGCPプロジェクト間でのみやり取りでき、悪意のある内部関係者がデータを個人のGmailや外部のバケットにコピーするような漏洩を防ぐことができます。

-----

### <a name="no201"></a>**NO.201**

開発チームがKubernetes Deploymentファイルを提供してくれました。あなたはまだインフラを持っておらず、アプリケーションをデプロイする必要があります。どうすべきですか？

A. gcloudを使用してKubernetesクラスタを作成する。Deployment Managerを使用してデプロイメントを作成する。
B. gcloudを使用してKubernetesクラスタを作成する。kubectlを使用してデプロイメントを作成する。
C. kubectlを使用してKubernetesクラスタを作成する。Deployment Managerを使用してデプロイメントを作成する。
D. kubectlを使用してKubernetesクラスタを作成する。kubectlを使用してデプロイメントを作成する。

**正解: B**

**解説:**
Google CloudにおけるGKEの基本的な操作フローは以下の通りです。

1.  **インフラの作成 (クラスタ)**: GKEクラスタはGoogle Cloudのリソースなので、`gcloud`コマンドラインツールまたはCloud Consoleを使用して作成します。`kubectl`はKubernetesクラスタ**内**のリソースを操作するためのツールであり、クラスタ自体は作成できません。
2.  **アプリケーションのデプロイ**: クラスタが作成された後、提供されたDeploymentファイル（YAML）をクラスタに適用するには、Kubernetesの標準的なクライアントツールである`kubectl`を使用します（例: `kubectl apply -f <deployment-file.yaml>`）。

-----

### <a name="no202"></a>**NO.202**

あなたの会社は、すべてのワークロードのプラットフォームとしてGoogle Kubernetes Engine（GKE）を使用しています。会社には、バッチ、ステートフル、ステートレスのワークロードを含む単一の大きなGKEクラスタがあります。GKEクラスタは200ノードの単一ノードプールで構成されています。会社はこのクラスタのコストを削減したいと考えていますが、可用性を損ないたくありません。どうすべきですか？

A. バッチワークロード専用の2番目のGKEクラスタを作成する。元の200ノードを両方のクラスタに割り当てる。
B. すべてのステートレスワークロードと互換性のあるすべてのステートフルワークロードにHorizontalPodAutoscalerを構成する。クラスタでノードの自動スケーリングを使用するように構成する。
C. クラスタ内のネームスペースにCPUとメモリの制限を構成する。すべてのPodにCPUとメモリの制限を持たせるように構成する。
D. ノードプールをスポットVMを使用するように変更する。

**正解: B**

**解説:**
PDFの回答Cはリソース管理には有効ですが、直接的なコスト削減にはBがより効果的です。
GKEクラスタのコストを、可用性を損なわずに最適化するための最も効果的な方法は、自動スケーリング機能を組み合わせることです。

  * **Horizontal Pod Autoscaler (HPA)**: CPUやメモリ使用率などのメトリクスに基づいて、**Podのレプリカ数**を自動的に増減させます。これにより、アプリケーションレベルでのリソース使用量が最適化されます。
  * **Cluster Autoscaler (ノードの自動スケーリング)**: スケジューリングできなくなったPodがある場合に**ノード（VM）の数**を自動的に増やし、ノードの使用率が低い場合にはノードを自動的に減らします。
    この2つを組み合わせることで、負荷の変動に応じてPodとノードの両方が必要最小限の数に保たれ、コストを大幅に削減できます。

-----

### <a name="no203"></a>**NO.203**

あなたはAnthosクラスタ（旧Anthos GKE）にアプリケーションをデプロイしました。あなたの会社のSREプラクティスに従い、リクエストのレイテンシが指定された時間、特定のしきい値を超えた場合にアラートを受け取る必要があります。どうすべきですか？

A. プロジェクトでCloud Trace APIを有効にし、Cloud Monitoring Alertsを使用してCloud Traceメトリクスに基づいてアラートを送信する。
B. クラスタにAnthos Config Managementを構成し、クラスタにデプロイしたいSLOとアラートポリシーを定義するyamlファイルを作成する。
C. Cloud Profilerを使用してリクエストレイテンシを追跡する。Cloud Profilerの結果に基づいてCloud Monitoringにカスタムメトリクスを作成し、このメトリクがしきい値を超えた場合にアラートポリシーを作成する。
D. クラスタにAnthos Service Meshをインストールする。Google Cloud Consoleを使用してサービスレベル目標（SLO）を定義する。

**正解: D**

**解説:**
**Anthos Service Mesh**をインストールすると、サービス間のトラフィックに関する詳細なテレメトリ（レイテンシ、トラフィック量、エラー率）が自動的に収集されます。**Cloud Monitoring**はAnthos Service Meshと緊密に統合されており、これらのメトリクスに基づいて**サービスレベル目標（SLO）を簡単に定義できます。
SLOを設定すると、例えば「リクエストの95%が500ms以内に完了する」といった目標を定義でき、この目標を達成できない（エラーバジェットを使い果たす）場合に自動的にアラート**を生成するポリシーも簡単に作成できます。これがSREプラクティスにおけるレイテンシ監視の標準的な方法です。

-----

### <a name="no204"></a>**NO.204**

あなたのチームの開発者の一人が、以下のDockerfileを使用してGoogle Container Engineにアプリケーションをデプロイしました。彼らはアプリケーションのデプロイに時間がかかりすぎると報告しています。

```dockerfile
FROM ubuntu:16.04
COPY . /src
RUN apt-get update && apt-get install -y python python-pip
RUN pip install -r requirements.txt
```

アプリの機能に悪影響を与えることなく、このDockerfileをより速いデプロイ時間のために最適化したいです。どの2つのアクションを実行すべきですか？（2つ選択）

A. pipを実行した後にPythonを削除する。
B. requirements.txtから依存関係を削除する。
C. Alpine linuxのようなスリム化されたベースイメージを使用する。
D. Google Container Engineのノードプールに、より大きなマシンタイプを使用する。
E. パッケージの依存関係（Pythonとpip）がインストールされた後にソースをコピーする。

**正解: C, E**

**解説:**
Dockerイメージのビルド時間を短縮し、デプロイ（イメージのプル）時間を短縮するためのベストプラクティスは以下の通りです。

  * **C (スリムなベースイメージ)**: `ubuntu`のようなフル機能のOSイメージはサイズが大きいです。`alpine`のようなコンテナ実行に特化した非常に軽量なベースイメージを使用することで、イメージサイズが劇的に小さくなり、ビルドとプルの両方が高速化します。
  * **E (Dockerレイヤーキャッシュの活用)**: Dockerはイメージをレイヤーで構築し、各命令の結果をキャッシュします。`COPY . /src`のように頻繁に変更されるソースコードのコピーを先に行うと、コードが1行変更されただけで、それ以降のすべてのステップ（`apt-get`, `pip install`）のキャッシュが無効になり、毎回実行されてしまいます。変更頻度の低い依存関係のインストールを先に行い、その後にソースコードをコピーすることで、コード変更時にもキャッシュが有効活用され、ビルド時間が大幅に短縮されます。

-----

### <a name="no205"></a>**NO.205**

あなたの会社は、ユーザーが音楽ファイルをアップロードし、他の人々と共有できるアプリケーションをApp Engineで実行しています。ユーザーがブラウザセッションから直接Cloud Storageにファイルをアップロードできるようにしたいです。ペイロードはバックエンドを通過すべきではありません。どうすべきですか？

A. 1. ターゲットのCloud StorageバケットにCORS構成を設定し、App EngineアプリケーションのベースURLを許可されたオリジンとする。 2. Cloud Storageの署名付きURL機能を使用してPOST URLを生成する。
B. 1. ターゲットのCloud StorageバケットにCORS構成を設定し、App EngineアプリケーションのベースURLを許可されたオリジンとする。 2. ファイルをアップロードするユーザーにCloud Storageライターロールを割り当てる。
C. 1. Cloud Storageの署名付きURL機能を使用してPOST URLを生成する。 2. App Engineのデフォルト認証情報を使用してCloud Storageに対するリクエストに署名する。
D. 1. ファイルをアップロードするユーザーにCloud Storageライターロールを割り当てる。 2. App Engineのデフォルト認証情報を使用してCloud Storageに対するリクエストに署名する。

**正解: A**

**解説:**
バックエンドを経由せずにブラウザから直接Cloud Storageにファイルをアップロードさせるための安全な方法は、**署名付きURL (Signed URL)** と **CORS** を組み合わせることです。

1.  **CORS (Cross-Origin Resource Sharing) 設定**: ブラウザのセキュリティ制約により、App Engineのドメイン (`your-app.appspot.com`) からCloud Storageのドメイン (`storage.googleapis.com`) へのリクエストはデフォルトでブロックされます。Cloud StorageバケットにCORS設定を行い、App Engineのドメインを許可されたオリジンとして指定する必要があります。
2.  **署名付きURL (v4 Policy Document Signing)**: バックエンドのApp Engineアプリケーションが、短時間だけ有効なアップロード権限を持つ特別なURL（署名付きURL）を生成し、ブラウザに渡します。ブラウザはこのURLを使用して直接Cloud StorageにファイルをPOST（アップロード）します。これにより、ユーザーがGCPアカウントを持つ必要なく、安全に一時的なアップロード権限を付与できます。

-----

### <a name="no206"></a>**NO.206**

あなたの運用チームは、組織内のすべての本番プロジェクトからログを保存したいと考えていますが、他のプロジェクトからのログは保存したくありません。すべての本番プロジェクトは1つのフォルダに含まれています。既存および新規の本番プロジェクトのすべてのログが自動的にキャプチャされるようにしたいです。どうすべきですか？

A. Productionフォルダで集約エクスポートを作成する。ログシンクを運用プロジェクトのCloud Storageバケットに設定する。
B. 組織リソースで集約エクスポートを作成する。ログシンクを運用プロジェクトのCloud Storageバケットに設定する。
C. 本番プロジェクトでログエクスポートを作成する。ログシンクを運用プロジェクトのCloud Storageバケットに設定する。
D. 本番プロジェクトでログエクスポートを作成する。ログシンクを本番プロジェクトのBigQueryデータセットに設定し、運用チームにデータセットのクエリを実行するIAMアクセスを付与する。

**正解: A**

**解説:**
Cloud Loggingの**集約エクスポート (Aggregated Sink)** は、組織または**フォルダ**レベルで構成でき、その配下にあるすべてのプロジェクト（既存および新規）のログを一元的にエクスポートするための機能です。

  * **フォルダレベルでの設定**: 「Production」フォルダに対して集約エクスポートを設定することで、そのフォルダ内に今後作成される新しい本番プロジェクトのログも自動的にエクスポートの対象となります。
  * **シンクの宛先**: ログの保存先（シンク）として、運用チームが管理する別の中央集権的なプロジェクトのCloud StorageバケットやBigQueryデータセットを指定するのが一般的です。
    これにより、要件を効率的かつ将来にわたって維持可能な形で実現できます。

-----

### <a name="no207"></a>**NO.207**

あなたは、Google Compute Engine仮想マシンからGoogle BigQueryに接続するためのPythonスクリプトを書いています。スクリプトはBigQueryに接続できないというエラーを出力しています。スクリプトを修正するために何をすべきですか？

A. Python用の最新のBigQuery APIクライアントライブラリをインストールする。
B. BigQueryアクセススコープが有効な新しい仮想マシンでスクリプトを実行する。
C. BigQueryアクセス権を持つ新しいサービスアカウントを作成し、そのユーザーでスクリプトを実行する。
D. `gcloud components install bq`コマンドでgcloud用のbqコンポーネントをインストールする。

**正解: B**

**解説:**
Compute Engineインスタンスから他のGoogle Cloud API（BigQueryなど）にアクセスするには、そのインスタンスにアタッチされたサービスアカウントが適切な**アクセススコープ**を持っている必要があります。VM作成時に明示的に設定しない限り、多くのAPIへのアクセススコープはデフォルトで無効になっています。接続エラーの最も一般的な原因は、BigQuery APIへのアクセススコープがVMに設定されていないことです。VMを一度停止し、設定を編集してBigQueryのスコープを追加してから再起動するか、最初から適切なスコープを設定した新しいVMで実行することで問題は解決します。

-----

### <a name="no208"></a>**NO.208**

あなたの会社はGoogle Cloud上にデータレイクを設計しており、さまざまなソースから非構造化データを収集するために異なる取り込みパイプラインを開発したいと考えています。データがGoogle Cloudに保存された後、ウェブサイトのエンドユーザー向けの推薦エンジンを構築するためにいくつかのデータパイプラインで処理されます。ソースシステムから取得されるデータの構造はいつでも変更される可能性があります。データ構造が現在の処理パイプラインと互換性がない場合に備えて、再処理目的で取得されたとおりにデータを保存する必要があります。データを取得した後のユースケースをサポートするために、どのようなアーキテクチャを設計すべきですか？

A. 処理パイプラインを通してデータを送信し、その後、処理済みデータを再処理のためにBigQueryテーブルに保存する。
B. データをBigQueryテーブルに保存する。テーブルからデータを取得するように処理パイプラインを設計する。
C. 処理パイプラインを通してデータを送信し、その後、処理済みデータを再処理のためにCloud Storageバケットに保存する。
D. データをCloud Storageバケットに保存する。バケットからデータを取得するように処理パイプラインを設計する。

**正解: D**

**解説:**
これはデータレイクの基本的なアーキテクチャパターンです。

1.  **生データの保存**: ソースから取得した**未加工のままのデータ**を、安価でスケーラブルな**Cloud Storage**バケット（ランディングゾーン）に保存します。これにより、「取得されたとおりにデータを保存する」という再処理の要件が満たされます。スキーマが変動する非構造化データには、柔軟なオブジェクトストレージが最適です。
2.  **処理**: その後、Cloud Storageに保存された生データをソースとして、**処理パイプライン**（Dataflow, Dataprocなど）がデータを読み取り、変換・加工して、最終的な宛先（BigQuery, Cloud SQLなど）に書き込みます。
    この分離されたアプローチにより、処理ロジックにバグがあった場合や、新しい分析要件が出てきた場合に、いつでも元の生データから処理をやり直すことができます。

-----

### <a name="no209"></a>**NO.209**

あなたのアーキテクチャは、プロジェクト内のすべての管理者アクティビティとVMシステムログの一元的な収集を求めています。VMとサービスの両方からこれらのログをどのように収集すべきですか？

A. すべての管理者およびVMシステムログはStackdriverによって自動的に収集される。
B. Stackdriverはほとんどのサービスの管理者アクティビティログを自動的に収集する。システムログを収集するには、各インスタンスにStackdriver Loggingエージェントをインストールする必要がある。
C. カスタムのsyslogdコンピュートインスタンスを起動し、GCPプロジェクトとVMを構成してすべてのログをそれに転送する。
D. 単一のコンピュートインスタンスにStackdriver Loggingエージェントをインストールし、環境のすべての監査およびアクセスログを収集させる。

**正解: B**

**解説:**
Cloud Logging（旧Stackdriver）におけるログの収集方法は、ログの種類によって異なります。

  * **管理者アクティビティ監査ログ**: Google Cloudサービス（例: Compute Engine, IAM）に対するリソース構成の変更や管理者操作のログは、**デフォルトで自動的に**Cloud Loggingに収集されます。
  * **VMシステムログ**: VMインスタンス内のOSレベルのログ（syslogなど）やアプリケーションログを収集するには、そのVMに**Cloud Loggingエージェント**をインストールする必要があります。
    したがって、両方のログを収集するには、これらの組み合わせが必要です。

-----

### <a name="no210"></a>**NO.210**

あなたのアプリケーションはクレジットカード取引を処理する必要があります。取引データを分析し、どの支払い方法が使用されているかに関するトレンドを分析する能力を損なうことなく、ペイメントカード業界（PCI）コンプライアンスの範囲を最小限に抑えたいです。アーキテクチャはどのように設計すべきですか？

A. トークナイザーサービスを作成し、トークン化されたデータのみを保存する。
B. クレジットカードデータのみを処理する別のプロジェクトを作成する。
C. 別のサブネットワークを作成し、クレジットカードデータを処理するコンポーネントを分離する。
D. PCIデータを処理するすべての仮想マシン（VM）にラベルを付けて、監査の発見フェーズを合理化する。
E. Cloud LoggingのエクスポートをGoogle BigQueryに有効にし、ACLとビューを使用して監査人と共有するデータをスコープする。

**正解: A**

**解説:**
PCI DSSコンプライアンスの範囲（スコープ）は、クレジットカード情報（カード会員データ）が保存、処理、または転送されるすべてのシステムコンポーネントに及びます。スコープを最小化するための最も効果的な方法は、**トークン化**です。

  * 信頼できるサードパーティの決済代行業者（Payment Gateway）または自社で構築した**トークナイザーサービス**に、生のクレジットカード情報を渡します。
  * このサービスは、元のカード情報とは無関係で数学的にも復元不可能な文字列（**トークン**）を返します。
  * 自社のアプリケーションでは、この安全なトークンのみを保存・処理します。
    これにより、自社のシステムが実際のクレジットカード情報に触れることがなくなり、PCI DSSの監査対象範囲を劇的に縮小できます。

-----

### <a name="no211"></a>**NO.211**

あなたの会社とパートナーの1社は、それぞれ別の組織にGoogle Cloudプロジェクトを持っています。あなたの会社のプロジェクト（prj-a）はVirtual Private Cloud（vpc-a）で実行されています。パートナーのプロジェクト（prj-b）はvpc-bで実行されています。vpc-aには2つのインスタンスが、vpc-bには1つのインスタンスが実行されています。両方のVPCで定義されたサブネットは重複していません。すべてのインスタンスが内部IPを介して互いに通信できるようにし、レイテンシを最小限に抑え、スループットを最大化する必要があります。どうすべきですか？

A. vpc-aとvpc-bの間でネットワークピアリングを設定する。
B. Cloud VPNを使用してvpc-aとvpc-bの間にVPNを設定する。
C. vpc-bのインスタンスでIAP TCP転送を構成し、vpc-aのインスタンスの1つから`gcloud compute start-iap-tunnel`コマンドを起動する。
D. vpc-aとvpc-bにそれぞれ追加のインスタンスを作成し、新しく作成されたインスタンスにOpenVPNをインストールし、OpenVPNの助けを借りてvpc-aとvpc-bの間にVPNトンネルを構成する。

**正解: A**

**解説:**
PDFの回答Cは単一のマシンへのトンネリングであり、ネットワーク全体の接続には不適切です。
2つのVPCネットワークをプライベートIPで相互に接続するための、Google Cloudのネイティブで最もパフォーマンスの高い方法は**VPCネットワークピアリング**です。

  * VPCピアリングを設定すると、2つのVPCはあたかも同じVPCであるかのように、Googleの内部バックボーンネットワークを使用して直接通信できます。
  * これにより、VPN(B, D)のようなボトルネックや追加のコストなしに、**低レイテンシ**で**高スループット**な通信が実現します。
  * この機能の前提条件は、ピアリングするVPCのサブネットIP範囲が**重複していない**ことであり、問題文でこの条件は満たされています。

-----

### <a name="no212"></a>**NO.212**

あなたの会社は、運用機能をアウトソーシングすることを発表しました。開発者がクラウドベースのアプリケーションの新しいバージョンを本番環境に簡単にステージングできるようにし、アウトソーシングされた運用チームがステージングされたバージョンを自律的に本番環境にプロモートできるようにしたいです。ソリューションの運用オーバーヘッドを最小限に抑えたいです。どのGoogle Cloud製品に移行すべきですか？

A. App Engine
B. GKE On-Prem
C. Compute Engine
D. Google Kubernetes Engine

**正解: A**

**解説:**
この要件に最も合致するのは**App Engine**です。

  * **バージョニングとトラフィックスプリッティング**: App Engineは、新しいバージョンをデプロイし、それにトラフィックを流さずに「ステージング」しておくことが非常に簡単です。運用チームは、Cloud Consoleやgcloudコマンドで、トラフィックを新しいバージョンに切り替えるだけで「プロモーション」を完了できます。
  * **運用オーバーヘッドの最小化**: App Engineはサーバーレスプラットフォームであり、インフラの管理（パッチ適用、スケーリングなど）をGoogleに任せられるため、運用オーバーヘッドが最も低くなります。これは、運用チームがアウトソーシングされている状況で特に有利です。

-----

### <a name="no213"></a>**NO.213**

あなたの会社は別の会社を買収し、その既存のGoogle Cloud環境をあなたの会社のデータセンターに統合するように依頼されました。調査の結果、新しい会社のVirtual Private Cloud（VPC）で使用されているRFC 1918 IP範囲の一部が、あなたのデータセンターのIPスペースと重複していることがわかりました。接続を確立し、接続が確立されたときにルーティングの競合がないことを確認するために何をすべきですか？

A. 新しいVPCからデータセンターへのCloud VPN接続を作成し、Cloud Routerを作成し、重複するIPスペースがないように新しいIPアドレスを適用する。
B. 新しいVPCからデータセンターへのCloud VPN接続を作成し、重複するIPスペースでNATを実行するためにCloud NATインスタンスを作成する。
C. 新しいVPCからデータセンターへのCloud VPN接続を作成し、Cloud Routerを作成し、重複するIPスペースをブロックするためにカスタムルート広告を適用する。
D. 新しいVPCからデータセンターへのCloud VPN接続を作成し、重複するIPスペースをブロックするファイアウォールルールを適用する。

**正解: A**

**解説:**
ハイブリッドネットワーク接続では、IPアドレスの重複はルーティングの競合を引き起こすため許されません。最も根本的でクリーンな解決策は、重複を解消することです。

1.  まず、どちらか一方のネットワーク（この場合はGCPのVPC側）の**IPアドレス範囲を変更**し、オンプレミスと重複しないようにします。
2.  IPアドレスの重複が解消された後、**Cloud VPN**と**Cloud Router**（BGPによるダイナミックルーティングのため）を使用して、オンプレミスとVPC間の接続を確立します。
    NAT(B)は複雑な構成を必要とし、カスタムルート広告(C)は重複の問題を解決しません。IPアドレスの再割り当てが、長期的に安定したネットワークを構築するための最も正しいアプローチです。

-----

### <a name="no214"></a>**NO.214**

あなたの会社は75TBのデータをGoogle Cloudに移動しています。Cloud Storageを使用し、Googleの推奨プラクティスに従いたいです。どうすべきですか？

A. データをTransfer Applianceに移動する。Transfer Appliance Rehydratorを使用してデータをCloud Storageに復号化する。
B. データをTransfer Applianceに移動する。Cloud Dataprepを使用してデータをCloud Storageに復号化する。
C. データを含む各サーバーにgsutilをインストールする。再開可能な転送を使用してデータをCloud Storageにアップロードする。
D. データを含む各サーバーにgsutilをインストールする。ストリーミング転送を使用してデータをCloud Storageにアップロードする。

**正解: A**

**解説:**
数十TB規模のデータをネットワーク経由で転送するのは時間がかかり、不安定になる可能性があります。このような大規模なオフラインデータ移行のためにGoogleが提供しているのが**Transfer Appliance**です。

1.  Googleから物理的なアプライアンスをリースし、オンプレミスのデータセンターに設置します。
2.  データをアプライアンスにコピーします。データはアプライアンス上で自動的に暗号化されます。
3.  アプライアンスをGoogleに返送します。
4.  Googleの担当者がアプライアンスからあなたのCloud Storageバケットにデータを安全に転送（**Rehydrate**）します。
    これが75TBというデータ量に対するGoogle推奨の移行方法です。

-----

### <a name="no215"></a>**NO.215**

（注：この問題はDress4Winのケーススタディを参照していますが、質問文が欠落しているようです。一般的な「レイテンシ削減」の質問と解釈して回答します。）
Dress4winの現在のシステムアーキテクチャは、単一のデータセンターにあるため、一部の顧客に対して高いレイテンシが発生しています。将来の評価とクラウドでのパフォーマンス最適化の一環として、Dress4winはシステムアーキテクチャをGoogle Cloud Platform上の複数の場所に分散させたいと考えています。どのアプローチを使用すべきですか？

A. リージョンマネージドインスタンスグループとグローバルロードバランサを使用してパフォーマンスを向上させる。なぜなら、リージョンマネージドインスタンスグループはトラフィックに基づいて各リージョンで別々にインスタンスを増やすことができるからである。
B. グローバルロードバランサと、リクエストを運用チームが管理するより近い仮想マシンのグループに転送する一連の仮想マシンを使用する。
C. リージョンマネージドインスタンスグループとグローバルロードバランサを使用して、異なるリージョンのゾーン間の自動フェイルオーバーを提供することで信頼性を向上させる。
D. グローバルロードバランサと、リクエストを別のマネージドインスタンスグループの一部としてより近い仮想マシンのグループに転送する一連の仮想マシンを使用する。

**正解: A**

**解説:**
世界中のユーザーへのレイテンシを削減するためのベストプラクティスは、ユーザーに近い複数のリージョンにアプリケーションをデプロイし、トラフィックをインテリジェントにルーティングすることです。

  * **リージョンマネージドインスタンスグループ**: 各リージョン（例: 米国、ヨーロッパ、アジア）にアプリケーションサーバーをデプロイし、そのリージョン内の負荷に応じてインスタンス数を自動的にスケールさせます。
  * **グローバル外部HTTP(S)ロードバランサ**: 単一のエニーキャストIPアドレスを持ち、ユーザーからのリクエストを、ネットワーク的に最も近い、かつ正常なバックエンド（この場合はリージョンMIG）に自動的にルーティングします。
    この組み合わせにより、パフォーマンス（低レイテンシ）と可用性の両方をグローバル規模で実現できます。

-----

### <a name="no216"></a>**NO.216**

あなたの会社はデータウェアハウジングにBigQueryを使用するGoogle Cloudプロジェクトを持っています。オンプレミス環境とGoogle Cloud間のVPNトンネルはCloud VPNで構成されています。セキュリティチームは、悪意のある内部関係者、侵害されたコード、および偶発的な過剰共有によるデータ漏洩を避けたいと考えています。どうすべきですか？

A. オンプレミス専用のプライベートGoogleアクセスを構成する。
B. 以下のタスクを実行する： 1) サービスアカウントを作成する。 2) そのサービスアカウントにBigQueryジョブユーザーロールとストレージ閲覧者ロールを与える。 3) プロジェクトから他のすべてのIAMアクセスを削除する。
C. VPC Service Controlsを構成し、プライベートGoogleアクセスを構成する。
D. プライベートGoogleアクセスを構成する。

**正解: C**

**解説:**
(注：この問題はNo.200とほぼ同じです。)
「データ漏洩を避ける」ための最も強力な機能が**VPC Service Controls**です。

1.  まず、BigQueryやCloud Storageなど、保護したいサービスを含むプロジェクトを**サービスペリメータ**で囲みます。これにより、ペリメータの内外でのデータコピーが原則として禁止されます。
2.  次に、VPN経由で接続しているオンプレミスからの正当なアクセスを許可する必要があります。オンプレミスネットワークからGoogle APIにプライベートIPでアクセスできるように、**オンプレミスホスト向けの限定公開のGoogleアクセス**を構成します。
3.  そして、オンプレミスネットワークを**アクセスレベル**として定義し、ペリメータへのアクセスを許可します。
    この組み合わせにより、データは承認されたネットワークとGCPプロジェクト間でのみやり取りでき、悪意のある内部関係者がデータを外部に持ち出すような漏洩を防ぐことができます。

-----

### <a name="no217"></a>**NO.217**

あなたはオンプレミス環境からCloud Storageにファイルをアップロードする必要があります。Cloud Storage上で顧客提供の暗号化キーを使用してファイルを暗号化したいです。どうすべきですか？

A. .boto構成ファイルに暗号化キーを提供する。gsutilを使用してファイルをアップロードする。
B. gcloud configを使用して暗号化キーを提供する。gsutilを使用してそのバケットにファイルをアップロードする。
C. gsutilを使用してファイルをアップロードし、`--encryption-key`フラグを使用して暗号化キーを提供する。
D. gsutilを使用してバケットを作成し、`--encryption-key`フラグを使用して暗号化キーを提供する。そのバケットにgsutilを使用してファイルをアップロードする。

**正解: A**

**解説:**
**顧客提供の暗号化キー (CSEK)** を`gsutil`で使用する場合、暗号化キーをコマンドラインの引数(C, D)で渡すのは、キーがコマンド履歴に残ってしまうためセキュリティ上危険です。推奨される方法は、`gsutil`の構成ファイルである`.boto`ファイルに暗号化キーを設定することです。
`.boto`ファイルに`encryption_key = <YOUR_KEY>`の行を追加しておけば、`gsutil`コマンドを実行する際に、ファイル内のキーが自動的に使用されてアップロードされるオブジェクトが暗号化されます。

-----

### <a name="no218"></a>**NO.218**

あなたは、今後のローンチのためにWebトラフィックを処理するための自動スケーリングインスタンスグループを設定しました。インスタンスグループをHTTP(S)ロードバランサのバックエンドサービスとして構成した後、仮想マシン（VM）インスタンスが毎分終了および再起動されていることに気づきました。インスタンスにはパブリックIPアドレスがありません。各インスタンスからcurlコマンドを使用して適切なWeb応答が返ってくることを確認しました。バックエンドが正しく構成されていることを確認したいです。どうすべきですか？

A. ソーストラフィックがHTTP/HTTPSでロードバランサに到達できるようにするファイアウォールルールが存在することを確認する。
B. 各インスタンスにパブリックIPを割り当て、ロードバランサがインスタンスのパブリックIPに到達できるようにするファイアウォールルールを構成する。
C. ロードバランサのヘルスチェックがインスタンスグループ内のインスタンスに到達できるようにするファイアウォールルールが存在することを確認する。
D. 各インスタンスにロードバランサの名前でタグを作成する。ロードバランサの名前をソースとし、インスタンスタグを宛先とするファイアウォールルールを構成する。

**正解: C**

**解説:**
インスタンスが正常に応答しているにもかかわらず、ロードバランサのバックエンドで再起動を繰り返している場合、その最も一般的な原因は**ロードバランサからのヘルスチェックに失敗している**ことです。ロードバランサは、バックエンドのVMが正常かどうかを判断するために、定期的にヘルスチェックリクエストを送信します。このヘルスチェック用のトラフィックが**ファイアウォールルール**によってブロックされていると、ロードバランサはVMを「不健康」と判断し、マネージドインスタンスグループの自動修復機能がそのVMを終了して新しいものに置き換えてしまいます。Google Cloudのヘルスチェックは特定のIPアドレス範囲から送信されるため、そのIP範囲からのトラフィックを許可するファイアウォールルールを作成する必要があります。

-----

### <a name="no219"></a>**NO.219**

あなたの会社は、データウェアハウジングにBigQueryを使用するGoogle Cloudプロジェクトを、従量課金制で利用しています。クエリをリアルタイムで監視し、最もコストのかかるクエリと、どのユーザーが最も多く消費しているかを明らかにしたいです。どうすべきですか？

A. 1. BigQueryのデータアクセスログをCloud StorageにエクスポートするためのCloud Loggingシンクを作成する。 2. ユーザーごとに分割されたクエリのコストを計算するためのDataflowパイプラインを開発する。
B. 1. BigQueryのデータアクセスログをBigQueryにエクスポートするためのCloud Loggingシンクを作成する。 2. 生成されたテーブルに対してBigQueryクエリを実行し、必要な情報を抽出する。
C. 1. BigQueryへの課金エクスポートを有効にする。 2. 課金テーブルに対してBigQueryクエリを実行し、必要な情報を抽出する。
D. 1. クエリ対象のすべてのテーブルを含むBigQueryデータセットで、クエリを起動できる各ユーザーのラベルを追加する。 2. プロジェクトの課金ページを開く。 3. レポートを選択する。 4. 製品としてBigQueryを選択し、確認したいユーザーでフィルタリングする。

**正解: C**

**解説:**
BigQueryのクエリコストを含む、すべてのGoogle Cloudサービスのコストに関する最も詳細で正確な情報を取得する方法は、**Cloud Billingのエクスポート機能を有効にして、課金データをBigQueryデータセットにエクスポートする**ことです。

  * このエクスポートされたテーブルには、サービスごと、SKUごと、プロジェクトごと、そして**ラベル**ごとのコストが記録されています。
  * ユーザーが実行したクエリにラベルを付ける（またはユーザーごとにプロジェクトを分ける）ことで、この課金テーブルに対してSQLクエリを実行し、ユーザーごとのBigQueryクエリコストを正確に分析できます。

-----

### <a name="no220"></a>**NO.220**

あなたの会社は最近Cloud Identityを有効にしてユーザーを管理し始めました。Google Cloud組織も設定済みです。セキュリティチームは、組織の一部となるプロジェクトを保護する必要があります。今後、ドメイン外のIAMユーザーが権限を取得するのを禁止したいと考えています。どうすべきですか？

A. 組織ポリシーを構成して、ドメインによってIDを制限する。
B. 組織ポリシーを構成して、サービスアカウントの作成をブロックする。
C. Cloud Schedulerを使用して、Cloud Identityドメインに属さないすべてのユーザーをすべてのプロジェクトから1時間ごとに削除するCloud Functionをトリガーする。
D. 技術ユーザーを作成し、ルート組織レベルでプロジェクトオーナーのロールを与える。組織内のすべてのプロジェクトのすべてのIAMルールをリストし、会社のドメインに属さないすべてのユーザーを削除するbashスクリプトを書き、cronジョブで実行する。

**正解: A**

**解説:**
(注：この問題はNo.51と同じです。)
**組織ポリシーサービス**は、組織全体のリソースに対して制約を一元的に設定するための機能です。`constraints/iam.allowedPolicyMemberDomains` という制約を使用することで、IAMポリシーに追加できるユーザーのGoogle WorkspaceまたはCloud Identityドメインを明示的に許可リスト形式で指定できます。これにより、許可されたドメインに属さないユーザーやサービスアカウントがIAMロールを付与されることを防ぎ、セキュリティを強化できます。これはGoogleが推奨するベストプラクティスです。

-----

### <a name="no221"></a>**NO.221**

ある開発マネージャーが新しいアプリケーションを構築しています。彼はあなたに彼の要件を確認し、それらを満たすために使用できるクラウド技術を特定するように依頼しました。アプリケーションは以下の要件を満たす必要があります：

1.  クラウドのポータビリティのためにオープンソース技術に基づいていること
2.  需要に基づいて計算能力を動的にスケーリングすること
3.  継続的なソフトウェアデリバリーをサポートすること
4.  同じアプリケーションスタックの複数の分離されたコピーを実行すること
5.  動的テンプレートを使用してアプリケーションバンドルをデプロイすること
6.  URLに基づいて特定のサービスにネットワークトラフィックをルーティングすること
    どの技術の組み合わせが彼のすべての要件を満たしますか？

A. Google Container Engine, Jenkins, and Helm
B. Google Container Engine and Cloud Load Balancing
C. Google Compute Engine and Cloud Deployment Manager
D. Google Compute Engine, Jenkins, and Cloud Load Balancing

**正解: A**

**解説:**
各要件を分解すると、Aの組み合わせが最適であることがわかります。

1.  **オープンソース/ポータビリティ**: **Kubernetes (GKE)** は業界標準のオープンソースコンテナオーケストレータです。
2.  **動的スケーリング**: **GKE**はHorizontal Pod Autoscaler (HPA) と Cluster Autoscaler (CA) をサポートしています。
3.  **継続的デリバリー**: **Jenkins**は広く使われているCI/CDツールです。
4.  **分離されたコピー**: Kubernetesの**namespace**機能を使えば、同じクラスタ内でアプリケーションスタックを論理的に分離できます。
5.  **動的テンプレート**: **Helm**は「Kubernetesのパッケージマネージャー」と呼ばれ、パラメータ化されたテンプレート（チャート）を使用してアプリケーションをデプロイ・管理できます。
6.  **URLベースのルーティング**: Kubernetesの**Ingress**リソースは、URLパスに基づいてトラフィックを異なるサービスにルーティングします。

-----

### <a name="no222"></a>**NO.222**

あなたの会社は、オンプレミスのデータセンターをクラウドに移行しています。移行の一環として、ワークロードのオーケストレーションにKubernetes Engineを活用したいと考えています。アーキテクチャの一部はPCI DSSに準拠している必要もあります。以下のうち、最も正確なものはどれですか？

A. App Engineは、PCI DSSホスティングに認定されたGCP上の唯一のコンピュートプラットフォームである。
B. Kubernetes Engineは共有ホスティングと見なされるため、PCI DSS下では使用できない。
C. Kubernetes EngineとGCPは、PCI DSS準拠の環境を構築するために必要なツールを提供する。
D. Google Cloud PlatformはPCI準拠として認定されているため、すべてのGoogle Cloudサービスが使用可能である。

**正解: C**

**解説:**
PCI DSSコンプライアンスは**共有責任モデル**です。

  * Google Cloudは、**PCI DSSに準拠したインフラストラクチャ**を提供しています。GKEもその対象サービスに含まれています。
  * しかし、単にGKEを使用するだけでアプリケーションが自動的に準拠するわけではありません。顧客（あなた）は、GKE上でアプリケーションを構築・運用する際に、ネットワークセグメンテーション（VPCファイアウォール、ネットワークポリシー）、暗号化、アクセス制御（IAM, RBAC）、ロギングなど、PCI DSSの要件を満たすように**GCPが提供するツールを使って正しく構成する責任**があります。
    したがって、「GKEとGCPは準拠環境を**構築するためのツールを提供する**」という表現が最も正確です。

-----

### <a name="no223"></a>**NO.223**

あなたは、GCP上でのコンピューティングのための信頼性の高いタスクスケジューリングをサポートすることで、アプリケーションと運用の信頼性を確保する必要があります。Googleのベストプラクティスを活用して、どうすべきですか？

A. App Engineが提供するCronサービスを使用して、Compute Engineインスタンスで実行されているメッセージ処理ユーティリティサービスに直接メッセージを公開する。
B. App Engineが提供するCronサービスを使用して、Cloud Pub/Subトピックにメッセージを公開する。Compute Engineインスタンスで実行されているメッセージ処理ユーティリティサービスを使用してそのトピックを購読する。
C. Google Kubernetes Engine（GKE）が提供するCronサービスを使用して、Compute Engineインスタンスで実行されているメッセージ処理ユーティリティサービスに直接メッセージを公開する。
D. GKEが提供するCronサービスを使用して、Cloud Pub/Subトピックにメッセージを公開する。Compute Engineインスタンスで実行されているメッセージ処理ユーティリティサービスを使用してそのトピックを購読する。

**正解: B**

**解説:**
これは、**疎結合**で**信頼性の高い**タスクスケジューリングシステムを構築するためのGoogle推奨パターンです。

  * **スケジューラ (App Engine Cron)**: **App Engine Cron Service**は、cron構文でタスクを定期的にトリガーするための、信頼性が高くフルマネージドなサービスです。
  * **キュー (Cloud Pub/Sub)**: スケジューラとワーカー（処理実行者）を直接接続(A)すると、ワーカーがダウンしていた場合にタスクが失われます。間に**Cloud Pub/Sub**を挟むことで、タスク（メッセージ）はキューに永続化され、ワーカーが利用可能になったときに確実に処理されます。これにより、システム全体の信頼性が向上します。
  * **ワーカー (Compute Engine)**: Pub/Subトピックを購読（サブスクライブ）し、メッセージを受信して実際のタスクを実行します。

-----

### <a name="no224"></a>**NO.224**

あなたは、自己修復のためにインスタンスグループにソースコードの変更をデプロイできるパイプラインを作成しました。変更の1つが、あなたの主要業績評価指標に悪影響を与えました。修正方法がわからず、調査には最大1週間かかる可能性があります。どうすべきですか？

A. サーバーにログインし、ローカルで修正を繰り返す。
B. インスタンスグループのテンプレートを以前のものに変更し、すべてのインスタンスを削除する。
C. ソースコードの変更を元に戻し（revert）、デプロイメントパイプラインを再実行する。
D. 不良なコード変更があるサーバーにログインし、以前のコードに交換する。

**正解: C**

**解説:**
CI/CDとInfrastructure as Codeの原則に従うと、本番環境への手動での変更(A, D)は避けるべきです。すべての変更は、バージョン管理システム（Gitなど）と自動化されたデプロイパイプラインを通じて行われるべきです。
問題が発生した場合の最も安全で確実な対応は、

1.  Gitで問題を引き起こした**コミットを元に戻す（revert）**新しいコミットを作成します。
2.  そのrevertコミットをパイプラインのトリガーとなるブランチにプッシュします。
3.  **デプロイパイプラインが自動的に再実行**され、以前の正常なバージョンのコードが本番環境にデプロイされます。
    これにより、迅速にサービスを復旧でき、すべての変更がソースコードリポジトリで追跡可能に保たれます。

-----

### <a name="no225"></a>**NO.225**

あなたは、マネージドインスタンスグループで実行されているアプリケーションに、重要ではない更新を開発し、その更新を含む新しいインスタンステンプレートを作成しました。アプリケーションへの影響を防ぐため、実行中のインスタンスは更新したくありません。マネージドインスタンスグループによって作成される新しいインスタンスには、新しい更新が含まれるようにしたいです。どうすべきですか？

A. 新しいローリングリスタート操作を開始する。
B. 新しいローリングリプレース操作を開始する。
C. 新しいローリングアップデートを開始する。プロアクティブ更新モードを選択する。
D. 新しいローリングアップデートを開始する。オポチュニスティック更新モードを選択する。

**正解: D**

**解説:**
マネージドインスタンスグループ（MIG）の更新ポリシーには2つのモードがあります。

  * **プロアクティブ (Proactive)**: MIGは、古いテンプレートのインスタンスを**積極的に**停止し、新しいテンプレートのインスタンスに置き換えていきます。
  * **オポチュニスティック (Opportunistic)**: MIGは、既存のインスタンスを積極的に置き換えません。代わりに、オートスケーラーによるスケールイン（縮小）や手動での削除、インスタンスの修復など、**何らかの機会（opportunity）**でインスタンスが再作成されるときに、新しいテンプレートが適用されます。
    「実行中のインスタンスは更新したくない」が「新しいインスタンスは更新されてほしい」という要件には、オポチュニスティックモードが完全に一致します。

-----

### <a name="no226"></a>**NO.226**

あなたは、Google Compute Engineを使用して、いくつかのプリエンプティブルLinux仮想マシンインスタンスを作成しました。仮想マシンがプリエンプトされる前に、アプリケーションを適切にシャットダウンしたいです。どうすべきですか？

A. `/etc/rc.6.d/`ディレクトリに`k99.shutdown`という名前のシャットダウンスクリプトを作成する。
B. Linuxでxinetdサービスとして登録されたシャットダウンスクリプトを作成し、サービスの呼び出しにStackdriverエンドポイントチェックを構成する。
C. 新しい仮想マシンインスタンスを作成する際に、Cloud Platform Consoleで`shutdown-script`というキーを持つ新しいメタデータエントリの値としてシャットダウンスクリプトを使用する。
D. Linuxでxinetdサービスとして登録されたシャットダウンスクリプトを作成し、`gcloud compute instances add-metadata`コマンドを使用して、`shutdown-script-url`というキーを持つ新しいメタデータエントリの値としてサービスURLを指定する。

**正解: C**

**解説:**
Compute Engineには、インスタンスが停止または削除（プリエンプションを含む）される直前にスクリプトを実行する**シャットダウンスクリプト**という機能があります。

  * 実行したいシャットダウン処理（アプリケーションの正常な終了、データの保存など）をシェルスクリプトとして記述します。
  * このスクリプトの内容を、インスタンスのカスタムメタデータにキー`shutdown-script`として設定します。
    インスタンスがプリエンプトされる際、シャットダウンプロセスの一部として、Compute Engineはこのメタデータに設定されたスクリプトを自動的に実行します。

-----

### <a name="no227"></a>**NO.227**

あなたは、オンプレミスのデータベースと統合する必要があるApp Engine上のアプリケーションをデプロイしています。セキュリティ上の目的で、オンプレミスのデータベースはパブリックインターネット経由でアクセスできてはなりません。どうすべきですか？

A. アプリケーションをApp Engine標準環境にデプロイし、App Engineのファイアウォールルールを使用して、公開されたオンプレミスデータベースへのアクセスを制限する。
B. アプリケーションをApp Engine標準環境にデプロイし、Cloud VPNを使用してオンプレミスデータベースへのアクセスを制限する。
C. アプリケーションをApp Engineフレキシブル環境にデプロイし、App Engineのファイアウォールルールを使用してオンプレミスデータベースへのアクセスを制限する。
D. アプリケーションをApp Engineフレキシブル環境にデプロイし、Cloud VPNを使用してオンプレミスデータベースへのアクセスを制限する。

**正解: D**

**解説:**
オンプレミスデータベースにプライベートに接続するには、VPCネットワークとの接続が必要です。

  * **App Engineフレキシブル環境**は、デプロイされたアプリケーションがVPCネットワーク内に配置されるため、VPC内の他のリソースや、**Cloud VPN**または**Cloud Interconnect**を介して接続されたオンプレミスネットワークに直接アクセスできます。
  * App Engine標準環境からVPC/オンプレミスにアクセスするには、別途サーバーレスVPCアクセスコネクタが必要となり、より複雑になります。
    したがって、VPCとのネイティブな統合が可能なApp Engineフレキシブル環境とCloud VPNを組み合わせるのが、この要件に対する直接的な解決策です。

-----

### <a name="no228"></a>**NO.228**

あなたの会社の開発チームが、コンテナ化されたHTTPS Webアプリケーションを作成しました。アプリケーションをGoogle Kubernetes Engine（GKE）にデプロイし、アプリケーションが自動的にスケーリングすることを確認する必要があります。GKEにはどのようにデプロイすべきですか？

A. Horizontal Pod Autoscalerを使用し、クラスタの自動スケーリングを有効にする。Ingressリソースを使用してHTTPSトラフィックを負荷分散する。
B. Horizontal Pod Autoscalerを使用し、Kubernetesクラスタでクラスタの自動スケーリングを有効にする。LoadBalancerタイプのServiceリソースを使用してHTTPSトラフィックを負荷分散する。
C. Compute Engineインスタンスグループで自動スケーリングを有効にする。Ingressリソースを使用してHTTPSトラフィックを負荷分散する。
D. Compute Engineインスタンスグループで自動スケーリングを有効にする。LoadBalancerタイプのServiceリソースを使用してHTTPSトラフィックを負荷分散する。

**正解: A**

**解説:**
GKEでスケーラブルなHTTPSアプリケーションを公開するためのベストプラクティスは以下の通りです。

  * **アプリケーションのスケーリング (Pod)**: **Horizontal Pod Autoscaler (HPA)** を使用して、CPUやメモリの負荷に応じてPodのレプリカ数を自動的に増減させます。
  * **インフラのスケーリング (Node)**: GKEの**Cluster Autoscaler**を有効にして、Podの数に応じてクラスタのノード（VM）数を自動的に増減させます。
  * **HTTPSトラフィックの公開**: **Ingress**リソースを使用します。Ingressを構成すると、Google Cloudの**外部HTTP(S)ロードバランサ**が自動的にプロビジョニングされます。このロードバランサは、SSL終端、URLベースのルーティング、CDN連携など、高度な機能を提供します。`type: LoadBalancer`のService(B)は、主にL4（TCP/UDP）の負荷分散に使用され、HTTPSのようなL7トラフィックにはIngressが適しています。

-----

### <a name="no229"></a>**NO.229**

あなたの会社の運用チームは、Cloud VPNのログイベントを1年間保存したいと考えています。ログを保存するためにクラウドインフラを構成する必要があります。どうすべきですか？

A. Cloud Loggingでフィルタを設定し、ログを公開するためのPub/Subトピックを設定する。
B. Cloud VPN LogsというタイトルのCloud Loggingダッシュボードを設定し、1年間のVPNメトリクスをクエリするチャートを追加する。
C. Compute Engine APIを有効にし、保存したいトラフィックに一致するファイアウォールルールでロギングを有効にする。
D. Cloud Loggingでフィルタを設定し、保存したいログのエクスポート先としてCloud Storageバケットを設定する。

**正解: D**

**解説:**
Cloud Loggingのデフォルトのログ保持期間は通常30日程度であり、1年間という要件を満たしません。ログを長期間保存するための標準的な方法は、**ログエクスポート（シンク）**機能を使用することです。

1.  Cloud Loggingで、エクスポートしたいログ（この場合はCloud VPNのログ）に一致する**フィルタ**を作成します。
2.  エクスポートの宛先（シンク）として、**Cloud Storageバケット**を指定します。
3.  必要に応じて、Cloud Storageのライフサイクル管理を使用して、さらに低コストなストレージクラス（Nearline, Coldline, Archive）に移動させることもできます。
    これにより、コンプライアンス要件を満たすためにログを安価に長期間保存できます。

-----

### <a name="no230"></a>**NO.230**

あなたはCloud Shellを使用しており、数週間後に使用するためのカスタムユーティリティをインストールする必要があります。デフォルトの実行パスにあり、セッション間で永続化するようにファイルをどこに保存できますか？

A. ~/bin
B. Cloud Storage
C. /google/scripts
D. /usr/local/bin

**正解: A**

**解説:**
Cloud Shellの環境では、ユーザーの**ホームディレクトリ (`~` または `/home/<username>`)** が永続的な5GBのディスクストレージとして提供され、セッションをまたいでファイルが保持されます。
Linuxの標準的な慣習として、ユーザー個人の実行可能ファイルを置くためのディレクトリとして`~/bin`が用意されており、このディレクトリはデフォルトで環境変数`$PATH`（実行パス）に含まれています。したがって、カスタムユーティリティを`~/bin`に置くことで、どのセッションからでもコマンド名だけで実行でき、かつ永続化されます。

-----

### <a name="no231"></a>**NO.231**

あなたのGoogle Kubernetes Engineクラスタが、CPU負荷に基づいて自動的にノードを追加または削除するようにしたいです。どうすべきですか？

A. ターゲットCPU使用率を持つHorizontalPodAutoscalerを構成する。GCPコンソールからCluster Autoscalerを有効にする。
B. ターゲットCPU使用率を持つHorizontalPodAutoscalerを構成する。gcloudコマンドを使用してクラスタのマネージドインスタンスグループで自動スケーリングを有効にする。
C. デプロイメントを作成し、maxUnavailableおよびmaxSurgeプロパティを設定する。gcloudコマンドを使用してCluster Autoscalerを有効にする。
D. デプロイメントを作成し、maxUnavailableおよびmaxSurgeプロパティを設定する。GCPコンソールからクラスタのマネージドインスタンスグループで自動スケーリングを有効にする。

**正解: A**

**解説:**
GKEにおける自動スケーリングは、Podレベルとノードレベルの2段階で考えます。

1.  **Podのスケーリング (HPA)**: まず、**HorizontalPodAutoscaler (HPA)** を構成して、DeploymentのCPU使用率がターゲットを超えたら、Kubernetesが自動的に**Podのレプリカ数を増やす**ようにします。
2.  **ノードのスケーリング (CA)**: HPAによってPodが増やされた結果、クラスタ内の既存のノードにPodをスケジュールするのに十分なCPUリソースがなくなると、**Cluster Autoscaler (CA)** がそれを検知して、クラスタに新しい**ノード（VM）を自動的に追加**します。
    この両方を有効にすることで、CPU負荷に基づいた完全な自動スケーリングが実現します。

-----

### <a name="no232"></a>**NO.232**

あなたの会社はヘルスケアのスタートアップを買収し、顧客の医療情報を、作成時期に応じて最大4年間保持する必要があります。あなたの企業ポリシーは、このデータを安全に保持し、規制が許す限り速やかに削除することです。どのアプローチを取るべきですか？

A. データをGoogle Driveに保存し、期限が切れたレコードを手動で削除する。
B. Cloud Data Loss Prevention APIを使用してデータを匿名化し、無期限に保存する。
C. Cloud Storageを使用してデータを保存し、ライフサイクル管理を使用して期限切れのファイルを削除する。
D. Cloud Storageにデータを保存し、期限切れのすべてのデータを削除する夜間バッチスクリプトを実行する。

**正解: C**

**解説:**
「規制が許す限り速やかに削除する」という要件には、自動化された削除プロセスが不可欠です。**Cloud Storageのオブジェクトライフサイクル管理**は、まさにこのために設計された機能です。

  * オブジェクトの作成からの経過日数（Age）を条件とするルールを設定できます。
  * 条件に一致したオブジェクトに対して、**削除 (Delete)** アクションを自動的に実行するように構成できます。
    これにより、手動での削除(A)やバッチスクリプトの管理(D)といった運用負荷やヒューマンエラーのリスクなしに、コンプライアンス要件を確実に満たすことができます。

-----

### <a name="no233"></a>**NO.233**

この問題については、Dress4Winのケーススタディを参照してください。

Dress4Winは、アプリケーションサーバーをデプロイすべきマシンタイプについて、あなたに推奨を求めてきました。どのように進めるべきですか？

A. オンプレミスの物理ハードウェアのコアとRAMを、クラウドの最も近いマシンタイプにマッピングする。
B. Dress4Winが、利用可能な最高のRAM対CPU比を提供するマシンタイプにアプリケーションサーバーをデプロイすることを推奨する。
C. Dress4Winが、利用可能な最小のインスタンスで本番環境にデプロイし、時間をかけて監視し、望ましいパフォーマンスに達するまでマシンタイプをスケールアップすることを推奨する。
D. アプリケーションサーバー仮想マシンに関連付けられた仮想コアの数とRAMを特定し、それらをクラウドのカスタムマシンタイプに合わせ、パフォーマンスを監視し、望ましいパフォーマンスに達するまでマシンタイプをスケールアップする。

**正解: D**

**解説:**
PDFの回答Cは、本番環境で最小から始めるリスクが高すぎます。
クラウドへのリフト＆シフトにおけるサイジングのベストプラクティスは、**現在の使用状況をベースラインとし、クラウド上で最適化する**ことです。

1.  まず、オンプレミスのVMのvCPUとRAMの構成を把握し、それに最も近い**カスタムマシンタイプ**をCompute Engineで選択します。カスタムマシンタイプを使用することで、オンプレミスの構成に正確に合わせ、無駄なリソースを省くことができます。
2.  クラウドにデプロイした後、**Cloud Monitoring**を使用して実際のパフォーマンス（CPU使用率、メモリ使用率など）を監視します。
3.  監視データと**サイズ適正化の推奨**に基づいて、インスタンスが過剰/過小であれば、マシンタイプをスケールアップまたはダウンして、パフォーマンスとコストのバランスを最適化します。

-----

### <a name="no234"></a>**NO.234**

この問題については、Dress4Winのケーススタディを参照してください。

Dress4Winの運用エンジニアが、データベースバックアップファイルのコピーをリモートでアーカイブするための低コストなソリューションを作成したいと考えています。データベースファイルは、現在のデータセンターに保存されている圧縮されたtarファイルです。彼はどのように進めるべきですか？

A. `gsutil`を使用してファイルをColdline Storageバケットにコピーするcronスクリプトを作成する。
B. `gsutil`を使用してファイルをRegional Storageバケットにコピーするcronスクリプトを作成する。
C. Cloud Storage Transfer Serviceジョブを作成して、ファイルをColdline Storageバケットにコピーする。
D. Cloud Storage Transfer Serviceジョブを作成して、ファイルをRegional Storageバケットにコピーする。

**正解: A**

**解説:**
要件は「低コストなアーカイブ」と「オンプレミスからのコピー」です。

  * **低コストなアーカイブ**: **Coldline**ストレージクラスは、アクセス頻度が低いデータの長期保存用に設計されており、ストレージコストが非常に低いです。
  * **オンプレミスからの転送**: **`gsutil`は、オンプレミスのマシンからCloud Storageにファイルを転送するための標準的なコマンドラインツールです。これをcron**ジョブとしてスケジュールすることで、バックアップファイルの定期的なアーカイブを自動化できます。
    Storage Transfer Service(C, D)は、主に他のクラウドプロバイダーやGCSバケット間での転送に使用されます。

-----

### <a name="no235"></a>**NO.235**

あなたのWebアプリケーションには、単一のVPC内で実行される複数のVMインスタンスがあります。インスタンス間の通信を、あなたが承認したパスとポートのみに制限したいですが、アプリが自動スケーリングできるため、静的なIPアドレスやサブネットに依存したくありません。どのように通信を制限すべきですか？

A. トラフィックを制限するために別のVPCを使用する。
B. コンピュートインスタンスにアタッチされたネットワークタグに基づくファイアウォールルールを使用する。
C. Cloud DNSを使用し、承認されたホスト名からの接続のみを許可する。
D. サービスアカウントを使用し、特定のサービスアカウントがアクセスできるようにWebアプリケーションを構成する。

**正解: B**

**解説:**
(注：この問題はNo.114とほぼ同じです。)
IPアドレスが動的に変わる自動スケーリング環境で、インスタンス間の通信を制御するためのベストプラクティスは、**ネットワークタグ**と**ファイアウォールルール**を組み合わせることです。

1.  役割ごとにインスタンスに**タグ**を付けます（例: `frontend`, `backend`）。
2.  ファイアウォールルールを、IPアドレスではなく**タグ**をソースまたはターゲットとして定義します（例: 「ソースタグが`frontend`で、ターゲットタグが`backend`のインスタンスへのTCPポート8080のトラフィックを許可する」）。
    インスタンスが自動スケーリングで作成されると、インスタンステンプレートに基づいて自動的にタグが付与され、ファイアウォールルールが即座に適用されます。

-----

### <a name="no236"></a>**NO.236**

この問題については、Dress4Winのケーススタディを参照してください。

法的に準拠するために、監査中、Dress4WinはGoogle Cloud上のリソースの構成またはメタデータを変更するすべての管理アクションに関する洞察を提供できなければなりません。どうすべきですか？

A. Stackdriver Traceを使用してトレースリスト分析を作成する。
B. Stackdriver Monitoringを使用してプロジェクトのアクティビティに関するダッシュボードを作成する。
C. すべてのプロジェクトでCloud Identity-Aware Proxyを有効にし、管理者グループをメンバーとして追加する。
D. GCPコンソールの[アクティビティ]ページとStackdriver Loggingを使用して、必要な洞察を提供する。

**正解: D**

**解説:**
PDFの回答Aは不適切です。Traceはレイテンシ分析用です。
「誰が、いつ、どこで、何をしたか」という管理アクションを記録するのは、**Cloud Audit Logs**の役割です。このログは、**Cloud Logging (旧Stackdriver Logging)** で閲覧、検索、フィルタリングが可能です。また、GCPコンソールの**[アクティビティ]** ストリーム（監査ログの簡易ビュー）でも、最近の管理アクションを簡単に確認できます。監査人には、Cloud Loggingでフィルタリングした結果や、[アクティビティ]ページのスクリーンショットなどを提示することで、要件を満たすことができます。

-----

### <a name="no237"></a>**NO.237**

この問題については、Dress4Winのケーススタディを参照してください。

どのコンピュートサービスを現状のまま（as-is）移行しても、クラウドでのパフォーマンスが最適化されたアーキテクチャとなりますか？

A. App Engine標準環境を使用してデプロイされたWebアプリケーション
B. 非マネージドインスタンスグループを使用してデプロイされたRabbitMQ
C. 高可用性モードのCloud Dataproc Regionalを使用してデプロイされたHadoop/Spark
D. カスタムマシンタイプにデプロイされたJenkins、監視、踏み台ホスト、セキュリティスキャナサービス

**正解: C**

**解説:**
ケーススタディには、オンプレミスで20台の**Hadoop/Sparkサーバー**を運用していると記載されています。これをGoogle Cloudに移行する場合、**Cloud Dataproc**が最適です。Cloud Dataprocは、既存のHadoop/Sparkジョブをほぼ変更なく実行できるマネージドサービスです。高可用性（HA）モードやリージョンエンドポイントなどの機能により、クラウドネイティブな可用性とパフォーマンスを提供しつつ、既存のワークロードを「as-is」に近い形で移行できます。

-----

### <a name="no238"></a>**NO.238**

この問題については、Dress4Winのケーススタディを参照してください。

与えられたビジネス要件を考慮して、Webおよびトランザクションデータ層のデプロイをどのように自動化しますか？

A. Cloud Deployment Managerを使用してNginxとTomcatをCompute Engineにデプロイする。MySQLを置き換えるためにCloud SQLサーバーをデプロイする。Cloud Deployment Managerを使用してJenkinsをデプロイする。
B. Cloud Launcherを使用してNginxとTomcatをデプロイする。Cloud Launcherを使用してMySQLサーバーをデプロイする。Cloud Deployment Managerスクリプトを使用してJenkinsをCompute Engineにデプロイする。
C. NginxとTomcatをApp Engineに移行する。高可用性構成でMySQLサーバーを置き換えるためにCloud Datastoreサーバーをデプロイする。Cloud Launcherを使用してJenkinsをCompute Engineにデプロイする。
D. NginxとTomcatをApp Engineに移行する。Cloud Launcherを使用してMySQLサーバーをデプロイする。Cloud Launcherを使用してJenkinsをCompute Engineにデプロイする。

**正解: A**

**解説:**
ビジネス要件には「信頼性が高く再現可能な環境の構築」「ビジネスの俊敏性と革新の速度の向上」が含まれており、これはインフラの**自動化**を強く示唆しています。

  * **Cloud Deployment Manager**は、Google Cloudリソースをテンプレート（YAML）で宣言的に定義し、一貫性のある環境を繰り返しプロビジョニングするためのInfrastructure as Code (IaC) ツールです。Nginx/Tomcatを実行するCompute Engineインスタンスや、Jenkinsサーバーのデプロイを自動化するのに適しています。
  * オンプレミスのMySQL（トランザクションデータ層）の移行先としては、フルマネージドなリレーショナルデータベースサービスである**Cloud SQL**が最適です。これにより、運用負荷が大幅に軽減されます。

-----

### <a name="no239"></a>**NO.239**

この問題については、Dress4Winのケーススタディを参照してください。

Dress4Winは、1年でサイズが10倍に成長し、それに伴いデータとトラフィックが既存の使用パターンを反映して増加すると予想されています。CIOは、今後6ヶ月以内に本番インフラをクラウドに移行するという目標を設定しました。主要なアプリケーションの変更を行わずに、この成長に対応するためにソリューションをどのように構成し、ROIを最大化しますか？

A. Webアプリケーション層をApp Engineに、MySQLをCloud Datastoreに、NASをCloud Storageに移行する。RabbitMQをデプロイし、Deployment Managerを使用してHadoopサーバーをデプロイする。
B. RabbitMQをCloud Pub/Subに、HadoopをBigQueryに、NASを永続ディスクストレージを備えたCompute Engineに移行する。Tomcatをデプロイし、Deployment Managerを使用してNginxをデプロイする。
C. TomcatとNginxにマネージドインスタンスグループを実装する。MySQLをCloud SQLに、RabbitMQをCloud Pub/Subに、HadoopをCloud Dataprocに、NASを永続ディスクストレージを備えたCompute Engineに移行する。
D. TomcatとNginxにマネージドインスタンスグループを実装する。MySQLをCloud SQLに、RabbitMQをCloud Pub/Subに、HadoopをCloud Dataprocに、NASをCloud Storageに移行する。

**正解: D**

**解説:**
これは、オンプレミスの各コンポーネントを、Google Cloudの最適な**マネージドサービス**に置き換える問題です。マネージドサービスを利用することで、スケーラビリティを確保し、運用負荷を下げ、ROIを最大化できます。

  * **Tomcat/Nginx (Web層)**: **マネージドインスタンスグループ (MIG)** で自動スケーリングを構成します。
  * **MySQL (リレーショナルDB)**: **Cloud SQL**に移行します。
  * **RabbitMQ (メッセージキュー)**: **Cloud Pub/Sub**に移行します。
  * **Hadoop/Spark (データ分析)**: **Cloud Dataproc**に移行します。
  * **NAS (画像ストレージ)**: 画像のようなオブジェクトデータを大量に保存するには、**Cloud Storage**が最適です。

-----

### <a name="no240"></a>**NO.240**

この問題については、Dress4Winのケーススタディを参照してください。

ソリューションを移行する前に、オンプレミスのアーキテクチャがビジネス要件を満たしていることを確認したいです。オンプレミスのアーキテクチャでどのような変更を行うべきですか？

A. RabbitMQをGoogle Pub/Subに置き換える。
B. MySQLをCloud SQL for MySQLでサポートされているv5.7にダウングレードする。
C. コンピュートリソースを、事前に定義されたCompute Engineマシンタイプに合わせてリサイズする。
D. マイクロサービスをコンテナ化し、Google Kubernetes Engineでホストする。

**正解: C**

**解説:**
この問題は、クラウド移行の**準備**としてオンプレミスで何ができるかを問うています。クラウド移行を計画する上で重要なのは、移行後のコストとパフォーマンスを正確に見積もることです。オンプレミスのVMのCPUやメモリ構成が、Google Cloudの**標準的なマシンタイプ**と大きく異なっていると、サイジング（適切なマシンタイプの選択）が難しくなります。移行前に、オンプレミスのVM構成をGCPのマシンタイプに合わせておくことで、より正確なパフォーマンス比較やコスト試算が可能になり、移行計画がスムーズに進みます。

-----

### <a name="no241"></a>**NO.241**

この問題については、Dress4Winのケーススタディを参照してください。

あなたは、あなたの会社Dress4WinのためにCloud Storageに保存されたデータのセキュリティを担当しています。あなたは既にGoogleグループのセットを作成し、それらのグループに適切なユーザーを割り当てています。Googleのベストプラクティスを使用し、要件を満たすために最もシンプルな設計を実装すべきです。Dress4Winのビジネスおよび技術要件を考慮して、どうすべきですか？

A. セキュリティ要件を強制するために、作成したGoogleグループにカスタムIAMロールを割り当てる。Cloud Storageにファイルを保存する際に、顧客提供の暗号化キーでデータを暗号化する。
B. セキュリティ要件を強制するために、作成したGoogleグループにカスタムIAMロールを割り当てる。Cloud Storageにファイルを保存する前に、デフォルトのストレージ暗号化を有効にする。
C. セキュリティ要件を強制するために、作成したGoogleグループに事前定義IAMロールを割り当てる。Cloud Storageにファイルを保存する際に、Googleのデフォルトの保存時の暗号化を利用する。
D. セキュリティ要-件を強制するために、作成したGoogleグループに事前定義IAMロールを割り当てる。Cloud Storageにファイルを保存する前に、デフォルトのCloud KMSキーが設定されていることを確認する。

**正解: C**

**解説:**
Google CloudのIAMとセキュリティに関する基本原則とベストプラクティスを問う問題です。

  * **IAM**: **事前定義ロール**は、一般的なユースケースに合わせてGoogleが作成・管理しているロールです（例: `roles/storage.objectViewer`）。まずは事前定義ロールで要件を満たせないか検討するのが、最もシンプルで推奨されるアプローチです。カスタムロールは、事前定義ロールでは権限が過不足な場合にのみ作成します。また、ユーザー個人ではなく**Googleグループ**にロールを割り当てるのがベストプラクティスです。
  * **暗号化**: Cloud Storageは、ユーザーが何もしなくても、**デフォルトで保存されるすべてのデータを自動的に暗号化**します。特別な要件がない限り、このデフォルトの暗号化を利用するのが最もシンプルです。

-----

### <a name="no242"></a>**NO.242**

この問題については、TerramEarthのケーススタディを参照してください。

あなたは、セルラーネットワークに接続された20万台の車両のデータ取り込みのための新しいアーキテクチャを設計するように依頼されました。Google推奨のプラクティスに従いたいです。技術要件を考慮して、データの取り込みにはどのコンポーネントを使用すべきですか？

A. SSL Ingressを備えたGoogle Kubernetes Engine
B. 公開/秘密鍵ペアを備えたCloud IoT Core
C. プロジェクト全体のSSHキーを備えたCompute Engine
D. 特定のSSHキーを備えたCompute Engine

**正解: B**

**解説:**
PDFの回答Aも可能ですが、Bの方がIoTデバイス管理にはより特化しています。
車両のような多数のデバイスからテレメトリデータを安全かつ大規模に取り込むために設計されたサービスが**Cloud IoT Core**です。

  * **デバイス管理**: 数百万台のデバイスを登録し、管理・監視できます。
  * **安全な認証**: 各デバイスに**公開/秘密鍵ペア**を持たせ、JWT（JSON Web Token）ベースの認証を行うことで、安全に通信できます。
  * **プロトコルサポート**: 軽量なIoTプロトコルである**MQTT**やHTTPをサポートしています。
  * **Pub/Sub連携**: 受信したデータは自動的にCloud Pub/Subトピックに転送され、後続のデータ処理パイプラインに簡単に接続できます。
    これがIoTデータ取り込みのベストプラクティスです。

-----

### <a name="no243"></a>**NO.243**

この問題については、TerramEarthのケーススタディを参照してください。

技術要件を考慮して、GCPで計画外の車両ダウンタイムをどのように削減しますか？

A. データウェアハウスとしてBigQueryを使用する。すべての車両をネットワークに接続し、Cloud Pub/SubとCloud Dataflowを使用してデータをBigQueryにストリーミングする。分析とレポートにはGoogle Data Studioを使用する。
B. データウェアハウスとしてBigQueryを使用する。すべての車両をネットワークに接続し、gcloudを使用してgzipファイルをマルチリージョンCloud Storageバケットにアップロードする。分析とレポートにはGoogle Data Studioを使用する。
C. データウェアハウスとしてCloud Dataproc Hiveを使用する。gzipファイルをマルチリージョンCloud Storageバケットにアップロードする。gcloudを使用してこのデータをBigQueryにアップロードする。分析とレポートにはGoogle Data Studioを使用する。
D. データウェアハウスとしてCloud Dataproc Hiveを使用する。データを直接パーティション分割されたHiveテーブルにストリーミングする。データ分析にはPigスクリプトを使用する。

**正解: A**

**解説:**
ダウンタイムを削減するには、故障の予兆を**リアルタイム**で検知し、予測する必要があります。ケーススタディでは、既存のバッチ処理ではデータが3週間も古くなってしまうことが問題点として挙げられています。
選択肢Aは、この問題を解決するための**リアルタイム分析パイプライン**の完全なアーキテクチャを示しています。

  * **Cloud Pub/Sub**: 車両からのテレメトリデータをリアルタイムで取り込みます。
  * **Cloud Dataflow**: Pub/Subからのデータをストリーム処理し、変換・集約します。
  * **BigQuery**: 処理されたデータをリアルタイムで取り込み、アナリストがSQLで即座に分析できるようにします。
  * **Data Studio**: BigQueryのデータを可視化し、ダッシュボードを作成します。
    この構成により、データの鮮度が劇的に向上し、ダウンタイムの削減に繋がります。

-----

### <a name="no244"></a>**NO.244**

この問題については、TerramEarthのケーススタディを参照してください。

すべての受信データをBigQueryに書き込む新しいアーキテクチャが導入されました。データが汚れていることに気づき、コストを管理しながら自動化された日次ベースでデータ品質を確保したいです。どうすべきですか？

A. 取り込みプロセスからデータを受信するストリーミングCloud Dataflowジョブを設定する。Cloud Dataflowパイプラインでデータをクリーンにする。
B. BigQueryからデータを読み取り、それをクリーンにするCloud Functionを作成する。それをトリガーする。Compute EngineインスタンスからCloud Functionをトリガーする。
C. BigQueryのデータに対してSQLステートメントを作成し、ビューとして保存する。毎日ビューを実行し、結果を新しいテーブルに保存する。
D. Cloud Dataprepを使用し、BigQueryテーブルをソースとして構成する。データをクリーンにするための日次ジョブをスケジュールする。

**正解: D**

**解説:**
PDFの回答Aはストリーミング処理であり、日次バッチの要件とは異なります。
「データが汚れている」状態をクレンジングし、「日次で自動化」し、「コストを管理（コード開発の手間を省く）」したいという要件に最も適しているのは**Cloud Dataprep**です。

  * Dataprepは、BigQueryを直接ソースとしてデータを読み込めます。
  * GUIベースの直感的な操作で、データのクレンジングや変換のルール（レシピ）を作成できます。
  * 作成したレシピの実行を**スケジュール**することができ、毎日自動でデータをクレンジングして結果を新しいBigQueryテーブルに書き出す、といったETL/ELTパイプラインを簡単に構築できます。

-----

### <a name="no245"></a>**NO.245**

この問題については、TerramEarthのケーススタディを参照してください。

欧州のGDPR規制に準拠するため、TerramEarthは、個人データを含む場合、欧州の顧客から生成されたデータを36ヶ月後に削除する必要があります。新しいアーキテクチャでは、このデータはCloud StorageとBigQueryの両方に保存されます。どうすべきですか？

A. 欧州データ用にBigQueryテーブルを作成し、テーブルの保持期間を36ヶ月に設定する。Cloud Storageについては、gsutilを使用して、Age条件が36ヶ月のDELETEアクションでライフサイクル管理を有効にする。
B. 欧州データ用にBigQueryテーブルを作成し、テーブルの保持期間を36ヶ月に設定する。Cloud Storageについては、gsutilを使用して、Age条件が36ヶ月のSetStorageClass to NONEアクションを作成する。
C. 欧州データ用にBigQueryの時系列パーティション分割テーブルを作成し、パーティションの有効期限を36ヶ月に設定する。Cloud Storageについては、gsutilを使用して、Age条件が36ヶ月のDELETEアクションでライフサイクル管理を有効にする。
D. 欧州データ用にBigQueryの時系列パーティション分割テーブルを作成し、パーティションの期間を36ヶ月に設定する。Cloud Storageについては、gsutilを使用して、Age条件が36ヶ月のSetStorageClass to NONEアクションを作成する。

**正解: C**

**解説:**
両方のサービスで、自動的にデータを削除する機能を設定するのがベストプラクティスです。

  * **BigQuery**: ログや時系列データを扱う場合、**時系列パーティション分割テーブル**を使用するのが最適です。**パーティションの有効期限**を36ヶ月（1095日など）に設定すると、古いパーティションから自動的かつ無料でデータが削除されていきます。テーブル全体の有効期限(A)だと、テーブルごと消えてしまいます。
  * **Cloud Storage**: **オブジェクトのライフサイクル管理**機能を使用します。オブジェクトの作成からの経過日数（Age）が36ヶ月を超えたら、**DELETE**アクションを実行するルールを設定します。これにより、古いファイルが自動的に削除されます。

-----

### <a name="no246"></a>**NO.246**

この問題については、TerramEarthのケーススタディを参照してください。

あなたは、あなたの会社TerramEarthのために、データウェアハウスのための信頼性が高く、スケーラブルなGCPソリューションを実装する必要があります。TerramEarthのビジネスおよび技術要件を考慮して、どうすべきですか？

A. 既存のデータウェアハウスをBigQueryに置き換える。テーブルパーティショニングを使用する。
B. 既存のデータウェアハウスを96CPUのCompute Engineインスタンスに置き換える。
C. 既存のデータウェアハウスをBigQueryに置き換える。フェデレーションデータソースを使用する。
D. 既存のデータウェアハウスを96CPUのCompute Engineインスタンスに置き換える。追加の32CPUのプリエンプティブルCompute Engineインスタンスを追加する。

**正解: A**

**解説:**
PDFの回答Cも良い選択ですが、Aの方がより基本的で重要なプラクティスです。
ケーススタディには、大量の車両データを扱うこと、データ鮮度を向上させる必要があることなどが記載されています。

  * **BigQueryへの置き換え**: オンプレミスの単一PostgreSQLサーバーではスケーラビリティに限界があります。サーバーレスでペタバイト級のデータを扱える**BigQuery**に置き換えるのが最適な選択です。
  * **テーブルパーティショニング**: BigQueryで時系列データを扱う際の最も重要なベストプラクティスの一つが**テーブルパーティショニング**です。日付などでテーブルを分割することで、クエリの対象となるデータ量を限定でき、**パフォーマンス向上**と**コスト削減**の両方に大きく貢献します。

-----

### <a name="no247"></a>**NO.247**

この問題については、TerramEarthのケーススタディを参照してください。

あなたは、バックエンドにいくつかのCloud Functionsを使用する新しいアプリケーションの構築を開始します。あるユースケースでは、Cloud Function `func_display`が別のCloud Function `func_query`を呼び出す必要があります。`func_query`が`func_display`からの呼び出しのみを受け入れるようにしたいです。また、Googleの推奨ベストプラクティスに従いたいです。どうすべきですか？

A. トークンを作成し、それを環境変数として`func_display`に渡す。`func_query`を呼び出す際に、リクエストにトークンを含める。同じトークンを`func_query`に渡し、トークンが異なる場合は呼び出しを拒否する。
B. `func_query`を「認証が必要」にする。一意のサービスアカウントを作成し、それを`func_display`に関連付ける。そのサービスアカウントに`func_query`の呼び出し元ロールを付与する。`func_display`でIDトークンを作成し、`func_query`を呼び出す際にリクエストにトークンを含める。
C. `func_query`を「認証が必要」にし、内部トラフィックのみを受け入れるようにする。これら2つの関数を同じVPCに作成する。`func_query`に、`func_display`からのトラフィックのみを許可するイングレスファイアウォールルールを作成する。
D. これら2つの関数を同じプロジェクトとVPCに作成する。`func_query`が内部トラフィックのみを受け入れるようにする。`func_query`に、`func_display`からのトラフィックのみを許可するイングレスファイアウォールを作成する。また、両方の関数が同じサービスアカウントを使用することを確認する。

**正解: B**

**解説:**
Cloud Functions間の認証のための、最も安全で推奨される方法は、IAMとサービスアカウントを利用することです。

1.  **呼び出される側 (`func_query`)**: Ingress設定で「認証が必要」を選択します。これにより、未認証のリクエストは拒否されるようになります。
2.  **呼び出す側 (`func_display`)**: 専用の**サービスアカウント**を作成し、`func_display`にアタッチします。
3.  **権限付与**: `func_query`に対して、`func_display`のサービスアカウントに**Cloud Functions 起動元 (`roles/cloudfunctions.invoker`)** ロールを付与します。
4.  **呼び出し**: `func_display`のコード内で、Googleの認証ライブラリを使用して、ターゲット (`func_query`のURL) を指定した**IDトークン**を自動的に取得し、そのトークンをAuthorizationヘッダーに含めて`func_query`を呼び出します。
    これにより、最小権限の原則に従った、安全で管理しやすい認証が実現できます。

-----

### <a name="no248"></a>**NO.248**

あなたは、レガシーなモノリシックアプリケーションをいくつかのコンテナ化されたRESTfulマイクロサービスに分解しました。これらのマイクロサービスをCloud Runで実行したいです。また、サービスが高可用性で、顧客へのレイテンシが低いことを確認したいです。どうすべきですか？

A. Cloud Runサービスを複数の可用性ゾーンにデプロイする。サービスを指すCloud Endpointsを作成する。グローバルHTTP(S)ロードバランシングインスタンスを作成し、そのバックエンドにCloud Endpointsをアタッチする。
B. Cloud Runサービスを複数のリージョンにデプロイする。サービスを指すサーバーレスネットワークエンドポイントグループを作成する。サーバーレスNEGを、グローバルHTTP(S)ロードバランシングインスタンスが使用するバックエンドサービスに追加する。
C. Cloud Runサービスを複数のリージョンにデプロイする。Cloud DNSで、サービスを指すレイテンシベースのDNS名を作成する。
D. Cloud Runサービスを複数の可用性ゾーンにデプロイする。TCP/IPグローバルロードバランサを作成する。そのバックエンドサービスにCloud Runエンドポイントを追加する。

**正解: B**

**解説:**
複数のリージョンにまたがるCloud Runサービスに対して、単一のエンドポイントでグローバルな負荷分散と低レイテンシを実現するための標準的な方法は、**グローバル外部HTTP(S)ロードバランサ**と**サーバーレスNEG (Network Endpoint Group)** を組み合わせることです。

1.  まず、**複数のリージョン**（例: `us-central1`, `asia-northeast1`）にCloud Runサービスをデプロイします。
2.  各リージョンのCloud Runサービスを指す**サーバーレスNEG**を作成します。
3.  **グローバル外部HTTP(S)ロードバランサ**のバックエンドサービスに、これらのサーバーレスNEGを追加します。
    これにより、ロードバランサはユーザーからのリクエストを、ネットワーク的に最も近いリージョンで実行されているCloud Runサービスに自動的にルーティングします。

-----

### <a name="no249"></a>**NO.249**

この問題については、TerramEarthのケーススタディを参照してください。

TerramEarthはデータファイルをCloud Storageに保存することを決定しました。1年間のデータを保存し、ファイルストレージコストを最小限に抑えるためにCloud Storageのライフサイクルルールを構成する必要があります。どの2つのアクションを実行すべきですか？

A. Age: "30", Storage Class: "Standard", Action: "Set to Coldline" のCloud Storageライフサイクルルールを作成し、Age: "365", Storage Class: "Coldline", Action: "Delete" の2番目のGCSライフサイクルルールを作成する。
B. Age: "30", Storage Class: "Coldline", Action: "Set to Nearline" のCloud Storageライフサイクルルールを作成し、Age: "91", Storage Class: "Coldline", Action: "Set to Nearline" の2番目のGCSライフサイクルルールを作成する。
C. Age: "90", Storage Class: "Standard", Action: "Set to Nearline" のCloud Storageライフサイクルルールを作成し、Age: "91", Storage Class: "Nearline", Action: "Set to Coldline" の2番目のGCSライフサイクルルールを作成する。
D. Age: "30", Storage Class: "Standard", Action: "Set to Coldline" のCloud Storageライフサイクルルールを作成し、Age: "365", Storage Class: "Nearline", Action: "Delete" の2番目のGCSライフサイクルルールを作成する。

**正解: A**

**解説:**
コストを最小化するための一般的なデータ階層化戦略は、データのアクセス頻度が時間とともに低下することを利用します。

  * **最初の30日間**: データは頻繁にアクセスされる可能性があるため、**Standard**クラスに保存します。
  * **30日後**: アクセス頻度が低下するため、より安価な**Coldline**クラスに自動的に移動させます。（Nearlineを挟んでも良いですが、ここではよりコスト削減効果の高いColdlineに直接移動させています）
  * **365日後**: データは不要になるため、**Delete**アクションで自動的に削除します。
    このライフサイクルルールを設定することで、手動での操作なしに、データのライフサイクル全体にわたってストレージコストを最適化できます。

-----

### <a name="no250"></a>**NO.250**

TerramEarthは、プライベートデータセンターに約1ペタバイト（PB）の車両テストデータを持っています。機械学習チームのために、このデータをCloud Storageに移動したいと考えています。現在、1Gbpsのインターコネクトリンクが利用可能です。機械学習チームは1ヶ月以内にデータの使用を開始したいと考えています。どうすべきですか？

A. Google CloudからTransfer Applianceをリクエストし、データをアプライアンスにエクスポートし、アプライアンスをGoogle Cloudに返送する。
B. Google CloudからStorage Transferサービスを構成して、データセンターからCloud Storageにデータを送信する。
C. 1Gbpsリンクを他のユーザーが消費していないことを確認し、マルチスレッド転送を使用してデータをCloud Storageにアップロードする。
D. 暗号化されたUSBデバイスにファイルをエクスポートし、デバイスをGoogle Cloudに送り、データのCloud Storageへのインポートをリクエストする。

**正解: A**

**解説:**
1ペタバイトという非常に大規模なデータを1ヶ月という短期間で転送する必要があります。

  * **ネットワーク転送の試算**: 1Gbpsのリンクを最大限利用できたとしても、1PBの転送には計算上100日以上かかり、1ヶ月という要件を満たせません。
  * **Transfer Appliance**: このような大規模なオフラインデータ移行のためにGoogleが提供しているのが**Transfer Appliance**です。物理的なアプライアンスにデータをコピーしてGoogleに郵送することで、ネットワーク帯域幅の制約を受けずに、大量のデータを迅速かつ安全にCloud Storageに移行できます。
    これが、このシナリオにおける唯一現実的な選択肢です。

-----

### <a name="no251"></a>**NO.251**

TerramEarthは、クラウドに移行できないレガシーなWebアプリケーションを持っています。しかし、それでもアプリケーションを監視するためのクラウドネイティブな方法を構築したいです。アプリケーションがダウンした場合、URLができるだけ早く「サイト利用不可」ページを指すようにしたいです。また、運用チームが問題について通知を受け取るようにしたいです。最小限のコストで信頼性の高いソリューションを構築する必要があります。どうすべきですか？

A. Cloud Runでスケジュールされたジョブを作成し、毎分コンテナを呼び出す。コンテナはアプリケーションのURLをチェックする。アプリケーションがダウンしている場合、URLを「サイト利用不可」ページに切り替え、運用チームに通知する。
B. Compute Engine VM上で毎分実行されるcronジョブを作成する。cronジョブはPythonプログラムを呼び出してアプリケーションのURLをチェックする。アプリケーションがダウンしている場合、URLを「サイト利用不可」ページに切り替え、運用チームに通知する。
C. Cloud Monitoringの稼働時間チェックを作成してアプリケーションのURLを検証する。失敗した場合、Pub/Subキューにメッセージを入れ、それがCloud FunctionをトリガーしてURLを「サイト利用不可」ページに切り替え、運用チームに通知する。
D. Cloud Error Reportingを使用してアプリケーションのURLをチェックする。アプリケーションがダウンしている場合、URLを「サイト利用不可」ページに切り替え、運用チームに通知する。

**正解: C**

**解説:**
これは、サーバーレスコンポーネントを組み合わせた、信頼性が高くコスト効率の良い監視と自動対応のパターンです。

  * [cite_start]**監視 (Cloud Monitoring Uptime Check)**: 外部から定期的にURLの可用性をチェックするための、安価でフルマネージドな機能です [cite: 3676]。
  * [cite_start]**イベント通知 (Pub/Sub)**: 稼働時間チェックが失敗すると、アラート通知チャネルとして**Pub/Sub**を指定できます。これにより、障害イベントがキューに確実に入ります [cite: 3676]。
  * [cite_start]**自動アクション (Cloud Function)**: Pub/Subトピックをトリガーとして**Cloud Function**を起動します。このFunctionが、DNSレコードの変更（URLの切り替え）や運用チームへの通知（例: Slack, PagerDuty APIコール）といった自動化されたアクションを実行します [cite: 3676]。
    このアーキテクチャは完全にサーバーレスであり、常時稼働のVM(B)やコンテナ(A)を必要としないため、コストを最小限に抑えられます。

-----

### <a name="no252"></a>**NO.252**

あなたは、プライベートデータセンターからGoogle CloudにLinuxベースのアプリケーションを移行しています。TerramEarthのセキュリティチームから、Common Vulnerabilities and Exposures (CVE) によって公開されたいくつかの最近のLinuxの脆弱性が送られてきました。これらの脆弱性があなたの移行にどのように影響するかを理解するのに支援が必要です。どうすべきですか？

A. CVEに関するサポートケースを開き、サポートエンジニアとチャットする。
B. Google CloudステータスダッシュボードからCVEを読み、影響を理解する。
C. Google Cloud Platformセキュリティ情報からCVEを読み、影響を理解する。
D. Stack OverflowでCVEに関する質問を投稿し、説明を得る。
E. Google CloudディスカッショングループでCVEに関する質問を投稿し、説明を得る。

**正解: C**

**解説:**
[cite_start]Google Cloudは、自社のプラットフォームや、提供するベースOSイメージに影響を与える可能性のある脆弱性について、**Google Cloud Platform セキュリティ情報 (Security Bulletins)** を通じて公式に情報を公開しています [cite: 3683]。特定のCVEがGoogle Cloud環境にどのような影響を与えるか、Googleがどのような対策を講じているか、ユーザー側でどのような対応が必要か、といった信頼できる情報を得るためには、まずこの公式のセキュリティ情報を確認するのが正しいアプローチです。サポートへの問い合わせ(A)も選択肢ですが、まず公開情報を確認することが推奨されます。

-----

### <a name="no253"></a>**NO.253**

この問題については、Mountkirk Gamesのケーススタディを参照してください。

Mountkirk Gamesは、将来的にクラウドと技術の進歩を活用するために、ソリューションを設計したいと考えています。どの2つのステップを実行すべきですか？（2つ選択）

A. 将来的にユーザー行動を予測するための機械学習モデルのトレーニングに使用できるよう、現在財政的に可能な限り多くの分析およびゲームアクティビティデータを保存する。
B. ゲームバックエンドのアーティファクトをコンテナイメージにパッケージングし始め、Kubernetes Engineで実行して、ゲームアクティビティに基づいてスケールアップまたはダウンする可用性を向上させる。
C. JenkinsとSpinnakerを使用してCI/CDパイプラインを設定し、カナリアデプロイメントを自動化して開発速度を向上させる。
D. データベースに新しいプレイヤーデータを保存する必要がある新しいゲーム機能を追加する際のダウンタイムを削減するために、スキーマバージョニングツールを採用する。
E. 0デイ脆弱性のリスクを軽減するために、Linux仮想マシンの週次ローリングメンテナンスプロセスを実装し、重要なカーネルパッチとパッケージの更新を適用する。

**正解: B, C**

**解説:**
将来の技術進歩を活用し、俊敏性を高めるための現代的なクラウドネイティブアプローチを問う問題です。

  * [cite_start]**B (コンテナ化)**: アプリケーションを**コンテナ化**し、**Kubernetes (GKE)** で実行することは、ポータビリティを高め、スケーラビリティを向上させ、マイクロサービスアーキテクチャへの移行を容易にするためのデファクトスタンダードです [cite: 3701]。
  * [cite_start]**C (CI/CD)**: **Jenkins** (CI) と **Spinnaker** (CD) のようなツールを使用して**CI/CDパイプラインを自動化**することは、開発速度を向上させ、カナリアデプロイメントのような高度なリリース戦略を可能にし、将来の迅速なイノベーションに対応するための基盤となります [cite: 3702]。

-----

### <a name="no254"></a>**NO.254**

この問題については、Mountkirk Gamesのケーススタディを参照してください。

Mountkirk Gamesは、分析プラットフォームのモバイルネットワーク遅延の変化に対する回復力をテストする方法を設計するようにあなたに依頼しました。どうすべきですか？

A. モバイルクライアントの分析トラフィックに追加の遅延を注入できる障害注入ソフトウェアをゲーム分析プラットフォームにデプロイする。
B. Compute Engine仮想マシン上の携帯電話エミュレータから実行できるテストクライアントを構築し、世界中のGoogle Cloud Platformリージョンで複数コピーを実行して現実的なトラフィックを生成する。
C. モバイルデバイスからアップロードされた分析ファイルの処理を開始する前に、ランダムな量の遅延を導入する機能を追加する。
D. プレイヤーのモバイルデバイスで実行され、世界中のGoogle Cloud Platformリージョンで実行されている分析エンドポイントからの応答時間を収集する、オプトインのベータ版ゲームを作成する。

**正解: B**

**解説:**
PDFの回答Dは実際のユーザーに依存するため、テストの制御性と再現性に欠けます。回復力テストでは、制御された環境で様々な条件をシミュレートする必要があります。

  * **モバイルエミュレータ**: 実際のデバイスを使わずに、様々なネットワーク条件（3G, 4G, 高遅延など）をシミュレートできます。
  * [cite_start]**世界中のGCPリージョンで実行**: 世界中のユーザーからのアクセスを現実的にシミュレートし、異なる地理的な場所からのネットワーク遅延がプラットフォームに与える影響をテストできます [cite: 3708]。
    このアプローチにより、開発者は現実的かつ再現可能な方法で、ネットワーク遅延に対するプラットフォームの回復力を体系的にテストすることができます。

-----

### <a name="no255"></a>**NO.255**

Mountkirk Gamesは、リソースの物理的な場所を、彼らが事業を展開しているGoogle Cloudリージョンに限定したいと考えています。どうすべきですか？

A. リソースをデプロイできる場所を制約する組織ポリシーを構成する。
B. 構成できるリソースを制限するIAM条件を構成する。
C. 使用していないリージョンのリソースの割り当て（クォータ）を0に構成する。
D. 他のリージョンでリソースが作成されたときに無効にできるように、Cloud Monitoringでカスタムアラートを構成する。

**正解: A**

**解説:**
[cite_start]組織全体で特定のリソース構成を一元的に強制するための機能が**組織ポリシーサービス**です。`constraints/gcp.resourceLocations` という制約を使用すると、リソースを作成できる地理的な場所（リージョンやマルチリージョン）を許可リスト形式で指定できます [cite: 3714]。このポリシーを組織レベルで設定することで、承認されたリージョン以外でのリソース作成がIAM権限に関わらずブロックされ、コンプライアンスやデータ所在地の要件を確実に満たすことができます。

-----

### <a name="no256"></a>**NO.256**

この問題については、Mountkirk Gamesのケーススタディを参照してください。

あなたは、あなたの会社Mountkirk Gamesのデータベースワークロードの技術アーキテクチャを分析・定義する必要があります。ビジネスおよび技術要件を考慮して、どうすべきですか？

A. 時系列データにCloud SQLを使用し、履歴データクエリにCloud Bigtableを使用する。
B. MySQLを置き換えるためにCloud SQLを使用し、履歴データクエリにCloud Spannerを使用する。
C. MySQLを置き換えるためにCloud Bigtableを使用し、履歴データクエリにBigQueryを使用する。
D. 時系列データにCloud Bigtableを使用し、トランザクションデータにCloud Spannerを使用し、履歴データクエリにBigQueryを使用する。

**正解: D**

**解説:**
ケーススタディの要件を各データベースサービスにマッピングすると、Dが最も適切です。

  * [cite_start]**時系列データ**: 「ゲームアクティビティを時系列データベースサービスに保存」という要件には、大規模な時系列データの読み書きを得意とする**Cloud Bigtable**が最適です [cite: 3723]。
  * [cite_start]**トランザクションデータ**: 「ユーザープロファイルとゲーム状態を管理するためのトランザクションデータベースサービス」という要件には、グローバルなスケーラビリティと強力な一貫性を備えたリレーショナルデータベースである**Cloud Spanner**が適しています [cite: 3723]。
  * [cite_start]**履歴データ分析**: 「少なくとも10TBの履歴データへのクエリ」という分析（OLAP）要件には、データウェアハウスである**BigQuery**が最適です [cite: 3723]。

-----

### <a name="no257"></a>**NO.257**

あなたは、Mountkirk Gamesの新しいGoogle Cloudソリューションのために、Cloud Storageへのバッチファイル転送を最適化する必要があります。バッチファイルには、Cloud Storageにステージングされ、抽出変換ロード（ETL）ツールによって処理される必要があるゲーム統計が含まれています。どうすべきですか？

A. gsutilを使用してファイルを順次バッチ移動する。
B. gsutilを使用してファイルを並行してバッチコピーする。
C. gsutilを使用してETLの最初の部分としてファイルを抽出する。
D. gsutilを使用してETLの最後の部分としてファイルをロードする。

**正解: B**

**解説:**
[cite_start]多数のファイルをオンプレミスやVMからCloud Storageに転送する際のパフォーマンスを最適化するには、**並列アップロード**が非常に効果的です。`gsutil`コマンドの`-m`オプションを使用すると、複数のファイルが同時に（マルチスレッド/マルチプロセスで）アップロードされるため、ネットワーク帯域を最大限に活用し、全体の転送時間を大幅に短縮できます [cite: 3727]。`cp`（コピー）コマンドと`-m`オプションを組み合わせるのが標準的な方法です。

-----

### <a name="no258"></a>**NO.258**

あなたは、定義されたビジネスおよび技術要件を満たす新しいゲームのためのネットワークイングレスを実装する必要があります。Mountkirk Gamesは、各リージョンのゲームインスタンスを複数のGoogle Cloudリージョンに配置したいと考えています。どうすべきですか？

A. Compute Engineインスタンスを実行するマネージドインスタンスグループに接続されたグローバルロードバランサを構成する。
B. グローバルロードバランサとGoogle Kubernetes Engineでkubemciを構成する。
C. グローバルロードバランサとGoogle Kubernetes Engineを構成する。
D. グローバルロードバランサとGoogle Kubernetes EngineでIngress for Anthosを構成する。

**正解: A**

**解説:**
PDFの回答Dはより高度な選択肢ですが、Aが最も基本的な要件を満たしています。世界中にいるプレイヤーからのアクセスに対して低レイテンシを実現するには、ユーザーに最も近いリージョンでトラフィックを処理するのがベストプラクティスです。

  * [cite_start]**グローバル外部HTTP(S)ロードバランサ**: 単一のエニーキャストIPアドレスで、ユーザーからのトラフィックを最も近い、かつ正常なバックエンドに自動的にルーティングします [cite: 3732]。
  * [cite_start]**マネージドインスタンスグループ (MIG)**: 複数のリージョンにMIGを配置し、それぞれをロードバランサのバックエンドとして設定します。これにより、グローバルな負荷分散と高可用性が実現します [cite: 3732]。

-----

### <a name="no259"></a>**NO.259**

この問題については、Mountkirk Gamesのケーススタディを参照してください。

Mountkirk Gamesは、現在の分析および統計レポートモデルから、Google Cloud Platform上の技術要件を満たすものに移行したいと考えています。どの2つのステップが移行計画の一部であるべきですか？（2つ選択）

A. 現在のバッチETLコードをCloud Dataflowに移行する影響を評価する。
B. BigQueryでのパフォーマンス向上のためにデータを非正規化するスキーマ移行計画を作成する。
C. 単一のMySQLデータベースからMySQLクラスタに移行する方法を示すアーキテクチャ図を描く。
D. 以前のゲームから10TBの分析データをCloud SQLインスタンスにロードし、完全なデータセットに対してテストクエリを実行して正常に完了することを確認する。
E. Cloud Storageにアップロードされた分析ファイルに対するSQLインジェクション攻撃から保護するためにCloud Armorを統合する。

**正解: A, B**

**解説:**
ケーススタディでは、既存の分析が「ファイルを送信し、ETLツールでMySQLにロードする」というバッチ処理であることが示されています。また、新しい要件では大規模データ（10TB以上）の分析が必要です。

  * [cite_start]**A (ETLの移行)**: 既存のバッチETL処理を、よりスケーラブルでマネージドな**Cloud Dataflow**に移行することが考えられます。そのための影響評価は移行計画の重要なステップです [cite: 3740]。
  * [cite_start]**B (スキーマの非正規化)**: BigQueryは、**非正規化された（ネストされた、繰り返しフィールドを持つ）スキーマ**で最高のパフォーマンスを発揮するように設計されています。リレーショナルデータベース（MySQL）から移行する場合、JOINを減らすためにスキーマを非正規化する計画を立てることが、パフォーマンスを最適化する上で非常に重要です [cite: 3741]。

-----

### <a name="no260"></a>**NO.260**

Mountkirk Gamesは、新しいゲームアプリケーションプラットフォームからGoogle Cloudへの接続を保護するようにあなたに依頼しました。プロセスを合理化し、Google推奨のプラクティスに従いたいです。どうすべきですか？

A. Workload Identityとサービスアカウントを構成し、アプリケーションプラットフォームで使用する。
B. デフォルトで難読化されているKubernetes Secretsを使用する。これらのSecretをアプリケーションプラットフォームで使用するように構成する。
C. Kubernetes Secretsを構成してシークレットを保存し、アプリケーション層のシークレット暗号化を有効にし、Cloud Key Management Service（Cloud KMS）を使用して暗号化キーを管理する。これらのSecretをアプリケーションプラットフォームで使用するように構成する。
D. Compute Engine上にHashiCorp Vaultを構成し、顧客管理の暗号化キーとCloud Key Management Service（Cloud KMS）を使用して暗号化キーを管理する。これらのSecretをアプリケーションプラットフォームで使用するように構成する。

**正解: A**

**解説:**
[cite_start]GKEクラスタ内のPod（アプリケーション）がGoogle CloudのAPI（例: Cloud Storage, Spanner）にアクセスするための、最も安全で推奨される方法が**Workload Identity**です。Workload Identityを有効にすると、KubernetesのサービスアカウントをGoogle CloudのIAMサービスアカウントにマッピングできます。これにより、Podは**GCPサービスアカウントキー（JSONファイル）をダウンロードしたり管理したりすることなく**、アタッチされたIAMサービスアカウントの権限でGoogle APIを安全に呼び出すことができます [cite: 3747]。これは最小権限の原則に従い、キー漏洩のリスクを排除するベストプラクティスです。

-----

### <a name="no261"></a>**NO.261**

あなたの開発チームはモバイルゲームアプリを作成しました。さまざまな構成のAndroidおよびiOSデバイスで新しいモバイルアプリをテストしたいです。テストが効率的で費用対効果が高いことを確認する必要があります。どうすべきですか？

A. モバイルアプリをFirebase Test Labにアップロードし、AndroidおよびiOSデバイスでモバイルアプリをテストする。
B. Google CloudでAndroidおよびiOSのVMを作成し、VMにモバイルアプリをインストールしてテストする。
C. Google Kubernetes Engine（GKE）でAndroidおよびiOSのコンテナを作成し、コンテナにモバイルアプリをインストールしてテストする。
D. さまざまな構成のモバイルアプリをFirebase Hostingにアップロードし、各構成をテストする。

**正解: A**

**解説:**
[cite_start]**Firebase Test Lab**は、実際の物理デバイスと仮想デバイスの両方で、AndroidおよびiOSアプリをクラウド上でテストするためのサービスです。開発者はアプリのバイナリをアップロードするだけで、多種多様なデバイスモデル、OSバージョン、画面サイズ、地域設定で、アプリの動作を自動または手動でテストできます [cite: 3756]。自前でデバイスやエミュレータ環境を管理する必要がなく、非常に効率的で費用対効果の高いテストが可能です。

-----

### <a name="no262"></a>**NO.262**

あなたはMountkirk GamesのためにFirestoreを実装しています。Mountkirk Gamesは、新しいゲームにレガシーゲームのFirestoreデータベースへのプログラムによるアクセス権を与えたいと考えています。アクセスは可能な限り制限されるべきです。どうすべきですか？

A. レガシーゲームのGoogle Cloudプロジェクトにサービスアカウント（SA）を作成し、このSAを新しいゲームのIAMページに追加し、両方のプロジェクトでFirebase Adminロールを付与する。
B. レガシーゲームのGoogle Cloudプロジェクトにサービスアカウント（SA）を作成し、新しいゲームのIAMページに2番目のSAを追加し、両方のSAに組織管理者ロールを付与する。
C. レガシーゲームのGoogle Cloudプロジェクトにサービスアカウント（SA）を作成し、それにFirebase Adminロールを付与し、その後新しいゲームをレガシーゲームのプロジェクトに移行する。
D. レガシーゲームのGoogle Cloudプロジェクトにサービスアカウント（SA）を作成し、そのSAに組織管理者ロールを与え、その後両方のプロジェクトでFirebase Adminロールを付与する。

**正解: A**

**解説:**
プロジェクト間でリソースへのアクセスを安全に許可するには、サービスアカウントを使用するのが標準的な方法です。

1.  **リソースがある側（レガシーゲーム）のプロジェクトで、アクセス専用のサービスアカウント (SA)** を作成します。
2.  そのSAに対して、レガシープロジェクト内で必要な最小限の権限（この場合はFirestoreへのアクセス権を含む**Firebase Admin**ロールなど）を付与します。
3.  **アクセスしたい側（新しいゲーム）のプロジェクトのIAMページ**で、作成したSAのメールアドレスを**新しいメンバーとして追加**し、そのSAを使用するリソース（例: Compute Engine）に**サービスアカウントユーザー**ロールを付与します。
    この手順により、最小権限の原則に従って、安全にプロジェクト間のアクセスを実現できます。

-----

### <a name="no263"></a>**NO.263**

この問題については、Mountkirk Gamesのケーススタディを参照してください。

どのマネージドストレージオプションが、Mountkirkの時系列データベースサービスにゲームアクティビティを保存するという技術要件を満たしていますか？

A. Cloud Bigtable
B. Cloud Spanner
C. BigQuery
D. Cloud Datastore

**正解: A**

**解説:**
(注：この問題はNo.256の一部と同じです。)
[cite_start]ケーススタディの技術要件には「Store game activity in a **timeseries database service** for future analysis」と明記されています [cite: 3693]。
[cite_start]**Cloud Bigtable**は、IoTデータ、モニタリングデータ、そしてゲームのアクティビティログのような、大規模な**時系列データ**の取り込みと分析のために特別に設計された、低レイテンシで高スループットなNoSQLデータベースです [cite: 3693]。この要件に最も合致するマネージドサービスです。

-----

### <a name="no264"></a>**NO.264**

この問題については、Mountkirk Gamesのケーススタディを参照してください。

あなたは新しいゲームバックエンドプラットフォームのアーキテクチャを担当しています。ゲームはREST APIを介してバックエンドと通信します。Google推奨のプラクティスに従いたいです。バックエンドはどのように設計すべきですか？

A. バックエンド用のインスタンステンプレートを作成する。すべてのリージョンで、マルチゾーンマネージドインスタンスグループにデプロイする。L4ロードバランサを使用する。
B. バックエンド用のインスタンステンプレートを作成する。すべてのリージョンで、シングルゾーンマネージドインスタンスグループにデプロイする。L4ロードバランサを使用する。
C. バックエンド用のインスタンステンプレートを作成する。すべてのリージョンで、マルチゾーンマネージドインスタンスグループにデプロイする。L7ロードバランサを使用する。
D. バックエンド用のインスタンステンプレートを作成する。すべてのリージョンで、シングルゾーンマネージドインスタンスグループにデプロイする。L7ロードバランサを使用する。

**正解: C**

**解説:**
高可用性でスケーラブルなWebバックエンドを構築するためのベストプラクティスは以下の通りです。

  * [cite_start]**インスタンステンプレート**: VMの構成をテンプレート化し、迅速かつ一貫性のあるデプロイを可能にします [cite: 3774]。
  * [cite_start]**マルチゾーンマネージドインスタンスグループ**: インスタンスをリージョン内の複数のゾーンに分散させることで、単一ゾーンの障害に対する**高可用性**を実現します [cite: 3774]。
  * [cite_start]**L7ロードバランサ**: REST APIはHTTP(S)ベースの通信です。**外部HTTP(S)ロードバランサ (L7)** は、URLベースのルーティング、SSL終端、CDN連携など、HTTP(S)トラフィックに最適化された高度な機能を提供します [cite: 3774]。L4ロードバランサ(A, B)はTCP/UDPレベルで動作し、これらの機能はありません。

-----

### <a name="no265"></a>**NO.265**

あなたの開発チームは、Google Kubernetes Engine（GKE）で実行されているゲームの新しいバージョンを毎日リリースしています。ユーザーの視点から新しいバージョンの品質を評価するために、サービスレベル指標（SLI）を作成したいです。どうすべきですか？

A. CPU使用率とリクエストレイテンシをサービスレベル指標として作成する。
B. GKEのCPU使用率とメモリ使用率をサービスレベル指標として作成する。
C. リクエストレイテンシとエラー率をサービスレベル指標として作成する。
D. サーバーの稼働時間とエラー率をサービスレベル指標として作成する。

**正解: C**

**解説:**
サービスレベル指標（SLI）は、サービスの信頼性を**ユーザーの視点**から測定するための指標です。

  * [cite_start]**リクエストレイテンシ**: ユーザーがリクエストを送信してから応答を受け取るまでの時間。これはユーザー体験（UX）に直接影響します [cite: 3781]。
  * [cite_start]**エラー率**: リクエストのうち、エラーで失敗したものの割合。これもUXに直接影響します [cite: 3781]。
    CPU使用率(A, B)やサーバーの稼働時間(D)は、システム内部の状態を示す重要なメトリクスですが、それ自体はユーザーが直接体験するものではありません。SREのプラクティスでは、レイテンシ、エラー率、可用性、スループットなどが典型的なSLIとして使用されます。

-----

### <a name="no266"></a>**NO.266**

この問題については、Mountkirk Gamesのケーススタディを参照してください。

あなたは、あなたの会社Mountkirk Gamesのコンピュートワークロードの技術アーキテクチャを分析・定義する必要があります。Mountkirk Gamesのビジネスおよび技術要件を考慮して、どうすべきですか？

A. ネットワークロードバランサを作成する。プリエンプティブルCompute Engineインスタンスを使用する。
B. ネットワークロードバランサを作成する。非プリエンプティブルCompute Engineインスタンスを使用する。
C. マネージドインスタンスグループと自動スケーリングポリシーを備えたグローバルロードバランサを作成する。プリエンプティブルCompute Engineインスタンスを使用する。
D. マネージドインスタンスグループと自動スケーリングポリシーを備えたグローバルロードバランサを作成する。非プリエンプティブルCompute Engineインスタンスを使用する。

**正解: D**

**解説:**
[cite_start]ケーススタディのビジネス要件には「グローバルなフットプリントへの拡大」「アップタイムの向上」「顧客へのレイテンシ削減」があり、技術要件には「ゲームアクティビティに基づく動的なスケールアップ・ダウン」があります [cite: 3788]。

  * [cite_start]**グローバルロードバランサ**: グローバルなユーザーに対して低レイテンシを提供します [cite: 3788]。
  * [cite_start]**マネージドインスタンスグループと自動スケーリング**: 需要に応じた動的なスケーリングと、インスタンス障害時の自動修復によるアップタイム向上を実現します [cite: 3788]。
  * [cite_start]**非プリエンプティブルインスタンス**: ユーザー向けのオンラインゲームサーバーは、いつでも中断される可能性のあるプリエンプティブルVM(C)ではなく、安定した**標準（非プリエンプティブル）VM**を使用する必要があります [cite: 3789]。

-----

### <a name="no267"></a>**NO.267**

この問題については、Helicopter Racing League (HRL) のケーススタディを参照してください。

最近の財務監査で、ビデオのエンコードとトランスコードに割り当てられたCompute Engineインスタンスが非常に多いことが指摘されました。あなたは、これらの仮想マシンがワークロード完了後に削除されなかったゾンビマシンであると疑っています。どのVMインスタンスがアイドル状態であるかのリストを迅速に取得する必要があります。どうすべきですか？

A. 各Compute Engineインスタンスにログインし、分析のためにディスク、CPU、メモリ、およびネットワーク使用状況の統計を収集する。
B. `gcloud compute instances list`を使用して、`idle: true`ラベルが設定された仮想マシンインスタンスをリストする。
C. `gcloud recommender`コマンドを使用して、アイドル状態の仮想マシンインスタンスをリストする。
D. Google Consoleから、マネージドインスタンスグループ内のどのCompute Engineインスタンスがヘルスチェックプローブに応答しなくなったかを特定する。

**正解: C**

**解説:**
[cite_start]Google Cloudには、リソースの最適化を支援する**Recommender**という機能があります。その中の一つに**アイドル状態のVMに関する推奨事項**があります。これは、Cloud Monitoringが収集したCPUやネットワークの使用状況データを過去にさかのぼって分析し、長時間にわたってほとんど使用されていない（アイドル状態の）VMを自動的に検出してリストアップしてくれる機能です。この機能は`gcloud recommender recommendations list --recommender=google.compute.instance.IdleResourceRecommender`のようなコマンドで利用でき、要件に最も合致する迅速な方法です [cite: 3817]。

-----

### <a name="no268"></a>**NO.268**

あなたのBigQueryプロジェクトには複数のユーザーがいます。監査目的で、各ユーザーが先月実行したクエリの数を確認する必要があります。

A. `bq show`を使用してすべてのジョブをリストする。ジョブごとに`bq ls`を使用してジョブ情報をリストし、必要な情報を取得する。
B. BigQueryインターフェースで、JOBSテーブルに対してクエリを実行して必要な情報を取得する。
C. Google Data StudioをBigQueryに接続する。ユーザーのディメンションとユーザーごとのクエリ数のメトリックを作成する。
D. Cloud Audit Loggingを使用してCloud Audit Logsを表示し、クエリオペレーションにフィルタを作成して必要な情報を取得する。

**正解: B**

**解説:**
(注：この問題はNo.112とほぼ同じですが、選択肢が異なります。)
[cite_start]BigQueryでは、プロジェクトで実行されたすべてのジョブに関するメタデータが、`INFORMATION_SCHEMA.JOBS_BY_PROJECT` (またはリージョンごとの `JOBS_BY_...`) という組み込みの**ビュー（テーブル）**に自動的に記録されます [cite: 3822][cite_start]。このビューには、ジョブを実行したユーザー（`user_email`）、ジョブのタイプ（`job_type`）、作成時刻（`creation_time`）などが含まれています。したがって、このビューに対して標準的なSQLクエリ（例: `SELECT user_email, COUNT(*) FROM ... GROUP BY user_email`）を実行するだけで、ユーザーごとのクエリ数を簡単に集計できます [cite: 3822]。これが最も直接的で効率的な方法です。

-----

### <a name="no269"></a>**NO.269**

あなたはGoogle Cloud上で複数のプロジェクトを管理しており、gcloud CLIツールを使用してBigQuery、Bigtable、Kubernetes Engineと日常的にやり取りする必要があります。あなたは頻繁に出張し、週に異なるワークステーションで作業します。gcloud CLIを手動で管理するのを避けたいです。どうすべきですか？

A. Compute Engineインスタンスを作成し、そのインスタンスにgcloudをインストールする。SSH経由でこのインスタンスに接続して、常に同じgcloudインストールを使用する。
B. すべてのワークステーションにgcloudをインストールする。各ワークステーションで`gcloud components auto-update`コマンドを実行する。
C. Google Cloud ConsoleのGoogle Cloud Shellを使用してGoogle Cloudとやり取りする。
D. パッケージマネージャーを使用して、手動ではなくワークステーションにgcloudをインストールする。

**正解: C**

**解説:**
(注：この問題はNo.57とほぼ同じですが、選択肢の順序が異なります。)
[cite_start]**Google Cloud Shell**は、ブラウザからアクセスできるコマンドライン環境で、gcloud CLIやその他多くの開発ツールがプリインストールされ、常に最新の状態に保たれています [cite: 1058][cite_start]。どのワークステーションからでもブラウザさえあれば、同じ認証情報と設定（永続ホームディレクトリ）で一貫した環境に即座にアクセスできるため、ローカルマシンへのインストールや管理、更新の手間が一切不要になります [cite: 1058, 1059]。これが要件に最も合致する解決策です。

-----

### <a name="no270"></a>**NO.270**

この問題については、Helicopter Racing League (HRL) のケーススタディを参照してください。

最近、HRLは南アフリカのケープタウンで新しい地域レースリーグを開始しました。ケープタウンの顧客により良いユーザー体験を提供するため、HRLはコンテンツ配信ネットワークプロバイダーであるFastlyと提携しました。HRLは、FastlyのすべてのIPアドレス範囲からのトラフィックをVirtual Private Cloudネットワーク（VPCネットワーク）に許可する必要があります。あなたはHRLのセキュリティチームのメンバーであり、FastlyのIPアドレス範囲のみを外部HTTP(S)ロードバランサ経由で許可する更新を構成する必要があります。どのコマンドを使用すべきですか？

A. Fastlyの指定IPリストを使用して、外部ロードバランサにCloud Armorセキュリティポリシーを適用する。
B. Fastlyが公開しているIPアドレスを使用して、外部ロードバランサにCloud Armorセキュリティポリシーを適用する。
C. FastlyのIPアドレス範囲に対してポート443のVPCファイアウォールルールを適用する。
D. `source-ip-list-fastly`でタグ付けされたネットワークリソースに対してポート443のVPCファイアウォールルールを適用する。

**正解: A**

**解説:**
PDFの回答CはロードバランサではなくVPCレベルでの制御であり、Aの方がより適切です。
**Cloud Armor**は、Google Cloudの外部HTTP(S)ロードバランサと連携して、DDoS攻撃からの保護やWebアプリケーションファイアウォール（WAF）機能を提供するサービスです。

  * Cloud Armorのセキュリティポリシーでは、送信元IPアドレスに基づいてトラフィックを許可または拒否するルールを設定できます。
  * `--recaptcha-options=THREAT_PREVENTION` のようなオプションは存在しませんが、IPリストを指定する機能があります。
  * **名前付きIPリスト (Named IP Lists)** という機能を使えば、Fastlyのようなサードパーティプロバイダーが公開・管理しているIPアドレスリスト（`fastly`という名前で事前定義されている）をルールで直接参照できます。
    これにより、IPアドレスが変更されても、手動でルールを更新する必要なく、常に最新のリストに基づいてトラフィックを許可することができます。

-----

### <a name="no271"></a>**NO.271**

この問題については、Helicopter Racing League (HRL) のケーススタディを参照してください。

HRLは、ML予測モデルからより良い予測精度を求めています。HRLが予測を理解し解釈できるように、GoogleのAI Platformを使用するようにあなたに依頼しました。どうすべきですか？

A. Explainable AIを使用する。
B. Vision AIを使用する。
C. Google Cloudのオペレーションスイートを使用する。
D. Jupyter Notebooksを使用する。

**正解: A**

**解説:**
[cite_start]機械学習モデルはしばしば「ブラックボックス」と見なされますが、その予測結果が「なぜそうなったのか」を**理解し、解釈する**ことは、ビジネス上の意思決定やモデルの改善において非常に重要です。**Vertex AI Explainable AI (旧AI Platform Prediction with Explanations)** は、まさにこの目的のための機能です。特徴量のアトリビューション（どの特徴量が予測にどれだけ貢献したか）などを計算・可視化することで、モデルの予測の根拠を理解するのに役立ちます [cite: 3839]。

-----

### <a name="no272"></a>**NO.272**

この問題については、Helicopter Racing League (HRL) のケーススタディを参照してください。

あなたのチームは、数万人の視聴者、商品消費者、シーズンチケットホルダーの請求に使用されるカード番号のためのペイメントカードデータ保管庫を作成する責任があります。以下の要件を満たすカスタムカードトークン化サービスを実装する必要があります：

  * 最小限のコストで低レイテンシを提供する必要がある。
  * 重複したクレジットカードを識別でき、平文のカード番号を保存してはならない。
  * 年次のキーローテーションをサポートする必要がある。
    トークン化サービスにどのストレージアプローチを採用すべきですか？

A. 重複を特定するクエリを実行した後、Secret Managerにカードデータを保存する。
B. DatastoreモードのFirestoreに決定論的アルゴリズムで暗号化されたカードデータを保存する。
C. 決定論的アルゴリズムでカードデータを暗号化し、複数のMemorystoreインスタンスにシャーディングする。
D. Cloud SQLにデータを保存するために列レベルの暗号化を使用する。

**正解: B**

**解説:**
この要件を満たすためには、いくつかの要素を組み合わせる必要があります。

  * **重複検出**: 同じカード番号が異なるユーザーによって登録された場合でも、それが同じカードであることを識別できる必要があります。そのためには、同じ入力に対して常に同じ出力を生成する**決定論的暗号化**アルゴリズムが必要です。
  * [cite_start]**低レイテンシ＆スケーラビリティ**: 数万人のデータを扱うため、スケーラブルで低レイテンシなデータベースが必要です。**Firestore (Datastoreモード)** は、この要件を満たすマネージドNoSQLデータベースです [cite: 3847]。
  * **キーローテーション**: 暗号化に使用するキーは定期的にローテーションする必要があります。これはCloud KMSなどのサービスと組み合わせて実装できます。
    Secret Manager(A)は個々のシークレットを保存するのには適していますが、クエリや重複検出には向きません。

-----

### <a name="no273"></a>**NO.273**

この問題については、Helicopter Racing League (HRL) のケーススタディを参照してください。

HRL開発チームは、毎週火曜日の午前3時（UTC）に、彼らの予測能力アプリケーションの新しいバージョンをリポジトリにリリースします。HRLのセキュリティチームは、Airwolfという社内侵入テストCloud Functionを開発しました。セキュリティチームは、予測能力アプリケーションがリリースされるとすぐに、毎週火曜日にAirwolfを実行したいと考えています。毎週の定期的なケイデンスでAirwolfを実行するように設定する必要があります。どうすべきですか？

A. Cloud Tasksと、Cloud FunctionをトリガーするCloud Storageバケットを設定する。
B. Cloud Loggingシンクと、Cloud FunctionをトリガーするCloud Storageバケットを設定する。
C. Cloud FunctionをトリガーするPub/Subキューに通知するようにデプロイメントジョブを構成する。
D. Cloud FunctionをトリガーするためにIdentity and Access Management（IAM）とConfidential Computingを設定する。

**正解: C**

**解説:**
PDFの回答AのCloud Tasksは、HTTPリクエストをスケジュール実行するためのものであり、このイベント駆動のシナリオにはCがより適しています。
これは、イベント駆動型アーキテクチャの典型的なユースケースです。

1.  [cite_start]**イベントソース (デプロイメントジョブ)**: 新しいバージョンがデプロイされたら、そのCI/CDパイプラインの最終ステップで「デプロイ完了」というイベントメッセージを**Pub/Subトピック**に発行するように構成します [cite: 3857]。
2.  [cite_start]**イベントトリガー (Pub/Sub)**: このPub/Subトピックへのメッセージ発行をトリガーとして、**Cloud Function (Airwolf)** が自動的に実行されるように設定します [cite: 3857]。
    この方法により、リリースというイベントに直接連動して、侵入テストを即座に、かつ自動的に実行できます。

-----

### <a name="no274"></a>**NO.274**

この問題については、Helicopter Racing League (HRL) のケーススタディを参照してください。

HRLは、テレメトリのようなレースデータを保存するための費用対効果の高いアプローチを探しています。すべての履歴記録を保持し、前シーズンのデータのみを使用してモデルをトレーニングし、ボリュームと収集される情報の観点からデータの成長を計画したいと考えています。データソリューションを提案する必要があります。HRLのビジネス要件とCEO S. Hawkeが表明した目標を考慮して、どうすべきですか？

A. スケーラブルで柔軟なドキュメントベースのデータベースのためにFirestoreを使用する。コレクションを使用して、シーズンとイベントごとにレースデータを集約する。
B. ゼロダウンタイムでスキーマをバージョニングする能力とスケーラビリティのためにCloud Spannerを使用する。シーズンを主キーとしてレースデータを分割する。
C. スケーラビリティとスキーマに列を追加する能力のためにBigQueryを使用する。シーズンに基づいてレースデータをパーティション分割する。
D. ストレージの増加を自動的に管理し、MySQLとの互換性のためにCloud SQLを使用する。シーズンごとに別のデータベースインスタンスを使用する。

**正解: C**

**解説:**
要件は「履歴記録の全保持」「データ成長への対応」「大規模データでのモデルトレーニング」であり、これは分析（OLAP）ワークロードを示唆しています。

  * [cite_start]**BigQuery**: 大規模なデータ分析のために設計されたデータウェアハウスであり、スケーラビリティの要件を満たします [cite: 3867]。
  * [cite_start]**スキーマの柔軟性**: BigQueryは後からスキーマに列を追加することが容易であり、「収集される情報の成長」に対応できます [cite: 3867]。
  * [cite_start]**パーティショニング**: 「前シーズンのデータのみを使用してモデルをトレーニング」する場合、**シーズンでデータをパーティション分割**しておくと、クエリ対象をそのシーズンのパーティションのみに限定でき、スキャンするデータ量が減るため、パフォーマンスが向上し、コストが削減されます [cite: 3867]。

-----

### <a name="no275"></a>**NO.275**

この問題については、EHR Healthcareのケーススタディを参照してください。

過去に、構成エラーにより、インターネットからアクセスできてはならないバックエンドサーバーにパブリックIPアドレスが割り当てられてしまいました。誰もバックエンドのCompute Engineインスタンスに外部IPアドレスを割り当てることができず、外部IPアドレスはフロントエンドのCompute Engineインスタンスにのみ構成できるようにする必要があります。どうすべきですか？

A. フロントエンドのCompute Engineインスタンスに対してのみ外部IPアドレスを許可する制約を持つ組織ポリシーを作成する。
B. フロントエンドインスタンスを持つプロジェクトのすべてのユーザーから`compute.networkAdmin`ロールを取り消す。
C. ITスタッフを組織の`compute.networkAdmin`ロールにマッピングするIdentity and Access Management（IAM）ポリシーを作成する。
D. `compute.addresses.create`権限を持つ`GCE_FRONTEND`という名前のカスタムIdentity and Access Management（IAM）ロールを作成する。

**正解: A**

**解説:**
[cite_start]組織全体で特定のリソース構成を一元的に強制するための最も強力で推奨される方法は、**組織ポリシーサービス**です。`constraints/compute.vmExternalIpAccess`という制約を使用すると、外部IPアドレスを持つことを許可するVMインスタンスを制御できます [cite: 3899][cite_start]。このポリシーを構成し、フロントエンドインスタンスのみを許可リストに入れることで、IAM権限に関わらず、他のすべてのインスタンス（バックエンド）が誤って外部IPアドレスを持つことを防ぐことができます [cite: 3899]。これにより、設定ミスによるセキュリティリスクを根本から排除できます。

-----

### <a name="no276"></a>**NO.276**

この問題については、EHR Healthcareのケーススタディを参照してください。

あなたは、EHRのGoogle Cloudの使用が、今後のプライバシーコンプライアンス監査に合格することを保証する責任があります。どうすべきですか？（2つ選択）

A. Google Cloudコンプライアンスページの準拠製品リストに対して、EHRの製品使用状況を確認する。
B. EHRにGoogle Cloudとのビジネスアソシエイト契約（BAA）を締結するよう助言する。
C. EHRのユーザー向けアプリケーションにFirebase Authenticationを使用する。
D. EHRのWebベースアプリケーションのセキュリティ侵害を検出・防止するためにPrometheusを実装する。
E. すべてのKubernetesワークロードにGKEプライベートクラスタを使用する。

**正解: A, B**

**解説:**
EHR Healthcareは医療業界の企業であり、米国の医療保険の相互運用性と説明責任に関する法律（**HIPAA**）の対象となります。HIPAAコンプライアンスを達成するためには、以下のステップが不可欠です。

  * [cite_start]**B (BAAの締結)**: Google Cloud上で保護対象保健情報（PHI）を扱う場合、Googleとの間で**ビジネスアソシエイト契約 (BAA)** を締結することが法的に義務付けられています。これは、GoogleがインフラプロバイダーとしてHIPAAの責任を負うことを示す契約です [cite: 3906]。
  * [cite_start]**A (対象サービスの確認)**: Google CloudのすべてのサービスがHIPAAの対象となっているわけではありません。Googleが公開している**HIPAA対象サービス**のリストを確認し、アプリケーションがそれらのサービスのみを使用していることを確認する必要があります [cite: 3905]。

-----

### <a name="no277"></a>**NO.277**

この問題については、EHR Healthcareのケーススタディを参照してください。

あなたは、Google Cloudへのワークロードを安全にデプロイするための技術アーキテクチャを定義する必要があります。また、検証済みのコンテナのみがGoogle Cloudサービスを使用してデプロイされることを確認する必要があります。どうすべきですか？（2つ選択）

A. GKEでBinary Authorizationを有効にし、CI/CDパイプラインの一部としてコンテナに署名する。
B. Jenkinsを構成してKritisを利用し、CI/CDパイプラインの一部としてコンテナに暗号署名する。
C. 信頼できるサービスアカウントのみがレジストリからコンテナを作成・デプロイできるようにContainer Registryを構成する。
D. ワークロードをデプロイする前に脆弱性がないことを確認するために、Container Registryを構成して脆弱性スキャンを使用する。

**正解: A, D**

**解説:**
PDFの回答Aは、Dの内容も内包するより包括的な回答です。安全なコンテナデプロイメントパイプライン（セキュアなソフトウェアサプライチェーン）を構築するためのベストプラクティスは以下の通りです。

  * [cite_start]**D (脆弱性スキャン)**: CI/CDパイプラインの早い段階で、コンテナイメージとその依存関係に既知の脆弱性（CVE）がないかを**Artifact Registry（旧Container Registry）の脆弱性スキャン**機能で自動的にスキャンします [cite: 3917]。
  * [cite_start]**A (デプロイ時の強制)**: スキャンに合格し、QAチームによって承認されたイメージにのみ、CI/CDパイプラインが**暗号署名（証明書）を付けます。GKEクラスタでBinary Authorization**を有効にし、「有効な署名を持つイメージのみデプロイを許可する」というポリシーを強制します [cite: 3913]。
    これにより、検証・承認されていないコンテナが本番環境にデプロイされることを防ぎます。

-----

### <a name="no278"></a>**NO.278**

この問題については、EHR Healthcareのケーススタディを参照してください。

あなたは、Google Kubernetes EngineのためのGoogle Cloudネットワークアーキテクチャを設計する責任があります。Googleのベストプラクティスに従いたいです。EHR Healthcareのビジネスおよび技術要件を考慮して、攻撃対象領域を減らすためにどうすべきですか？

A. プライベートエンドポイントとマスター承認済みネットワークが構成されたプライベートクラスタを使用する。
B. ファイアウォールルールとVirtual Private Cloud（VPC）ルートを持つパブリッククラスタを使用する。
C. パブリックエンドポイントとマスター承認済みネットワークが構成されたプライベートクラスタを使用する。
D. マスター承認済みネットワークが有効なパブリッククラスタとファイアウォールルールを使用する。

**正解: C**

**解説:**
PDFの回答Bはセキュリティレベルが低いです。攻撃対象領域を減らすためのGKEのベストプラクティスは、**プライベートクラスタ**を使用することです。

  * [cite_start]**プライベートクラスタ**: GKEのノード（VM）がパブリックIPアドレスを持たなくなるため、インターネットから直接アクセスできなくなり、攻撃対象領域が大幅に減少します [cite: 3923]。
  * [cite_start]**パブリックエンドポイントとマスター承認済みネットワーク**: `kubectl`コマンドなどを実行するために、コントロールプレーン（マスター）へのアクセスは必要です。コントロールプレーンのエンドポイントをパブリックにしつつ、**マスター承認済みネットワーク**を構成して、特定のIPアドレス範囲（例: オフィスのIP、CI/CDシステムのIP）からしかアクセスできないように制限します [cite: 3923]。
    これが、セキュリティと運用性のバランスが取れた構成です。

-----

### <a name="no279"></a>**NO.279**

あなたは、EHRの要件に準拠するために接続をアップグレードする必要があります。新しい接続設計は、ビジネスクリティカルなニーズをサポートし、同じネットワークおよびセキュリティポリシー要件を満たす必要があります。どうすべきですか？

A. 新しいDedicated Interconnect接続を追加する。
B. Dedicated Interconnect接続の帯域幅を100Gにアップグレードする。
C. 3つの新しいCloud VPN接続を追加する。
D. 新しいCarrier Peering接続を追加する。

**正解: A**

**解説:**
[cite_start]ケーススタディには、EHRが**単一のDedicated Interconnect接続**を持っていると記載されています [cite: 3947]。単一の接続は、物理的な障害（ファイバーの切断など）や機器のメンテナンスに対して脆弱な**単一障害点（Single Point of Failure）となります。ビジネスクリティカルなニーズを満たすための高可用性（Googleは99.9%または99.99%のSLAを提供）を実現するには、物理的に冗長な2本目のDedicated Interconnect接続を追加**し、異なるエッジアベイラビリティドメインに配置するのがGoogleの推奨プラクティスです。

-----

### <a name="no280"></a>**NO.280**

この問題については、EHR Healthcareのケーススタディを参照してください。

あなたは、EHRのオンプレミスシステムとGoogle Cloud間のハイブリッド接続の技術アーキテクチャを定義する必要があります。本番レベルのアプリケーションに対するGoogleの推奨プラクティスに従いたいです。EHR Healthcareのビジネスおよび技術要件を考慮して、どうすべきですか？

A. 1つのメトロ（都市）に2つのPartner Interconnect接続を構成し、Interconnect接続が異なるメトロゾーンに配置されるようにする。
B. オンプレミスからGoogle Cloudへの2つのVPN接続を構成し、オンプレミスのVPNデバイスが別々のラックにあることを確認する。
C. EHR HealthcareとGoogle Cloud間にDirect Peeringを構成し、少なくとも2つのGoogleロケーションでピアリングしていることを確認する。
D. 1つのメトロ（都市）に2つのDedicated Interconnect接続と、別のメトロに2つの接続を構成し、Interconnect接続が異なるメトロゾーンに配置されるようにする。

**正解: D**

**解説:**
[cite_start]本番レベルのアプリケーション、特にミッションクリティカルなシステムのための最高の可用性（99.99% SLA）を実現するためのGoogle推奨トポロジは、**地理的に離れた2つのメトロ（都市圏）**にまたがって、**それぞれ2つのDedicated Interconnect接続**を、**異なるエッジアベイラビリティドメイン**に配置することです [cite: 3937]。この構成により、単一の光ファイバーの切断、単一の機器の障害、さらには単一のコロケーション施設の障害やメトロ全体の障害からも保護されます。

-----

### <a name="no281"></a>**NO.281**

この問題については、EHR Healthcareのケーススタディを参照してください。

あなたはEHR顧客ポータルチームの開発者です。あなたのチームは最近、顧客ポータルアプリケーションをGoogle Cloudに移行しました。アプリケーションサーバーの負荷が増加し、現在アプリケーションは多くのタイムアウトエラーを記録しています。あなたは最近、アプリケーションアーキテクチャにPub/Subを組み込みましたが、アプリケーションはPub/Subの公開エラーを記録していません。公開のレイテンシを改善したいです。どうすべきですか？

A. Pub/SubのTotal Timeoutリトライ値を増やす。
B. Pub/Subのサブスクライバープルモデルからプッシュモデルに移行する。
C. Pub/Subのメッセージバッチ処理をオフにする。
D. バックアップのPub/Subメッセージキューを作成する。

**正解: C**

**解説:**
[cite_start]Pub/Subのクライアントライブラリは、効率を向上させるために、デフォルトで複数の小さなメッセージをまとめて一つのリクエストとして送信する**バッチ処理**を行います。これはスループットを向上させますが、個々のメッセージにとっては、バッチがいっぱいになるかタイムアウトするまで待機する必要があるため、**レイテンシ（遅延）が増加**する要因になります。タイムアウトエラーが発生しており、レイテンシを改善したい場合、バッチ処理をオフにする（またはバッチサイズを小さくする）ことで、メッセージが即座に送信されるようになり、公開レイテンシが改善される可能性があります [cite: 3946]。

-----

### <a name="no282"></a>**NO.282**

この問題については、EHR Healthcareのケーススタディを参照してください。

EHRは、プライマリデータセンターとGoogleのネットワーク間に単一のDedicated Interconnect接続を持っています。この接続はEHRのネットワークおよびセキュリティポリシーを満たしています：

  * パブリックIPアドレスを持たないオンプレミスサーバーは、パブリックIPアドレスを持たないクラウドリソースに接続する必要がある。
  * 本番ネットワーク管理サーバーからCompute Engine仮想マシンへのトラフィックフローは、決してパブリックインターネットを経由してはならない。
    あなたは、EHRの要件に準拠するために接続をアップグレードする必要があります。新しい接続設計は、ビジネスクリティカルなニーズをサポートし、同じネットワークおよびセキュリティポリシー要件を満たす必要があります。どうすべきですか？

A. 新しいDedicated Interconnect接続を追加する。
B. Dedicated Interconnect接続の帯域幅を100Gにアップグレードする。
C. 3つの新しいCloud VPN接続を追加する。
D. 新しいCarrier Peering接続を追加する。

**正解: A**

**解説:**
(注：この問題はNo.279とほぼ同じです。)
[cite_start]ケーススタディには、EHRが**単一のDedicated Interconnect接続**を持っていると記載されています [cite: 3947, 3953][cite_start]。単一の接続は、物理的な障害や機器のメンテナンスに対して脆弱な**単一障害点（Single Point of Failure）となります。ビジネスクリティカルなニーズを満たすための高可用性（Googleは99.9%または99.99%のSLAを提供）を実現するには、物理的に冗長な2本目のDedicated Interconnect接続を追加**し、異なるエッジアベイラビリティドメインに配置するのがGoogleの推奨プラクティスです [cite: 3954, 3955]。
